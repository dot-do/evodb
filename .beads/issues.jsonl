{"id":"evodb-047i","title":"PRODUCT: Client SDK for React/Vue/Svelte","description":"## Problem\nEvoDB is focused on server-side Cloudflare Workers but modern apps need client-side integration.\n\n## Solution\nCreate client SDKs:\n```typescript\n// React hook\nimport { useEvoDB, useQuery, useMutation } from '@evodb/react';\n\nfunction UserList() {\n  const { data, loading } = useQuery('users', {\n    where: { active: true },\n    limit: 10\n  });\n  \n  return \u003cul\u003e{data?.map(user =\u003e \u003cli\u003e{user.name}\u003c/li\u003e)}\u003c/ul\u003e;\n}\n```\n\n## Packages\n- @evodb/client - Core client for any JS environment\n- @evodb/react - React hooks and components\n- @evodb/vue - Vue composables\n- @evodb/svelte - Svelte stores\n\n## Impact\nEnables full-stack applications using EvoDB","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:15.327865-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.838057-06:00","closed_at":"2026-01-21T20:08:17.838057-06:00","close_reason":"Already implemented"}
{"id":"evodb-04r","title":"TDD: Encapsulate V8 stack trace capture","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:56.07856-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.680192-06:00","closed_at":"2026-01-20T17:49:16.680192-06:00","close_reason":"Closed","external_ref":"gh-102"}
{"id":"evodb-0pgc","title":"TDD: E2E tests for complete CDC pipeline","description":"## Overview\nAdd end-to-end tests for the complete CDC (Change Data Capture) pipeline from child DOs through parent to R2 storage.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\nWrite E2E tests that verify the complete CDC flow:\n\n```typescript\ndescribe('CDC Pipeline E2E', () =\u003e {\n  describe('child→parent→R2 flow', () =\u003e {\n    it('should propagate changes from child DO to R2', async () =\u003e {\n      // Write to child, verify appears in R2\n    });\n    \n    it('should batch changes efficiently', async () =\u003e {\n      // Multiple child writes should batch before R2 write\n    });\n    \n    it('should preserve change ordering across pipeline', async () =\u003e {\n      // Order of operations maintained end-to-end\n    });\n    \n    it('should handle multiple children writing concurrently', async () =\u003e {\n      // Concurrent child writes merge correctly\n    });\n  });\n  \n  describe('reconnection handling', () =\u003e {\n    it('should resume CDC after child reconnection', async () =\u003e {\n      // Child disconnects and reconnects, no data loss\n    });\n    \n    it('should resume CDC after parent reconnection', async () =\u003e {\n      // Parent restarts, children reconnect seamlessly\n    });\n    \n    it('should handle R2 unavailability gracefully', async () =\u003e {\n      // Buffer during R2 outage, flush on recovery\n    });\n    \n    it('should reconcile state after network partition', async () =\u003e {\n      // Split-brain recovery scenario\n    });\n  });\n  \n  describe('ordering guarantees', () =\u003e {\n    it('should maintain causal ordering within single child', async () =\u003e {\n      // Operations from one child in correct order\n    });\n    \n    it('should provide total ordering across children', async () =\u003e {\n      // Global sequence numbers assigned correctly\n    });\n    \n    it('should handle out-of-order message delivery', async () =\u003e {\n      // Reorder buffer works correctly\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement E2E Test Harness\n\n1. Create test harness components:\n   - Mock child DO implementation\n   - Mock parent DO implementation  \n   - Mock R2 storage with verification\n   - Network simulation layer\n   \n2. Implement test utilities:\n   - Change event generators\n   - Pipeline state inspectors\n   - Timing/ordering validators\n   \n3. Make all E2E tests pass with real implementation\n\n### REFACTOR Phase - Performance Assertions\n\n1. Add performance assertions:\n   ```typescript\n   it('should propagate changes within 100ms p99', async () =\u003e {\n     // Latency SLA verification\n   });\n   \n   it('should handle 1000 changes/second throughput', async () =\u003e {\n     // Throughput SLA verification\n   });\n   ```\n\n2. Add observability:\n   - Pipeline lag metrics\n   - Throughput measurements\n   - Error rate tracking\n   \n3. Optimize critical paths:\n   - Reduce serialization overhead\n   - Batch optimization\n   - Connection pooling improvements\n\n## Acceptance Criteria\n- [ ] All RED phase E2E tests written and failing\n- [ ] Test harness implemented and functional\n- [ ] All GREEN phase tests passing\n- [ ] Performance assertions added and passing\n- [ ] Pipeline latency p99 \u003c 100ms\n- [ ] Throughput \u003e 1000 changes/second sustained","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:17.888414-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:52:37.997699-06:00","closed_at":"2026-01-21T19:52:37.997699-06:00","close_reason":"Closed"}
{"id":"evodb-0q8","title":"TDD: Add lance-reader package tests (21% coverage)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:31.457236-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.415388-06:00","closed_at":"2026-01-20T16:49:22.415388-06:00","close_reason":"Closed","external_ref":"gh-60"}
{"id":"evodb-0qrg","title":"PRODUCT: Compression strategy documentation","description":"## Problem\nREADME mentions compression (dictionary, delta, RLE) but lacks documentation on when to use each.\n\n## Solution\nAdd compression documentation:\n\n1. **Compression Types**\n   - Dictionary encoding: Low-cardinality strings\n   - Delta encoding: Sequential numbers, timestamps\n   - Run-length encoding: Repeated values\n   - Bitpacking: Small integers, booleans\n\n2. **Auto-selection**\n   - Document how EvoDB chooses encoding\n   - Configuration options for overrides\n\n3. **Performance Trade-offs**\n   - Compression ratio vs decode speed\n   - Memory usage during decompression\n\n4. **Benchmarks**\n   - Show real-world compression ratios\n   - Compare to raw JSON storage\n\n## Deliverables\n- COMPRESSION.md documentation\n- Benchmark results in README\n\n## Impact\nHelps users optimize storage costs","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:27.980621-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:17:08.580635-06:00","closed_at":"2026-01-21T20:17:08.580635-06:00","close_reason":"Closed"}
{"id":"evodb-0sx","title":"TDD: Add JSON parse error handling in lakehouse","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:36.360751-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.667962-06:00","closed_at":"2026-01-20T17:49:16.667962-06:00","close_reason":"Closed","external_ref":"gh-103"}
{"id":"evodb-16m","title":"TDD: Fix CDC sourceCursors race condition","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:34.83366-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.132444-06:00","closed_at":"2026-01-20T16:55:12.132444-06:00","close_reason":"Closed","external_ref":"gh-61"}
{"id":"evodb-187","title":"Add chaos testing suite","description":"TDD: Only 1 chaos test file exists. Add chaos tests for network failures, timeouts, and partial writes.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:06.437653-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:54:46.031781-06:00","closed_at":"2026-01-20T18:54:46.031781-06:00","close_reason":"Closed","external_ref":"gh-128"}
{"id":"evodb-1kq","title":"TDD: Add Step type discriminator guards","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:07.350593-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:11.949749-06:00","closed_at":"2026-01-20T16:55:11.949749-06:00","close_reason":"Closed","external_ref":"gh-62"}
{"id":"evodb-1wad","title":"EDIT: Add CDC and compaction examples to @evodb/writer README","description":"## Overview\nEnhance the @evodb/writer package README with detailed examples for buffer configuration, compaction strategies, partition modes, and CDC flows.\n\n## Current State\nThe writer package README needs more comprehensive documentation on advanced features.\n\n## Required Changes\n\n### 1. Buffer Configuration Examples\n```typescript\nimport { createWriter, BufferConfig } from '@evodb/writer'\n\nconst writer = createWriter({\n  buffer: {\n    // Memory buffer settings\n    maxSize: 10 * 1024 * 1024, // 10MB\n    maxDocuments: 10000,\n    \n    // Flush triggers\n    flushInterval: 5000, // 5 seconds\n    flushOnClose: true,\n    \n    // Overflow behavior\n    onOverflow: 'flush', // or 'error', 'drop-oldest'\n  },\n})\n\n// Monitor buffer state\nwriter.on('buffer:flush', (stats) =\u003e {\n  console.log(`Flushed ${stats.documents} documents (${stats.bytes} bytes)`)\n})\n\nwriter.on('buffer:overflow', (stats) =\u003e {\n  console.log('Buffer overflow detected')\n})\n```\n\n### 2. Compaction Strategies\n```typescript\nimport { createCompactor, CompactionStrategy } from '@evodb/writer'\n\n// Size-tiered compaction (good for write-heavy)\nconst sizeTiered = createCompactor({\n  strategy: CompactionStrategy.SIZE_TIERED,\n  minFilesToCompact: 4,\n  maxFilesToCompact: 32,\n  targetFileSize: 256 * 1024 * 1024, // 256MB\n})\n\n// Leveled compaction (good for read-heavy)\nconst leveled = createCompactor({\n  strategy: CompactionStrategy.LEVELED,\n  levels: 7,\n  levelMultiplier: 10,\n  level0FileLimit: 4,\n})\n\n// Time-window compaction (good for time-series)\nconst timeWindow = createCompactor({\n  strategy: CompactionStrategy.TIME_WINDOW,\n  windowSize: 'day', // or 'hour', 'week'\n  retentionPeriod: '90d',\n})\n\n// Run compaction\nawait compactor.compact({\n  dryRun: false,\n  maxDuration: 60000, // 1 minute max\n})\n```\n\n### 3. Partition Modes Explained\n```typescript\nimport { PartitionMode } from '@evodb/writer'\n\n// Hash partitioning (even distribution)\nconst hashPartitioned = createWriter({\n  partitionMode: PartitionMode.HASH,\n  partitionKey: 'userId',\n  partitionCount: 16,\n})\n\n// Range partitioning (for range queries)\nconst rangePartitioned = createWriter({\n  partitionMode: PartitionMode.RANGE,\n  partitionKey: 'timestamp',\n  rangeSize: 'day', // one partition per day\n})\n\n// List partitioning (explicit buckets)\nconst listPartitioned = createWriter({\n  partitionMode: PartitionMode.LIST,\n  partitionKey: 'region',\n  partitions: {\n    'us': ['us-east', 'us-west'],\n    'eu': ['eu-west', 'eu-central'],\n    'asia': ['asia-east', 'asia-south'],\n  },\n})\n```\n\n### 4. CDC Flow Diagram and Examples\n```\n┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n│   Source    │───▶│  CDC Stream │───▶│   EvoDB     │\n│  (Postgres) │    │  (Debezium) │    │   Writer    │\n└─────────────┘    └─────────────┘    └─────────────┘\n                          │\n                          ▼\n                   ┌─────────────┐\n                   │  Transform  │\n                   │   (optional)│\n                   └─────────────┘\n```\n\n```typescript\nimport { createCDCWriter } from '@evodb/writer'\n\nconst cdcWriter = createCDCWriter({\n  // Source configuration\n  source: {\n    type: 'postgres',\n    connectionString: process.env.DATABASE_URL,\n    tables: ['users', 'orders'],\n    slotName: 'evodb_cdc',\n  },\n  \n  // Transform CDC events\n  transform: (event) =\u003e ({\n    ...event.after,\n    _operation: event.op, // 'c', 'u', 'd'\n    _timestamp: event.ts_ms,\n  }),\n  \n  // Handle deletes\n  onDelete: 'tombstone', // or 'ignore', 'soft-delete'\n})\n\n// Start CDC pipeline\nawait cdcWriter.start()\n\n// Monitor CDC lag\ncdcWriter.on('lag', (lag) =\u003e {\n  if (lag \u003e 5000) {\n    console.warn(`CDC lag: ${lag}ms`)\n  }\n})\n```\n\n## Acceptance Criteria\n- [ ] Buffer configuration covers all options\n- [ ] Compaction strategies have clear use case guidance\n- [ ] Partition modes include performance implications\n- [ ] CDC diagram is accurate and helpful\n- [ ] All examples are tested with real scenarios","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:58.905529-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:59:56.954295-06:00","closed_at":"2026-01-21T19:59:56.954295-06:00","close_reason":"Closed"}
{"id":"evodb-1z9","title":"TDD: Extract magic numbers to config constants","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:10:00.224989-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.676871-06:00","closed_at":"2026-01-20T17:49:16.676871-06:00","close_reason":"Closed","external_ref":"gh-104"}
{"id":"evodb-20b7","title":"TDD: Implement @evodb/react client SDK","description":"## Overview\nImplement the official React client SDK for EvoDb with hooks for queries, mutations, and real-time subscriptions.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. useQuery Hook Tests\n```typescript\ndescribe('useQuery', () =\u003e {\n  it('should return loading state initially', () =\u003e {\n    const { result } = renderHook(() =\u003e useQuery('SELECT * FROM users'));\n    expect(result.current.loading).toBe(true);\n    expect(result.current.data).toBeUndefined();\n  });\n\n  it('should return data after query resolves', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e useQuery('SELECT * FROM users'));\n    await waitFor(() =\u003e !result.current.loading);\n    expect(result.current.data).toEqual([{ id: 1, name: 'Alice' }]);\n  });\n\n  it('should return error on query failure', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e useQuery('INVALID SQL'));\n    await waitFor(() =\u003e result.current.error);\n    expect(result.current.error).toBeInstanceOf(Error);\n  });\n\n  it('should support parameterized queries', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e \n      useQuery('SELECT * FROM users WHERE id = ?', [1])\n    );\n    await waitFor(() =\u003e !result.current.loading);\n    expect(result.current.data).toEqual([{ id: 1, name: 'Alice' }]);\n  });\n\n  it('should refetch when dependencies change', async () =\u003e {\n    const { result, rerender, waitFor } = renderHook(\n      ({ id }) =\u003e useQuery('SELECT * FROM users WHERE id = ?', [id]),\n      { initialProps: { id: 1 } }\n    );\n    await waitFor(() =\u003e !result.current.loading);\n    rerender({ id: 2 });\n    await waitFor(() =\u003e result.current.data[0].id === 2);\n  });\n\n  it('should support suspense mode', async () =\u003e {\n    const wrapper = ({ children }) =\u003e (\n      \u003cSuspense fallback={\u003cdiv\u003eLoading...\u003c/div\u003e}\u003e\n        \u003cEvoDbProvider client={client}\u003e{children}\u003c/EvoDbProvider\u003e\n      \u003c/Suspense\u003e\n    );\n    const { result } = renderHook(() =\u003e useQuery('SELECT * FROM users', { suspense: true }), { wrapper });\n    expect(result.current.data).toBeDefined(); // No loading state with suspense\n  });\n});\n```\n\n#### 2. useMutation Hook Tests\n```typescript\ndescribe('useMutation', () =\u003e {\n  it('should return mutate function', () =\u003e {\n    const { result } = renderHook(() =\u003e useMutation('INSERT INTO users (name) VALUES (?)'));\n    expect(typeof result.current.mutate).toBe('function');\n    expect(result.current.loading).toBe(false);\n  });\n\n  it('should execute mutation with parameters', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e \n      useMutation('INSERT INTO users (name) VALUES (?)')\n    );\n    act(() =\u003e result.current.mutate(['Bob']));\n    await waitFor(() =\u003e !result.current.loading);\n    expect(result.current.data).toEqual({ changes: 1, lastInsertRowid: 2 });\n  });\n\n  it('should handle mutation errors', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e \n      useMutation('INSERT INTO users (invalid_col) VALUES (?)')\n    );\n    act(() =\u003e result.current.mutate(['value']));\n    await waitFor(() =\u003e result.current.error);\n    expect(result.current.error).toBeInstanceOf(Error);\n  });\n\n  it('should support onSuccess callback', async () =\u003e {\n    const onSuccess = vi.fn();\n    const { result, waitFor } = renderHook(() =\u003e \n      useMutation('INSERT INTO users (name) VALUES (?)', { onSuccess })\n    );\n    act(() =\u003e result.current.mutate(['Bob']));\n    await waitFor(() =\u003e onSuccess.mock.calls.length \u003e 0);\n    expect(onSuccess).toHaveBeenCalledWith({ changes: 1, lastInsertRowid: 2 });\n  });\n\n  it('should support onError callback', async () =\u003e {\n    const onError = vi.fn();\n    const { result, waitFor } = renderHook(() =\u003e \n      useMutation('INVALID SQL', { onError })\n    );\n    act(() =\u003e result.current.mutate([]));\n    await waitFor(() =\u003e onError.mock.calls.length \u003e 0);\n    expect(onError).toHaveBeenCalled();\n  });\n});\n```\n\n#### 3. useSubscription Hook Tests\n```typescript\ndescribe('useSubscription', () =\u003e {\n  it('should subscribe to table changes', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e \n      useSubscription('users')\n    );\n    expect(result.current.connected).toBe(false);\n    await waitFor(() =\u003e result.current.connected);\n    expect(result.current.connected).toBe(true);\n  });\n\n  it('should receive insert events', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e useSubscription('users'));\n    await waitFor(() =\u003e result.current.connected);\n    \n    // Trigger insert from another connection\n    await client.execute('INSERT INTO users (name) VALUES (?)', ['New User']);\n    \n    await waitFor(() =\u003e result.current.lastEvent);\n    expect(result.current.lastEvent).toMatchObject({\n      type: 'INSERT',\n      table: 'users',\n      row: { name: 'New User' }\n    });\n  });\n\n  it('should receive update events', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e useSubscription('users'));\n    await waitFor(() =\u003e result.current.connected);\n    \n    await client.execute('UPDATE users SET name = ? WHERE id = ?', ['Updated', 1]);\n    \n    await waitFor(() =\u003e result.current.lastEvent?.type === 'UPDATE');\n    expect(result.current.lastEvent).toMatchObject({\n      type: 'UPDATE',\n      table: 'users',\n      oldRow: { id: 1, name: 'Alice' },\n      newRow: { id: 1, name: 'Updated' }\n    });\n  });\n\n  it('should receive delete events', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e useSubscription('users'));\n    await waitFor(() =\u003e result.current.connected);\n    \n    await client.execute('DELETE FROM users WHERE id = ?', [1]);\n    \n    await waitFor(() =\u003e result.current.lastEvent?.type === 'DELETE');\n    expect(result.current.lastEvent).toMatchObject({\n      type: 'DELETE',\n      table: 'users',\n      row: { id: 1 }\n    });\n  });\n\n  it('should unsubscribe on unmount', async () =\u003e {\n    const { result, waitFor, unmount } = renderHook(() =\u003e useSubscription('users'));\n    await waitFor(() =\u003e result.current.connected);\n    \n    unmount();\n    \n    // Verify WebSocket is closed\n    expect(mockWebSocket.close).toHaveBeenCalled();\n  });\n\n  it('should filter events by query', async () =\u003e {\n    const { result, waitFor } = renderHook(() =\u003e \n      useSubscription('users', { filter: { id: 1 } })\n    );\n    await waitFor(() =\u003e result.current.connected);\n    \n    // Insert for id=2 should not trigger\n    await client.execute('INSERT INTO users (id, name) VALUES (?, ?)', [2, 'Other']);\n    await new Promise(r =\u003e setTimeout(r, 100));\n    expect(result.current.lastEvent).toBeUndefined();\n    \n    // Update for id=1 should trigger\n    await client.execute('UPDATE users SET name = ? WHERE id = ?', ['Updated', 1]);\n    await waitFor(() =\u003e result.current.lastEvent);\n    expect(result.current.lastEvent.row.id).toBe(1);\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/react/src/provider.tsx\nimport { createContext, useContext, ReactNode } from 'react';\nimport { EvoDbClient } from '@evodb/client';\n\nconst EvoDbContext = createContext\u003cEvoDbClient | null\u003e(null);\n\nexport function EvoDbProvider({ \n  client, \n  children \n}: { \n  client: EvoDbClient; \n  children: ReactNode \n}) {\n  return (\n    \u003cEvoDbContext.Provider value={client}\u003e\n      {children}\n    \u003c/EvoDbContext.Provider\u003e\n  );\n}\n\nexport function useEvoDb(): EvoDbClient {\n  const client = useContext(EvoDbContext);\n  if (!client) {\n    throw new Error('useEvoDb must be used within EvoDbProvider');\n  }\n  return client;\n}\n\n// packages/react/src/useQuery.ts\nexport function useQuery\u003cT = unknown\u003e(\n  sql: string,\n  params: unknown[] = [],\n  options: QueryOptions = {}\n): QueryResult\u003cT\u003e {\n  const client = useEvoDb();\n  const [state, setState] = useState\u003cQueryState\u003cT\u003e\u003e({\n    loading: true,\n    data: undefined,\n    error: undefined,\n  });\n\n  // Suspense support\n  if (options.suspense) {\n    const result = use(client.query\u003cT\u003e(sql, params));\n    return { loading: false, data: result, error: undefined, refetch };\n  }\n\n  useEffect(() =\u003e {\n    let cancelled = false;\n    setState(s =\u003e ({ ...s, loading: true }));\n    \n    client.query\u003cT\u003e(sql, params)\n      .then(data =\u003e {\n        if (!cancelled) setState({ loading: false, data, error: undefined });\n      })\n      .catch(error =\u003e {\n        if (!cancelled) setState({ loading: false, data: undefined, error });\n      });\n    \n    return () =\u003e { cancelled = true; };\n  }, [sql, JSON.stringify(params)]);\n\n  return state;\n}\n\n// packages/react/src/useMutation.ts\nexport function useMutation\u003cT = unknown\u003e(\n  sql: string,\n  options: MutationOptions\u003cT\u003e = {}\n): MutationResult\u003cT\u003e {\n  const client = useEvoDb();\n  const [state, setState] = useState\u003cMutationState\u003cT\u003e\u003e({\n    loading: false,\n    data: undefined,\n    error: undefined,\n  });\n\n  const mutate = useCallback(async (params: unknown[] = []) =\u003e {\n    setState(s =\u003e ({ ...s, loading: true }));\n    try {\n      const data = await client.execute\u003cT\u003e(sql, params);\n      setState({ loading: false, data, error: undefined });\n      options.onSuccess?.(data);\n      return data;\n    } catch (error) {\n      setState({ loading: false, data: undefined, error: error as Error });\n      options.onError?.(error as Error);\n      throw error;\n    }\n  }, [sql, client]);\n\n  return { ...state, mutate };\n}\n\n// packages/react/src/useSubscription.ts\nexport function useSubscription(\n  table: string,\n  options: SubscriptionOptions = {}\n): SubscriptionResult {\n  const client = useEvoDb();\n  const [state, setState] = useState\u003cSubscriptionState\u003e({\n    connected: false,\n    lastEvent: undefined,\n    events: [],\n  });\n\n  useEffect(() =\u003e {\n    const subscription = client.subscribe(table, {\n      filter: options.filter,\n      onConnect: () =\u003e setState(s =\u003e ({ ...s, connected: true })),\n      onEvent: (event) =\u003e {\n        setState(s =\u003e ({\n          ...s,\n          lastEvent: event,\n          events: [...s.events, event],\n        }));\n        options.onEvent?.(event);\n      },\n      onError: options.onError,\n    });\n\n    return () =\u003e subscription.unsubscribe();\n  }, [table, JSON.stringify(options.filter)]);\n\n  return state;\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. Add Query Caching\n```typescript\n// packages/react/src/cache.ts\nexport class QueryCache {\n  private cache = new Map\u003cstring, CacheEntry\u003e();\n  private gcInterval: NodeJS.Timer;\n\n  constructor(private options: CacheOptions = {}) {\n    this.gcInterval = setInterval(() =\u003e this.gc(), options.gcInterval ?? 60000);\n  }\n\n  get\u003cT\u003e(key: string): T | undefined {\n    const entry = this.cache.get(key);\n    if (!entry) return undefined;\n    if (Date.now() \u003e entry.expiresAt) {\n      this.cache.delete(key);\n      return undefined;\n    }\n    return entry.data as T;\n  }\n\n  set\u003cT\u003e(key: string, data: T, ttl?: number): void {\n    this.cache.set(key, {\n      data,\n      expiresAt: Date.now() + (ttl ?? this.options.defaultTtl ?? 300000),\n    });\n  }\n\n  invalidate(pattern: string | RegExp): void {\n    for (const key of this.cache.keys()) {\n      if (typeof pattern === 'string' ? key.includes(pattern) : pattern.test(key)) {\n        this.cache.delete(key);\n      }\n    }\n  }\n}\n```\n\n#### 2. Add Optimistic Updates\n```typescript\n// packages/react/src/useMutation.ts\nexport function useMutation\u003cT = unknown\u003e(\n  sql: string,\n  options: MutationOptions\u003cT\u003e = {}\n): MutationResult\u003cT\u003e {\n  const client = useEvoDb();\n  const cache = useQueryCache();\n  \n  const mutate = useCallback(async (params: unknown[] = []) =\u003e {\n    // Apply optimistic update\n    let rollback: (() =\u003e void) | undefined;\n    if (options.optimisticUpdate) {\n      rollback = cache.applyOptimistic(\n        options.invalidateQueries ?? [],\n        options.optimisticUpdate(params)\n      );\n    }\n\n    try {\n      const data = await client.execute\u003cT\u003e(sql, params);\n      // Invalidate affected queries\n      if (options.invalidateQueries) {\n        for (const pattern of options.invalidateQueries) {\n          cache.invalidate(pattern);\n        }\n      }\n      options.onSuccess?.(data);\n      return data;\n    } catch (error) {\n      // Rollback optimistic update on error\n      rollback?.();\n      options.onError?.(error as Error);\n      throw error;\n    }\n  }, [sql, client, cache]);\n\n  return { ...state, mutate };\n}\n```\n\n#### 3. Add React Query-like API\n```typescript\n// packages/react/src/createQuery.ts\nexport function createQuery\u003cTInput, TOutput\u003e(\n  options: CreateQueryOptions\u003cTInput, TOutput\u003e\n) {\n  return function useCreatedQuery(input: TInput) {\n    const sql = options.sql(input);\n    const params = options.params?.(input) ?? [];\n    return useQuery\u003cTOutput\u003e(sql, params, {\n      staleTime: options.staleTime,\n      cacheTime: options.cacheTime,\n      suspense: options.suspense,\n    });\n  };\n}\n\n// Usage\nconst useUser = createQuery({\n  sql: (id: number) =\u003e 'SELECT * FROM users WHERE id = ?',\n  params: (id: number) =\u003e [id],\n  staleTime: 60000,\n});\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] Query caching with configurable TTL\n- [ ] Optimistic updates for mutations\n- [ ] Suspense mode support\n- [ ] TypeScript types fully exported\n- [ ] Bundle size \u003c 10KB gzipped\n- [ ] Documentation with examples\n\n## Labels\nreact, sdk, hooks, suspense, real-time","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:34.142865-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:58:19.640261-06:00","closed_at":"2026-01-21T19:58:19.640261-06:00","close_reason":"Closed"}
{"id":"evodb-26x","title":"Implement Lazy Loading for Lance Reader Components","description":"## Current State\nThe @evodb/lance-reader package loads all components eagerly:\n- IVF-PQ index implementation\n- HNSW index implementation\n- Arrow IPC reader\n- Protobuf parser\n- Storage adapters\n\nFor snippet constraints (32MB RAM, 5ms CPU), this is problematic:\n- Cold start loads unused code\n- Memory pressure from unused components\n\n## Proposed Improvement\n1. Implement dynamic imports for index types:\n```typescript\n// Current\nimport { IvfPqIndex } from './ivf-pq.js';\n\n// Proposed\nconst IvfPqIndex = await import('./ivf-pq.js').then(m =\u003e m.IvfPqIndex);\n```\n\n2. Create lazy factory functions:\n```typescript\nexport async function createVectorIndex(type: 'ivf-pq' | 'hnsw') {\n  if (type === 'ivf-pq') {\n    const { IvfPqIndex } = await import('./ivf-pq.js');\n    return new IvfPqIndex();\n  }\n  // ...\n}\n```\n\n3. Use conditional exports in package.json for tree-shaking:\n```json\n{\n  \"exports\": {\n    \"./ivf-pq\": { \"import\": \"./dist/ivf-pq.js\" },\n    \"./hnsw\": { \"import\": \"./dist/hnsw.js\" }\n  }\n}\n```\n\n## Migration Path\n- Non-breaking: Add lazy alternatives alongside eager exports\n- Deprecate eager imports in v0.2.0\n- Remove eager imports in v0.3.0\n\n## Edge Computing Impact\n- Faster cold starts (load only needed code)\n- Lower memory footprint for snippets\n- Better fit for 5ms CPU constraint","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:29.89016-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:26:37.754917-06:00","closed_at":"2026-01-21T20:26:37.754917-06:00","close_reason":"Closed","labels":["architecture","edge","lance","performance"]}
{"id":"evodb-2no","title":"Clean up deprecated APIs and add migration guide","description":"## Problem\n\nMultiple packages have deprecated APIs that need cleanup or removal:\n\n**@evodb/core**:\n- `core/src/storage.ts:18` - StorageProvider deprecated\n- `core/src/storage.ts:98` - StorageMetadata deprecated  \n- `core/src/storage.ts:110` - Storage interface deprecated\n- `core/src/schema.ts:37` - isSchemaCompatible deprecated\n- `core/src/types.ts:94-150` - Multiple type validation functions deprecated (SnapshotId, BatchId, WalId, SchemaId)\n- `core/src/circuit-breaker.ts:41,62,66` - Circuit breaker states deprecated\n\n**@evodb/query**:\n- `query/src/index.ts:541-565` - Multiple deprecated exports (SimpleQueryEngine alias, configs)\n- `query/src/types.ts:1034,1130` - QueryResult and QueryStats deprecated\n\n**@evodb/reader**:\n- `reader/src/index.ts:2-108` - Entire package deprecated in favor of @evodb/query\n\n**@evodb/lakehouse**:\n- `lakehouse/src/r2.ts:139` - createR2AdapterFromObjectStorage deprecated\n- `lakehouse/src/types.ts:465` - StorageProvider deprecated\n\n## Recommendation\n1. Create a migration guide for deprecated APIs\n2. Set a timeline for removal (e.g., next major version)\n3. Add console warnings for deprecated function usage in development\n4. Remove deprecated code in next major version\n\n## Acceptance Criteria\n- Migration guide document created\n- Deprecation warnings added to deprecated functions\n- Issue linked to epic for next major version cleanup","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:20.983279-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:52.618682-06:00","closed_at":"2026-01-21T20:18:52.618682-06:00","close_reason":"Closed","labels":["deprecation","documentation","tech-debt"]}
{"id":"evodb-2nr","title":"TDD: Fix unchecked array access patterns","description":"core/src/shred.ts:195-210 - Unsafe type casts without validation. Add proper type guards before assertions.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:08:53.025993-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T15:57:56.426478-06:00","closed_at":"2026-01-20T15:57:56.426478-06:00","close_reason":"Closed","external_ref":"gh-32"}
{"id":"evodb-2pw","title":"TDD: Split core package into submodules","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:40.132781-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.102451-06:00","closed_at":"2026-01-20T16:55:12.102451-06:00","close_reason":"Closed","external_ref":"gh-63"}
{"id":"evodb-2zo","title":"DOCS: API reference documentation","description":"## Problem\nREADME files provide overview but lack comprehensive API reference documentation.\n\n## Solution\nCreate API documentation for each package:\n1. TypeDoc/API Extractor for generated docs\n2. Host on GitHub Pages or dedicated docs site\n3. Include all public types, functions, and classes\n4. Add inline examples for each method\n\n## Packages Needing Docs\n- @evodb/core - EvoDB, QueryBuilder, shred/unshred, encode/decode\n- @evodb/lakehouse - Table, Snapshot, Manifest operations\n- @evodb/writer - LakehouseWriter, CDCBuffer\n- @evodb/reader - QueryEngine, Cache integration\n- @evodb/query - Zone maps, bloom filters, query planning\n\n## Impact\nEssential for production adoption - developers need reference docs","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:51.25846-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:17:51.198823-06:00","closed_at":"2026-01-21T20:17:51.198823-06:00","close_reason":"Closed"}
{"id":"evodb-385","title":"Writer atomicity unit tests","description":"TDD: Writer atomicity is untested. Add unit tests verifying atomic commit/rollback behavior under failure conditions.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:03.162136-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:54:46.022176-06:00","closed_at":"2026-01-20T18:54:46.022176-06:00","close_reason":"Closed","external_ref":"gh-129"}
{"id":"evodb-3ju","title":"Reduce branded types from 6 to 2-3 essential","description":"## Problem\n6 branded types (BlockId, SnapshotId, BatchId, WalId, SchemaId, TableId) with validators add ~20KB.\n\n## TDD Approach\n1. Audit usage of each branded type across codebase\n2. Keep only BlockId and TableId (most critical)\n3. Remove validators - trust TypeScript at compile time\n4. Update type definitions\n5. Verify tests pass\n\n## Expected Impact\n- ~15KB bundle reduction\n- Simpler type system\n\n## Keep\n- BlockId (critical for storage)\n- TableId (critical for queries)\n\n## Remove or simplify\n- SnapshotId → string\n- BatchId → string\n- WalId → string\n- SchemaId → number\n- All isValid*() validators","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:21.421674-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:37:34.460454-06:00","closed_at":"2026-01-21T12:37:34.460454-06:00","close_reason":"Closed"}
{"id":"evodb-3vb","title":"TDD: Integrate zone maps into query planner","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:53.835106-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.011284-06:00","closed_at":"2026-01-20T16:55:12.011284-06:00","close_reason":"Closed","external_ref":"gh-64"}
{"id":"evodb-3w2","title":"TDD: Replace Record\u003cstring,unknown\u003e with specific types","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:13.024695-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.156167-06:00","closed_at":"2026-01-20T16:49:22.156167-06:00","close_reason":"Closed"}
{"id":"evodb-3zm","title":"Simplify query-engine-selector.ts to basic heuristics","description":"## Problem\nquery-engine-selector.ts (497 lines) implements complex query analysis that could be 100 lines.\n\n## TDD Approach\n1. Write tests for essential selection logic\n2. Replace complexity analysis with simple heuristics\n3. Remove enum-based complexity levels\n4. Verify correct engine selection\n\n## Expected Impact\n- ~400 lines removed\n- ~10KB bundle reduction\n\n## Simplify From\n```typescript\nanalyzeQueryComplexity() // Complex scoring\nwouldBenefitFromZoneMaps() // Detailed analysis\nwouldBenefitFromBloomFilters() // More analysis\n```\n\n## Simplify To\n```typescript\nfunction selectEngine(query) {\n  if (query.predicates.length \u003e 5) return 'query';\n  if (query.aggregates.length \u003e 0) return 'query';\n  return 'reader';\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:30.746023-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:27:45.677596-06:00","closed_at":"2026-01-21T12:27:45.677596-06:00","close_reason":"Closed"}
{"id":"evodb-409","title":"TDD: Add key prefix path traversal validation","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:38.030559-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.664236-06:00","closed_at":"2026-01-20T17:49:16.664236-06:00","close_reason":"Closed","external_ref":"gh-105"}
{"id":"evodb-40of","title":"TDD: Implement secondary index support","description":"## Overview\n\nImplement secondary index support to enable efficient queries on non-primary-key fields. This includes B-tree indexes, compound indexes, and partial indexes for optimized query performance.\n\n## TDD Red-Green-Refactor Cycle\n\n### RED Phase: Write Failing Tests First\n\n```typescript\n// tests/indexes/secondary-index.test.ts\nimport { describe, it, expect, beforeEach, afterEach, vi } from 'vitest'\nimport { EvoDB } from '../src/evodb'\n\ndescribe('Secondary Index Support', () =\u003e {\n  let db: EvoDB\n\n  beforeEach(async () =\u003e {\n    db = new EvoDB({ name: 'test-indexes' })\n    await db.clear()\n  })\n\n  afterEach(async () =\u003e {\n    await db.close()\n  })\n\n  describe('createIndex', () =\u003e {\n    it('should create a secondary index on a field', async () =\u003e {\n      const result = await db.createIndex('users', 'email')\n      \n      expect(result).toEqual({\n        name: 'users_email_idx',\n        field: 'email',\n        collection: 'users',\n        type: 'btree',\n        unique: false\n      })\n    })\n\n    it('should create a unique index', async () =\u003e {\n      const result = await db.createIndex('users', 'email', { unique: true })\n      \n      expect(result.unique).toBe(true)\n    })\n\n    it('should throw if unique index on non-unique data', async () =\u003e {\n      await db.put('users:1', { email: 'test@example.com' })\n      await db.put('users:2', { email: 'test@example.com' })\n      \n      await expect(\n        db.createIndex('users', 'email', { unique: true })\n      ).rejects.toThrow('Duplicate values found for unique index')\n    })\n\n    it('should build index from existing data', async () =\u003e {\n      await db.put('users:1', { email: 'alice@example.com', name: 'Alice' })\n      await db.put('users:2', { email: 'bob@example.com', name: 'Bob' })\n      await db.put('users:3', { email: 'charlie@example.com', name: 'Charlie' })\n      \n      await db.createIndex('users', 'email')\n      \n      const indexStats = await db.getIndexStats('users_email_idx')\n      expect(indexStats.entries).toBe(3)\n    })\n\n    it('should support custom index names', async () =\u003e {\n      const result = await db.createIndex('users', 'email', { \n        name: 'idx_user_emails' \n      })\n      \n      expect(result.name).toBe('idx_user_emails')\n    })\n\n    it('should list all indexes', async () =\u003e {\n      await db.createIndex('users', 'email')\n      await db.createIndex('users', 'name')\n      await db.createIndex('posts', 'authorId')\n      \n      const indexes = await db.listIndexes()\n      \n      expect(indexes).toHaveLength(3)\n      expect(indexes.map(i =\u003e i.name)).toContain('users_email_idx')\n      expect(indexes.map(i =\u003e i.name)).toContain('users_name_idx')\n      expect(indexes.map(i =\u003e i.name)).toContain('posts_authorId_idx')\n    })\n\n    it('should drop an index', async () =\u003e {\n      await db.createIndex('users', 'email')\n      await db.dropIndex('users_email_idx')\n      \n      const indexes = await db.listIndexes()\n      expect(indexes).toHaveLength(0)\n    })\n  })\n\n  describe('Query Using Index', () =\u003e {\n    beforeEach(async () =\u003e {\n      // Seed data\n      for (let i = 0; i \u003c 1000; i++) {\n        await db.put(`users:${i}`, {\n          email: `user${i}@example.com`,\n          age: 20 + (i % 50),\n          status: i % 2 === 0 ? 'active' : 'inactive'\n        })\n      }\n      \n      await db.createIndex('users', 'email')\n      await db.createIndex('users', 'age')\n      await db.createIndex('users', 'status')\n    })\n\n    it('should use index for equality queries', async () =\u003e {\n      const result = await db.query({\n        collection: 'users',\n        where: { email: 'user500@example.com' }\n      })\n      \n      expect(result.data).toHaveLength(1)\n      expect(result.data[0].email).toBe('user500@example.com')\n      expect(result.queryPlan.usedIndex).toBe('users_email_idx')\n    })\n\n    it('should use index for range queries', async () =\u003e {\n      const result = await db.query({\n        collection: 'users',\n        where: { age: { $gte: 45, $lt: 50 } }\n      })\n      \n      expect(result.data.length).toBeGreaterThan(0)\n      expect(result.queryPlan.usedIndex).toBe('users_age_idx')\n      expect(result.data.every(u =\u003e u.age \u003e= 45 \u0026\u0026 u.age \u003c 50)).toBe(true)\n    })\n\n    it('should use index for IN queries', async () =\u003e {\n      const result = await db.query({\n        collection: 'users',\n        where: { status: { $in: ['active'] } }\n      })\n      \n      expect(result.queryPlan.usedIndex).toBe('users_status_idx')\n      expect(result.data).toHaveLength(500)\n    })\n\n    it('should be faster with index than full scan', async () =\u003e {\n      // Query with index\n      const start1 = performance.now()\n      await db.query({\n        collection: 'users',\n        where: { email: 'user500@example.com' }\n      })\n      const indexTime = performance.now() - start1\n\n      // Drop index and query without\n      await db.dropIndex('users_email_idx')\n      \n      const start2 = performance.now()\n      await db.query({\n        collection: 'users',\n        where: { email: 'user500@example.com' }\n      })\n      const scanTime = performance.now() - start2\n\n      expect(indexTime).toBeLessThan(scanTime)\n    })\n\n    it('should explain query plan', async () =\u003e {\n      const plan = await db.explain({\n        collection: 'users',\n        where: { email: 'user500@example.com' }\n      })\n      \n      expect(plan).toEqual({\n        type: 'index_scan',\n        index: 'users_email_idx',\n        bounds: { email: ['user500@example.com', 'user500@example.com'] },\n        estimatedCost: expect.any(Number),\n        estimatedRows: expect.any(Number)\n      })\n    })\n\n    it('should choose best index when multiple available', async () =\u003e {\n      // Age has lower cardinality than email\n      const plan = await db.explain({\n        collection: 'users',\n        where: { \n          email: 'user500@example.com',\n          age: 30 \n        }\n      })\n      \n      // Should choose email (higher cardinality = more selective)\n      expect(plan.index).toBe('users_email_idx')\n    })\n  })\n\n  describe('Index Maintenance', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.createIndex('users', 'email')\n    })\n\n    it('should update index on insert', async () =\u003e {\n      await db.put('users:new', { email: 'new@example.com' })\n      \n      const result = await db.query({\n        collection: 'users',\n        where: { email: 'new@example.com' }\n      })\n      \n      expect(result.data).toHaveLength(1)\n      expect(result.queryPlan.usedIndex).toBe('users_email_idx')\n    })\n\n    it('should update index on update', async () =\u003e {\n      await db.put('users:1', { email: 'old@example.com' })\n      await db.put('users:1', { email: 'updated@example.com' })\n      \n      // Old value should not be found\n      const oldResult = await db.query({\n        collection: 'users',\n        where: { email: 'old@example.com' }\n      })\n      expect(oldResult.data).toHaveLength(0)\n      \n      // New value should be found\n      const newResult = await db.query({\n        collection: 'users',\n        where: { email: 'updated@example.com' }\n      })\n      expect(newResult.data).toHaveLength(1)\n    })\n\n    it('should update index on delete', async () =\u003e {\n      await db.put('users:1', { email: 'delete@example.com' })\n      await db.delete('users:1')\n      \n      const result = await db.query({\n        collection: 'users',\n        where: { email: 'delete@example.com' }\n      })\n      \n      expect(result.data).toHaveLength(0)\n    })\n\n    it('should enforce unique constraint on update', async () =\u003e {\n      await db.createIndex('users', 'username', { unique: true })\n      await db.put('users:1', { username: 'alice' })\n      await db.put('users:2', { username: 'bob' })\n      \n      await expect(\n        db.put('users:2', { username: 'alice' })\n      ).rejects.toThrow('Unique constraint violation')\n    })\n\n    it('should handle null values in index', async () =\u003e {\n      await db.put('users:1', { email: null })\n      await db.put('users:2', { email: 'test@example.com' })\n      await db.put('users:3', { /* no email field */ })\n      \n      const nullResult = await db.query({\n        collection: 'users',\n        where: { email: null }\n      })\n      \n      expect(nullResult.data).toHaveLength(2) // null and missing\n    })\n  })\n\n  describe('Compound Indexes', () =\u003e {\n    it('should create compound index', async () =\u003e {\n      const result = await db.createIndex('users', ['lastName', 'firstName'])\n      \n      expect(result).toEqual({\n        name: 'users_lastName_firstName_idx',\n        fields: ['lastName', 'firstName'],\n        collection: 'users',\n        type: 'btree',\n        unique: false\n      })\n    })\n\n    it('should use compound index for prefix queries', async () =\u003e {\n      await db.put('users:1', { lastName: 'Smith', firstName: 'John', age: 30 })\n      await db.put('users:2', { lastName: 'Smith', firstName: 'Jane', age: 25 })\n      await db.put('users:3', { lastName: 'Doe', firstName: 'John', age: 35 })\n      \n      await db.createIndex('users', ['lastName', 'firstName'])\n      \n      // Query on prefix (lastName only) should use index\n      const result = await db.query({\n        collection: 'users',\n        where: { lastName: 'Smith' }\n      })\n      \n      expect(result.data).toHaveLength(2)\n      expect(result.queryPlan.usedIndex).toBe('users_lastName_firstName_idx')\n    })\n\n    it('should use compound index for full match', async () =\u003e {\n      await db.put('users:1', { lastName: 'Smith', firstName: 'John' })\n      await db.put('users:2', { lastName: 'Smith', firstName: 'Jane' })\n      \n      await db.createIndex('users', ['lastName', 'firstName'])\n      \n      const result = await db.query({\n        collection: 'users',\n        where: { lastName: 'Smith', firstName: 'John' }\n      })\n      \n      expect(result.data).toHaveLength(1)\n      expect(result.queryPlan.usedIndex).toBe('users_lastName_firstName_idx')\n    })\n\n    it('should NOT use compound index for non-prefix query', async () =\u003e {\n      await db.createIndex('users', ['lastName', 'firstName'])\n      await db.put('users:1', { lastName: 'Smith', firstName: 'John' })\n      \n      // Query on firstName only (not a prefix) - should not use compound index\n      const result = await db.query({\n        collection: 'users',\n        where: { firstName: 'John' }\n      })\n      \n      expect(result.queryPlan.usedIndex).toBeUndefined()\n      expect(result.queryPlan.type).toBe('full_scan')\n    })\n\n    it('should support unique compound index', async () =\u003e {\n      await db.createIndex('users', ['email', 'tenantId'], { unique: true })\n      \n      await db.put('users:1', { email: 'test@example.com', tenantId: 'tenant1' })\n      await db.put('users:2', { email: 'test@example.com', tenantId: 'tenant2' }) // OK\n      \n      await expect(\n        db.put('users:3', { email: 'test@example.com', tenantId: 'tenant1' })\n      ).rejects.toThrow('Unique constraint violation')\n    })\n  })\n\n  describe('Partial Indexes', () =\u003e {\n    it('should create partial index with filter', async () =\u003e {\n      const result = await db.createIndex('users', 'email', {\n        partial: { where: { status: 'active' } }\n      })\n      \n      expect(result.partial).toEqual({ where: { status: 'active' } })\n    })\n\n    it('should only index matching documents', async () =\u003e {\n      await db.put('users:1', { email: 'active@example.com', status: 'active' })\n      await db.put('users:2', { email: 'inactive@example.com', status: 'inactive' })\n      \n      await db.createIndex('users', 'email', {\n        partial: { where: { status: 'active' } }\n      })\n      \n      const stats = await db.getIndexStats('users_email_idx')\n      expect(stats.entries).toBe(1) // Only active user\n    })\n\n    it('should use partial index for matching queries', async () =\u003e {\n      await db.put('users:1', { email: 'active@example.com', status: 'active' })\n      \n      await db.createIndex('users', 'email', {\n        partial: { where: { status: 'active' } }\n      })\n      \n      // Query that matches partial index filter\n      const result = await db.query({\n        collection: 'users',\n        where: { email: 'active@example.com', status: 'active' }\n      })\n      \n      expect(result.queryPlan.usedIndex).toBe('users_email_idx')\n    })\n\n    it('should NOT use partial index for non-matching queries', async () =\u003e {\n      await db.createIndex('users', 'email', {\n        partial: { where: { status: 'active' } }\n      })\n      \n      // Query without status filter - cannot use partial index\n      const result = await db.query({\n        collection: 'users',\n        where: { email: 'test@example.com' }\n      })\n      \n      expect(result.queryPlan.usedIndex).toBeUndefined()\n    })\n\n    it('should maintain partial index correctly', async () =\u003e {\n      await db.createIndex('users', 'email', {\n        partial: { where: { status: 'active' } }\n      })\n      \n      // Insert inactive user\n      await db.put('users:1', { email: 'test@example.com', status: 'inactive' })\n      \n      let stats = await db.getIndexStats('users_email_idx')\n      expect(stats.entries).toBe(0)\n      \n      // Update to active - should add to index\n      await db.put('users:1', { email: 'test@example.com', status: 'active' })\n      \n      stats = await db.getIndexStats('users_email_idx')\n      expect(stats.entries).toBe(1)\n      \n      // Update back to inactive - should remove from index\n      await db.put('users:1', { email: 'test@example.com', status: 'inactive' })\n      \n      stats = await db.getIndexStats('users_email_idx')\n      expect(stats.entries).toBe(0)\n    })\n  })\n\n  describe('Index Types', () =\u003e {\n    it('should support hash index for equality-only queries', async () =\u003e {\n      const result = await db.createIndex('users', 'id', { type: 'hash' })\n      \n      expect(result.type).toBe('hash')\n    })\n\n    it('should throw when using hash index for range query', async () =\u003e {\n      await db.createIndex('users', 'age', { type: 'hash' })\n      await db.put('users:1', { age: 30 })\n      \n      const result = await db.query({\n        collection: 'users',\n        where: { age: { $gt: 25 } }\n      })\n      \n      // Should fall back to scan, not use hash index\n      expect(result.queryPlan.usedIndex).toBeUndefined()\n    })\n  })\n})\n```\n\n### GREEN Phase: Minimal Implementation to Pass Tests\n\n```typescript\n// src/indexes/index-manager.ts\nexport type IndexType = 'btree' | 'hash'\n\nexport interface IndexDefinition {\n  name: string\n  collection: string\n  fields: string[]\n  type: IndexType\n  unique: boolean\n  partial?: { where: Record\u003cstring, any\u003e }\n}\n\nexport interface IndexStats {\n  name: string\n  entries: number\n  size: number\n  depth: number\n}\n\nexport class IndexManager {\n  private indexes: Map\u003cstring, Index\u003e = new Map()\n  \n  async createIndex(\n    collection: string,\n    fields: string | string[],\n    options: {\n      name?: string\n      unique?: boolean\n      type?: IndexType\n      partial?: { where: Record\u003cstring, any\u003e }\n    } = {}\n  ): Promise\u003cIndexDefinition\u003e {\n    const fieldArray = Array.isArray(fields) ? fields : [fields]\n    const name = options.name || `${collection}_${fieldArray.join('_')}_idx`\n    \n    const definition: IndexDefinition = {\n      name,\n      collection,\n      fields: fieldArray,\n      type: options.type || 'btree',\n      unique: options.unique || false,\n      partial: options.partial\n    }\n    \n    // Check for duplicate values if unique\n    if (definition.unique) {\n      await this.validateUniqueness(collection, fieldArray)\n    }\n    \n    // Create index structure\n    const index = definition.type === 'btree' \n      ? new BTreeIndex(definition)\n      : new HashIndex(definition)\n    \n    // Build from existing data\n    await this.buildIndex(index, collection)\n    \n    this.indexes.set(name, index)\n    \n    return definition\n  }\n\n  async dropIndex(name: string): Promise\u003cvoid\u003e {\n    this.indexes.delete(name)\n  }\n\n  listIndexes(): IndexDefinition[] {\n    return Array.from(this.indexes.values()).map(i =\u003e i.definition)\n  }\n\n  getIndexStats(name: string): IndexStats {\n    const index = this.indexes.get(name)\n    if (!index) throw new Error(`Index ${name} not found`)\n    return index.getStats()\n  }\n\n  chooseIndex(collection: string, query: QueryConditions): Index | null {\n    const candidates = Array.from(this.indexes.values())\n      .filter(i =\u003e i.definition.collection === collection)\n      .filter(i =\u003e this.canUseIndex(i, query))\n    \n    if (candidates.length === 0) return null\n    \n    // Choose most selective index (highest cardinality for equality)\n    return candidates.reduce((best, current) =\u003e {\n      const bestScore = this.scoreIndex(best, query)\n      const currentScore = this.scoreIndex(current, query)\n      return currentScore \u003e bestScore ? current : best\n    })\n  }\n\n  private canUseIndex(index: Index, query: QueryConditions): boolean {\n    const { fields, type, partial } = index.definition\n    \n    // Check if query uses index fields (prefix for compound)\n    const queryFields = Object.keys(query)\n    const usesPrefix = fields[0] \u0026\u0026 queryFields.includes(fields[0])\n    \n    if (!usesPrefix) return false\n    \n    // Hash indexes only support equality\n    if (type === 'hash') {\n      const condition = query[fields[0]]\n      if (typeof condition === 'object' \u0026\u0026 condition !== null) {\n        return false // Range query\n      }\n    }\n    \n    // Check partial index filter\n    if (partial) {\n      if (!this.queryMatchesPartialFilter(query, partial.where)) {\n        return false\n      }\n    }\n    \n    return true\n  }\n\n  async onDataChange(\n    collection: string, \n    key: string, \n    oldValue: any, \n    newValue: any\n  ): Promise\u003cvoid\u003e {\n    for (const index of this.indexes.values()) {\n      if (index.definition.collection !== collection) continue\n      \n      // Check partial filter\n      const { partial } = index.definition\n      const oldMatches = !partial || this.matchesFilter(oldValue, partial.where)\n      const newMatches = !partial || this.matchesFilter(newValue, partial.where)\n      \n      if (oldMatches \u0026\u0026 oldValue) {\n        await index.remove(this.getIndexKey(oldValue, index.definition.fields), key)\n      }\n      \n      if (newMatches \u0026\u0026 newValue) {\n        if (index.definition.unique) {\n          const existing = await index.find(this.getIndexKey(newValue, index.definition.fields))\n          if (existing \u0026\u0026 existing !== key) {\n            throw new Error('Unique constraint violation')\n          }\n        }\n        await index.insert(this.getIndexKey(newValue, index.definition.fields), key)\n      }\n    }\n  }\n}\n\n// B-Tree Index implementation\nclass BTreeIndex implements Index {\n  private tree: BTree\u003cstring, Set\u003cstring\u003e\u003e = new BTree()\n  \n  constructor(public definition: IndexDefinition) {}\n\n  async insert(indexKey: string, docKey: string): Promise\u003cvoid\u003e {\n    let keys = this.tree.get(indexKey)\n    if (!keys) {\n      keys = new Set()\n      this.tree.set(indexKey, keys)\n    }\n    keys.add(docKey)\n  }\n\n  async remove(indexKey: string, docKey: string): Promise\u003cvoid\u003e {\n    const keys = this.tree.get(indexKey)\n    if (keys) {\n      keys.delete(docKey)\n      if (keys.size === 0) {\n        this.tree.delete(indexKey)\n      }\n    }\n  }\n\n  async find(indexKey: string): Promise\u003cstring | undefined\u003e {\n    const keys = this.tree.get(indexKey)\n    return keys?.values().next().value\n  }\n\n  async range(start: string, end: string): Promise\u003cstring[]\u003e {\n    const results: string[] = []\n    for (const [key, docKeys] of this.tree.entries(start, end)) {\n      results.push(...docKeys)\n    }\n    return results\n  }\n\n  getStats(): IndexStats {\n    return {\n      name: this.definition.name,\n      entries: this.countEntries(),\n      size: this.estimateSize(),\n      depth: this.tree.depth\n    }\n  }\n}\n```\n\n### REFACTOR Phase: Clean Up and Optimize\n\n1. **Optimize B-Tree Implementation**\n   ```typescript\n   // Use efficient B-tree library or implement with:\n   // - Bulk loading for index creation\n   // - Node caching for frequently accessed paths\n   // - Prefix compression for string keys\n   ```\n\n2. **Add Index-Only Scans**\n   ```typescript\n   // Return data directly from index when all needed fields are indexed\n   interface CoveringIndex {\n     includes: string[] // Additional fields stored in index\n   }\n   ```\n\n3. **Implement Index Statistics Collection**\n   ```typescript\n   class IndexStatisticsCollector {\n     // Track cardinality, distribution, null counts\n     // Update stats incrementally on changes\n     // Use for query optimizer decisions\n   }\n   ```\n\n4. **Add Background Index Building**\n   ```typescript\n   async createIndexAsync(\n     collection: string,\n     fields: string[],\n     options: IndexOptions\n   ): Promise\u003cIndexBuildProgress\u003e {\n     // Build index without blocking writes\n     // Report progress\n     // Handle concurrent modifications\n   }\n   ```\n\n## Acceptance Criteria\n\n- [ ] All RED phase tests pass\n- [ ] createIndex, dropIndex, listIndexes work correctly\n- [ ] Queries use indexes when applicable\n- [ ] Query planner chooses best index\n- [ ] Index maintenance on insert/update/delete works\n- [ ] Unique constraints enforced\n- [ ] Compound indexes support prefix queries\n- [ ] Partial indexes filter correctly\n- [ ] Performance improvement measurable\n\n## Dependencies\n\n- B-tree data structure implementation\n- Query planner integration\n- Change notification system for maintenance","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:39:27.71664-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:53:53.17739-06:00","closed_at":"2026-01-21T19:53:53.17739-06:00","close_reason":"Closed"}
{"id":"evodb-43e4","title":"Add Dependency Injection Container for Testing","description":"## Current State\nDependencies are created inline, making testing difficult:\n\n```typescript\n// QueryEngine constructor\nconstructor(config: QueryEngineConfig) {\n  this.planner = new QueryPlanner(config);           // Hard to mock\n  this.zoneMapOptimizer = new ZoneMapOptimizer();   // Hard to mock\n  this.bloomFilterManager = new BloomFilterManager(); // Hard to mock\n  this.cacheManager = new CacheManager(config);      // Hard to mock\n  // ...\n}\n```\n\nTest files have to mock at the class level or use real implementations.\n\n## Proposed Improvement\n1. Create simple DI container (no external deps):\n```typescript\ninterface Container {\n  register\u003cT\u003e(key: symbol, factory: () =\u003e T): void;\n  resolve\u003cT\u003e(key: symbol): T;\n  createScope(): Container;\n}\n```\n\n2. Define service keys:\n```typescript\nexport const QUERY_PLANNER = Symbol('QueryPlanner');\nexport const ZONE_MAP_OPTIMIZER = Symbol('ZoneMapOptimizer');\nexport const CACHE_MANAGER = Symbol('CacheManager');\n```\n\n3. Inject via container:\n```typescript\nconstructor(config: QueryEngineConfig, container?: Container) {\n  const c = container ?? defaultContainer;\n  this.planner = c.resolve(QUERY_PLANNER);\n  this.cacheManager = c.resolve(CACHE_MANAGER);\n}\n```\n\n4. In tests, override specific services:\n```typescript\nconst testContainer = createContainer();\ntestContainer.register(CACHE_MANAGER, () =\u003e new MockCacheManager());\nconst engine = new QueryEngine(config, testContainer);\n```\n\n## Alternative: Config-based injection\n```typescript\ninterface QueryEngineConfig {\n  // ... existing\n  _internal?: {\n    planner?: QueryPlanner;\n    cacheManager?: CacheManager;\n  };\n}\n```\n\n## Migration Path\n- Non-breaking: Add optional container parameter\n- Update tests to use container\n- Document testing patterns\n\n## Benefits\n- Easier unit testing\n- Better isolation in tests\n- Clearer dependency graph","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:31.45349-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:10:14.924505-06:00","closed_at":"2026-01-22T03:10:14.924505-06:00","close_reason":"Closed","labels":["DI","architecture","testing"]}
{"id":"evodb-47z","title":"TDD: Enable vitest coverage reporting","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:18.127375-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:04:55.100167-06:00","closed_at":"2026-01-20T14:04:55.100167-06:00","close_reason":"Closed"}
{"id":"evodb-48n6","title":"Improve query engine memory tracking granularity","description":"## Problem\n\nThe memory tracking in `query/src/engine.ts` is good but could be more granular:\n\n**Current Implementation** (lines 368-519):\n```typescript\nclass MemoryTracker {\n  private currentBytes: number = 0;\n  private peakBytes: number = 0;\n  private readonly limitBytes: number;\n  // ...\n}\n```\n\n**Improvement opportunities**:\n\n1. **Per-operation tracking**:\n   - Currently tracks total memory but not per-operation\n   - Would help identify which operations use most memory\n   - e.g., filtering vs sorting vs aggregation\n\n2. **Memory estimation accuracy**:\n   - `estimateMemorySize` uses rough approximations\n   - Line 387: `return 40 + value.length * 2;` for strings\n   - Could be more accurate for V8's actual string storage\n\n3. **Streaming result memory**:\n   - `StreamingQueryResult` doesn't track memory as accurately\n   - Line 749-764: Returns 0 for `peakMemoryBytes`\n\n4. **Memory warnings before limit**:\n   - Only throws when limit exceeded\n   - Could emit warning at 80% of limit\n\n## Recommendation\n1. Add per-phase memory tracking (scan, filter, sort, aggregate)\n2. Emit memory warnings at configurable threshold\n3. Track memory in streaming results\n4. Add memory breakdown to QueryStats\n\n## Impact\nBetter memory debugging and capacity planning for large queries.\n\n## Files\n- `query/src/engine.ts:368-519` - MemoryTracker class\n- `query/src/types.ts` - QueryStats interface","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:26.347788-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:30:24.875453-06:00","closed_at":"2026-01-21T20:30:24.875453-06:00","close_reason":"Closed","labels":["observability","performance","query-engine"]}
{"id":"evodb-4dt","title":"TDD: Fix string intern pool memory leak","description":"core/src/encode.ts:14-25 - Pool clears entirely at 10K causing cache thrashing. Implement LRU eviction.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:08:53.880357-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T15:57:56.39865-06:00","closed_at":"2026-01-20T15:57:56.39865-06:00","close_reason":"Closed","external_ref":"gh-33"}
{"id":"evodb-4ll","title":"TDD: Type R2 mock options properly","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:03.110663-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:13:12.500902-06:00","closed_at":"2026-01-20T14:13:12.500902-06:00","close_reason":"Closed"}
{"id":"evodb-4lxv","title":"PRODUCT: GraphQL adapter","description":"## Problem\nREST/JSON API is implied but many teams prefer GraphQL.\n\n## Solution\nCreate GraphQL adapter:\n```typescript\nimport { createGraphQLHandler } from '@evodb/graphql';\n\nconst handler = createGraphQLHandler({\n  db: evodb,\n  tables: ['users', 'posts', 'comments']\n});\n\n// Generates schema from EvoDB tables\n// type User {\n//   id: ID!\n//   name: String!\n//   posts: [Post!]!\n// }\n\nexport default {\n  fetch: (req) =\u003e handler(req)\n};\n```\n\n## Features\n- Auto-generate GraphQL schema from EvoDB schemas\n- Support for relationships\n- Subscriptions via CDC\n- DataLoader for N+1 prevention\n\n## Impact\nEnables GraphQL-first development patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:10.482505-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:31:56.707187-06:00","closed_at":"2026-01-21T20:31:56.707187-06:00","close_reason":"Closed"}
{"id":"evodb-4md","title":"TDD: Add structured logging framework","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:50.534258-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:23:01.113211-06:00","closed_at":"2026-01-20T14:23:01.113211-06:00","close_reason":"Closed"}
{"id":"evodb-4p7","title":"TDD: Fix RPC pendingBatches unbounded Map growth","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:08.059976-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:27:33.838894-06:00","closed_at":"2026-01-20T17:27:33.838894-06:00","close_reason":"Closed"}
{"id":"evodb-4rv","title":"TDD: Extract mock data from production code","description":"query/src/engine.ts:1220-1445 - 1000+ lines of mock data in production QueryEngine. Move to test fixtures.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:03.15985-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.485903-06:00","closed_at":"2026-01-20T16:23:39.485903-06:00","close_reason":"Closed","external_ref":"gh-34"}
{"id":"evodb-4s8","title":"Rename tests to follow .unit.test.ts convention","description":"TDD: 90% of tests don't follow naming convention. Rename test files to use .unit.test.ts or .integration.test.ts suffixes for proper stratification.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:05.345086-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:54:46.036661-06:00","closed_at":"2026-01-20T18:54:46.036661-06:00","close_reason":"Closed","external_ref":"gh-130"}
{"id":"evodb-4tq","title":"TDD: Add observability metrics export (Prometheus)","status":"closed","priority":0,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:10.511699-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:27:33.896926-06:00","closed_at":"2026-01-20T17:27:33.896926-06:00","close_reason":"Closed"}
{"id":"evodb-4v3","title":"Type assertions in encoding need runtime validation","description":"TDD: encode.ts uses type assertions without runtime validation. Add type guards to validate data types at runtime before encoding.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:17:58.777731-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:23:12.623215-06:00","closed_at":"2026-01-20T18:23:12.623218-06:00"}
{"id":"evodb-505","title":"TDD: Add atomic CDC buffer LSN updates","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:31.023636-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:36:42.909765-06:00","closed_at":"2026-01-20T17:36:42.909765-06:00","close_reason":"Closed"}
{"id":"evodb-58tn","title":"PRODUCT: Backup and disaster recovery","description":"## Problem\nTime-travel snapshots exist but no formal backup/restore procedures.\n\n## Solution\nAdd backup utilities:\n```typescript\nimport { backup, restore, PointInTimeRecovery } from '@evodb/backup';\n\n// Full backup to external storage\nawait backup(db, {\n  destination: 's3://my-bucket/backups',\n  compression: 'zstd',\n  encryption: { key: process.env.BACKUP_KEY }\n});\n\n// Point-in-time recovery\nconst pitr = new PointInTimeRecovery(db);\nawait pitr.restoreTo(new Date('2024-01-15T10:30:00Z'));\n\n// Incremental backups\nawait backup(db, {\n  type: 'incremental',\n  since: lastBackupId\n});\n```\n\n## CLI Support\n```bash\nnpx evodb backup --destination s3://backups\nnpx evodb restore --from s3://backups/2024-01-15\n```\n\n## Impact\nEssential for production data protection","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:27.176136-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.089046-06:00","closed_at":"2026-01-21T20:14:08.089046-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-5ab0","title":"PRODUCT: Indexing and secondary indexes","description":"## Problem\nREADME mentions schema evolution but no explicit index management. Users cannot create secondary indexes for performance.\n\n## Solution\nAdd index management API:\n```typescript\n// Create indexes\nawait db.schema.createIndex('users', {\n  name: 'idx_users_email',\n  columns: ['email'],\n  unique: true\n});\n\n// Composite indexes\nawait db.schema.createIndex('orders', {\n  name: 'idx_orders_user_date',\n  columns: ['userId', 'createdAt']\n});\n\n// Text search index\nawait db.schema.createIndex('posts', {\n  name: 'idx_posts_content',\n  columns: ['title', 'body'],\n  type: 'fulltext'\n});\n```\n\n## Impact\nCritical for query performance at scale","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:14.215745-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.864419-06:00","closed_at":"2026-01-21T20:08:17.864419-06:00","close_reason":"Already implemented"}
{"id":"evodb-5hqd","title":"TDD: Add E2E tests for CDC pipeline","description":"## Problem\nThe E2E test suite only has 3 tests covering query-lakehouse integration and RPC protocol. There are no E2E tests for the critical CDC (Change Data Capture) pipeline that flows from writer -\u003e lakehouse -\u003e query.\n\n## Coverage Gap\n- No E2E tests for: Writer CDC ingestion -\u003e Flush -\u003e Lakehouse manifest update -\u003e Query reads new data\n- No tests for CDC with schema evolution\n- No tests for partition pruning with CDC-written data\n\n## Acceptance Criteria\n- [ ] Create `cdc-pipeline.e2e.test.ts`\n- [ ] Test complete CDC flow: ingest -\u003e buffer -\u003e flush -\u003e query\n- [ ] Test schema evolution through CDC pipeline\n- [ ] Test partition pruning works with CDC-written partitions\n- [ ] Test time-travel queries across CDC snapshots\n- [ ] Test concurrent CDC from multiple sources\n\n## TDD Approach\nDefine integration scenarios:\n1. Single-source CDC to query roundtrip\n2. Multi-source CDC with merge semantics\n3. CDC with schema changes mid-stream\n4. Query performance with many CDC-written blocks","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:09.090075-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:52:38.029265-06:00","closed_at":"2026-01-21T19:52:38.029265-06:00","close_reason":"Closed","labels":["cdc","e2e","tdd","testing"]}
{"id":"evodb-5iz","title":"TDD: Fix lance-reader 4 failing filter tests","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:12.907654-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:11.918844-06:00","closed_at":"2026-01-20T16:55:11.918844-06:00","close_reason":"Closed","external_ref":"gh-69"}
{"id":"evodb-5nm","title":"Standardize magic numbers as named constants","description":"## Problem\n\nMagic numbers are scattered throughout the codebase. While some are documented, others lack context:\n\n**Examples found**:\n\n1. **`writer/src/buffer.ts`**:\n- Line 617: `(stats.entryCount / 10000) * 50` - backpressure calculation\n- Line 618: `(stats.estimatedSize / (4 * 1024 * 1024)) * 30` - 4MB threshold\n- Line 619: `(pendingBlockCount / 10) * 20` - pending block threshold\n- Line 656: `10 + (990 * ratio)` - delay calculation\n\n2. **`query/src/engine.ts`**:\n- Line 341: `MAX_COLUMN_NAME_LENGTH = 256` (good - named constant)\n- Line 219: `Math.floor(obj.size / 100)` - 100 bytes per row estimate\n- Line 1229: `DEFAULT_MAX_CACHE_ENTRIES = 1000` (good - named constant)\n\n3. **`core/src/encode.ts`**:\n- Various buffer size constants\n- Checksum positions\n\n4. **`lakehouse/src/partition.ts`**:\n- Line 796: `indexThreshold = 50` - partition index threshold\n\n## Recommendation\n1. Extract all magic numbers to named constants at module level\n2. Group related constants together\n3. Add JSDoc explaining the rationale for each value\n4. Consider making some configurable\n\n## Acceptance Criteria\n- All magic numbers replaced with named constants\n- Constants have JSDoc explaining the value\n- Constants are grouped logically","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:00.200167-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:10:19.906205-06:00","closed_at":"2026-01-22T03:10:19.906205-06:00","close_reason":"Standardized magic numbers as named constants in @evodb/core. Added 15 new named constants for backpressure control, row estimation, and partition indexing. Updated writer/src/buffer.ts, query/src/engine.ts, and lakehouse/src/partition.ts to use these constants instead of inline values. All affected tests pass.","labels":["code-quality","maintainability"]}
{"id":"evodb-5ra5","title":"Add consistent retry logic pattern across async operations","description":"## Problem\n\nThe codebase has various approaches to retry logic that should be standardized:\n\n**Current implementations**:\n\n1. **Circuit breaker in `core/src/circuit-breaker.ts`**:\n- Implements retry with exponential backoff\n- Has max retries and cooldown period\n- Good pattern but underutilized\n\n2. **R2 operations in `rpc/src/server.ts`**:\n```typescript\nconsole.error('R2 flush failed, using fallback:', error); // line 757\n```\nFalls back to DO storage but doesn't retry R2.\n\n3. **Writer pending blocks in `writer/src/writer.ts`**:\n- Stores failed blocks in DO for later retry\n- Uses scheduled alarms for retry (`PENDING_BLOCK_RETRY_INTERVAL_MS`)\n- Good pattern\n\n4. **Async patterns tests** (`core/src/__tests__/async-patterns.unit.test.ts`):\n- Shows various retry patterns in test fixtures\n- Could be extracted as reusable utilities\n\n## Recommendation\n1. Extract retry logic from circuit breaker into reusable utility\n2. Create `withRetry\u003cT\u003e(fn, options)` wrapper function\n3. Standardize retry options: maxRetries, backoff type, delays\n4. Use consistent pattern across R2 operations, RPC, etc.\n\n## Proposed API\n```typescript\ninterface RetryOptions {\n  maxRetries: number;\n  initialDelayMs: number;\n  maxDelayMs: number;\n  backoff: 'exponential' | 'linear' | 'constant';\n  retryOn?: (error: Error) =\u003e boolean;\n}\n\nasync function withRetry\u003cT\u003e(fn: () =\u003e Promise\u003cT\u003e, options: RetryOptions): Promise\u003cT\u003e;\n```\n\n## Files\n- `core/src/circuit-breaker.ts`\n- `rpc/src/server.ts`\n- New: `core/src/retry.ts`","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:13.769667-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:45:15.454991-06:00","closed_at":"2026-01-21T19:45:15.454991-06:00","close_reason":"Closed","labels":["error-handling","patterns","reliability"]}
{"id":"evodb-5rh","title":"TypeScript: Reduce unsafe 'as unknown' casts in production code","description":"## Summary\n\nFound multiple instances of `as unknown` type casts in production code that bypass TypeScript's type safety. While some are legitimate (test mocks), several in production code indicate type design gaps.\n\n## Locations\n\n### Production Code (Needs Review)\n\n1. **lakehouse/src/manifest.ts:304** - `parsed as unknown as TableManifest`\n   - After JSON.parse, casting directly to TableManifest without validation\n   - Should use a type guard or zod schema for runtime validation\n\n2. **observability/src/metrics.ts:290,315** - Metric casting\n   - `metric as unknown as InternalMetric`\n   - `histogram as unknown as InternalHistogram`\n   - Internal types should be accessible without casts\n\n3. **core/src/query-ops.ts:835,851,867,883** - Aggregation nulls\n   - `null as unknown` for min/max/first/last initializers\n   - Could use `null as T | null` or proper generic constraints\n\n4. **core/src/stack-trace.ts:41** - V8 error check\n   - `errorConstructor as unknown as V8Error` - legitimate for runtime detection\n\n5. **core/src/shred.ts:366** - Array type cast\n   - `existing as unknown[]` - may indicate schema type mismatch\n\n### Benchmark Code (Lower Priority)\n\n- **benchmark/src/datasets/*.ts** - Multiple `shred(data as unknown[])` casts\n- **benchmark/src/generators/data-generator.ts** - `undefined as unknown` initializers\n\n## Recommendations\n\n1. Add runtime validation after JSON.parse with type guards or zod\n2. Export internal types or use module augmentation\n3. Use proper generic constraints for aggregation initializers\n4. Document legitimate escape hatches\n\n## References\n\n- TypeScript strict mode best practices\n- Runtime type validation patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:00.025717-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.198019-06:00","closed_at":"2026-01-21T20:14:08.198019-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-5xa","title":"TDD: Add RPC event handler error logging","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:32.308686-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.675188-06:00","closed_at":"2026-01-20T17:49:16.675188-06:00","close_reason":"Closed","external_ref":"gh-109"}
{"id":"evodb-5zo","title":"TDD: Add error codes enum to StorageError","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:10:01.73725-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:48:04.524128-06:00","closed_at":"2026-01-20T17:48:04.524128-06:00","close_reason":"Closed"}
{"id":"evodb-61v3","title":"TDD: Implement transaction API with ACID guarantees","description":"## Overview\n\nImplement a transaction API that provides full ACID (Atomicity, Consistency, Isolation, Durability) guarantees for EvoDB operations.\n\n## TDD Red-Green-Refactor Cycle\n\n### RED Phase: Write Failing Tests First\n\n```typescript\n// tests/transactions/transaction-api.test.ts\nimport { describe, it, expect, beforeEach } from 'vitest'\nimport { EvoDB } from '../src/evodb'\n\ndescribe('Transaction API', () =\u003e {\n  let db: EvoDB\n\n  beforeEach(async () =\u003e {\n    db = new EvoDB({ name: 'test-transactions' })\n    await db.clear()\n  })\n\n  describe('Basic Transaction Operations', () =\u003e {\n    it('should begin a new transaction', async () =\u003e {\n      const tx = await db.transaction()\n      expect(tx).toBeDefined()\n      expect(tx.id).toBeDefined()\n      expect(tx.status).toBe('active')\n    })\n\n    it('should commit a transaction and persist changes', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.put('key1', { value: 'test' })\n      await tx.commit()\n      \n      // Changes should be visible after commit\n      const result = await db.get('key1')\n      expect(result).toEqual({ value: 'test' })\n    })\n\n    it('should rollback a transaction and discard changes', async () =\u003e {\n      await db.put('key1', { value: 'original' })\n      \n      const tx = await db.transaction()\n      await tx.put('key1', { value: 'modified' })\n      await tx.rollback()\n      \n      // Changes should be discarded\n      const result = await db.get('key1')\n      expect(result).toEqual({ value: 'original' })\n    })\n\n    it('should throw when operating on committed transaction', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.commit()\n      \n      await expect(tx.put('key', 'value')).rejects.toThrow('Transaction already committed')\n    })\n\n    it('should throw when operating on rolled back transaction', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.rollback()\n      \n      await expect(tx.put('key', 'value')).rejects.toThrow('Transaction already rolled back')\n    })\n  })\n\n  describe('Atomicity', () =\u003e {\n    it('should apply all operations or none on commit', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.put('key1', { value: 1 })\n      await tx.put('key2', { value: 2 })\n      await tx.put('key3', { value: 3 })\n      await tx.commit()\n\n      const results = await Promise.all([\n        db.get('key1'),\n        db.get('key2'),\n        db.get('key3')\n      ])\n      expect(results).toEqual([{ value: 1 }, { value: 2 }, { value: 3 }])\n    })\n\n    it('should discard all operations on rollback', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.put('key1', { value: 1 })\n      await tx.put('key2', { value: 2 })\n      await tx.rollback()\n\n      const results = await Promise.all([\n        db.get('key1'),\n        db.get('key2')\n      ])\n      expect(results).toEqual([undefined, undefined])\n    })\n\n    it('should rollback on error during commit', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.put('key1', { value: 1 })\n      \n      // Simulate failure\n      vi.spyOn(db, '_commitBatch').mockRejectedValueOnce(new Error('Disk full'))\n      \n      await expect(tx.commit()).rejects.toThrow('Disk full')\n      \n      // All changes should be rolled back\n      const result = await db.get('key1')\n      expect(result).toBeUndefined()\n    })\n  })\n\n  describe('Isolation Levels', () =\u003e {\n    it('should support READ_COMMITTED isolation', async () =\u003e {\n      await db.put('key1', { value: 'initial' })\n      \n      const tx1 = await db.transaction({ isolation: 'READ_COMMITTED' })\n      const tx2 = await db.transaction({ isolation: 'READ_COMMITTED' })\n      \n      await tx1.put('key1', { value: 'tx1-modified' })\n      \n      // tx2 should still see original value (not committed)\n      const value = await tx2.get('key1')\n      expect(value).toEqual({ value: 'initial' })\n      \n      await tx1.commit()\n      \n      // After commit, tx2 should see new value\n      const valueAfterCommit = await tx2.get('key1')\n      expect(valueAfterCommit).toEqual({ value: 'tx1-modified' })\n      \n      await tx2.rollback()\n    })\n\n    it('should support REPEATABLE_READ isolation', async () =\u003e {\n      await db.put('key1', { value: 'initial' })\n      \n      const tx1 = await db.transaction({ isolation: 'REPEATABLE_READ' })\n      const tx2 = await db.transaction({ isolation: 'REPEATABLE_READ' })\n      \n      // tx1 reads value\n      const initialRead = await tx1.get('key1')\n      expect(initialRead).toEqual({ value: 'initial' })\n      \n      // tx2 modifies and commits\n      await tx2.put('key1', { value: 'tx2-modified' })\n      await tx2.commit()\n      \n      // tx1 should still see original value (repeatable read)\n      const repeatRead = await tx1.get('key1')\n      expect(repeatRead).toEqual({ value: 'initial' })\n      \n      await tx1.rollback()\n    })\n\n    it('should support SERIALIZABLE isolation', async () =\u003e {\n      await db.put('counter', { value: 0 })\n      \n      const tx1 = await db.transaction({ isolation: 'SERIALIZABLE' })\n      const tx2 = await db.transaction({ isolation: 'SERIALIZABLE' })\n      \n      // Both read current value\n      const val1 = await tx1.get('counter')\n      const val2 = await tx2.get('counter')\n      \n      // Both try to increment\n      await tx1.put('counter', { value: val1.value + 1 })\n      await tx2.put('counter', { value: val2.value + 1 })\n      \n      await tx1.commit()\n      \n      // tx2 should fail due to serialization conflict\n      await expect(tx2.commit()).rejects.toThrow('Serialization conflict')\n      \n      // Final value should be 1, not 2\n      const final = await db.get('counter')\n      expect(final).toEqual({ value: 1 })\n    })\n  })\n\n  describe('Consistency', () =\u003e {\n    it('should enforce schema constraints within transaction', async () =\u003e {\n      db.defineSchema('users', {\n        type: 'object',\n        required: ['email'],\n        properties: {\n          email: { type: 'string', format: 'email' }\n        }\n      })\n\n      const tx = await db.transaction()\n      \n      await expect(\n        tx.put('users:1', { name: 'John' }) // missing email\n      ).rejects.toThrow('Validation failed')\n      \n      await tx.rollback()\n    })\n\n    it('should enforce uniqueness constraints', async () =\u003e {\n      db.defineIndex('users', 'email', { unique: true })\n      await db.put('users:1', { email: 'test@example.com' })\n\n      const tx = await db.transaction()\n      await expect(\n        tx.put('users:2', { email: 'test@example.com' })\n      ).rejects.toThrow('Unique constraint violation')\n      \n      await tx.rollback()\n    })\n  })\n\n  describe('Durability', () =\u003e {\n    it('should persist committed transactions across restarts', async () =\u003e {\n      const tx = await db.transaction()\n      await tx.put('persistent-key', { value: 'durable' })\n      await tx.commit()\n\n      // Simulate restart\n      await db.close()\n      db = new EvoDB({ name: 'test-transactions' })\n\n      const result = await db.get('persistent-key')\n      expect(result).toEqual({ value: 'durable' })\n    })\n\n    it('should write to WAL before committing', async () =\u003e {\n      const walSpy = vi.spyOn(db, '_writeToWAL')\n      \n      const tx = await db.transaction()\n      await tx.put('key1', { value: 'test' })\n      await tx.commit()\n\n      expect(walSpy).toHaveBeenCalledBefore(db._commitBatch)\n    })\n  })\n})\n```\n\n### GREEN Phase: Minimal Implementation to Pass Tests\n\n```typescript\n// src/transactions/transaction-manager.ts\nexport type IsolationLevel = 'READ_COMMITTED' | 'REPEATABLE_READ' | 'SERIALIZABLE'\n\nexport interface TransactionOptions {\n  isolation?: IsolationLevel\n  timeout?: number\n}\n\nexport class Transaction {\n  readonly id: string\n  private _status: 'active' | 'committed' | 'rolled_back' = 'active'\n  private operations: Map\u003cstring, { type: 'put' | 'delete', value?: any }\u003e = new Map()\n  private readSet: Map\u003cstring, number\u003e = new Map() // key -\u003e version\n  private snapshot: Map\u003cstring, any\u003e = new Map()\n  \n  constructor(\n    private db: EvoDB,\n    private options: TransactionOptions = {}\n  ) {\n    this.id = crypto.randomUUID()\n    if (options.isolation === 'REPEATABLE_READ' || options.isolation === 'SERIALIZABLE') {\n      this.takeSnapshot()\n    }\n  }\n\n  get status() { return this._status }\n\n  private assertActive() {\n    if (this._status === 'committed') {\n      throw new Error('Transaction already committed')\n    }\n    if (this._status === 'rolled_back') {\n      throw new Error('Transaction already rolled back')\n    }\n  }\n\n  async get(key: string): Promise\u003cany\u003e {\n    this.assertActive()\n    \n    // Check local operations first\n    if (this.operations.has(key)) {\n      const op = this.operations.get(key)!\n      return op.type === 'delete' ? undefined : op.value\n    }\n    \n    // For REPEATABLE_READ/SERIALIZABLE, use snapshot\n    if (this.snapshot.has(key)) {\n      return this.snapshot.get(key)\n    }\n    \n    // Read from database\n    const { value, version } = await this.db._getWithVersion(key)\n    this.readSet.set(key, version)\n    \n    if (this.options.isolation === 'REPEATABLE_READ' || this.options.isolation === 'SERIALIZABLE') {\n      this.snapshot.set(key, value)\n    }\n    \n    return value\n  }\n\n  async put(key: string, value: any): Promise\u003cvoid\u003e {\n    this.assertActive()\n    await this.db._validateConstraints(key, value)\n    this.operations.set(key, { type: 'put', value })\n  }\n\n  async delete(key: string): Promise\u003cvoid\u003e {\n    this.assertActive()\n    this.operations.set(key, { type: 'delete' })\n  }\n\n  async commit(): Promise\u003cvoid\u003e {\n    this.assertActive()\n    \n    try {\n      // Check for serialization conflicts\n      if (this.options.isolation === 'SERIALIZABLE') {\n        await this.checkSerializationConflicts()\n      }\n      \n      // Write to WAL first (durability)\n      await this.db._writeToWAL(this.id, this.operations)\n      \n      // Apply all operations atomically\n      await this.db._commitBatch(this.operations)\n      \n      this._status = 'committed'\n    } catch (error) {\n      await this.rollback()\n      throw error\n    }\n  }\n\n  async rollback(): Promise\u003cvoid\u003e {\n    if (this._status !== 'active') return\n    \n    this.operations.clear()\n    this.readSet.clear()\n    this.snapshot.clear()\n    this._status = 'rolled_back'\n  }\n\n  private async checkSerializationConflicts(): Promise\u003cvoid\u003e {\n    for (const [key, readVersion] of this.readSet) {\n      const currentVersion = await this.db._getVersion(key)\n      if (currentVersion !== readVersion) {\n        throw new Error('Serialization conflict')\n      }\n    }\n  }\n\n  private async takeSnapshot(): Promise\u003cvoid\u003e {\n    // Implementation for snapshot isolation\n  }\n}\n\nexport class TransactionManager {\n  private activeTransactions: Map\u003cstring, Transaction\u003e = new Map()\n  \n  async begin(db: EvoDB, options?: TransactionOptions): Promise\u003cTransaction\u003e {\n    const tx = new Transaction(db, options)\n    this.activeTransactions.set(tx.id, tx)\n    return tx\n  }\n}\n```\n\n### REFACTOR Phase: Clean Up and Optimize\n\n1. **Optimize Locking Strategy**\n   - Implement fine-grained locking (per-key vs global)\n   - Add lock timeouts to prevent deadlocks\n   - Implement deadlock detection\n\n2. **Add Savepoints**\n   ```typescript\n   interface Savepoint {\n     id: string\n     operations: Map\u003cstring, Operation\u003e\n   }\n   \n   async savepoint(name: string): Promise\u003cvoid\u003e\n   async rollbackTo(name: string): Promise\u003cvoid\u003e\n   async releaseSavepoint(name: string): Promise\u003cvoid\u003e\n   ```\n\n3. **Performance Optimizations**\n   - Batch WAL writes\n   - Implement write-ahead logging with group commit\n   - Add transaction pooling for high-concurrency\n\n4. **Additional Features**\n   - Nested transactions\n   - Read-only transactions (optimized)\n   - Transaction retry with exponential backoff\n\n## Acceptance Criteria\n\n- [ ] All RED phase tests pass\n- [ ] Transaction begin/commit/rollback work correctly\n- [ ] Atomicity: all-or-nothing semantics enforced\n- [ ] Consistency: constraints enforced within transactions\n- [ ] Isolation: READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE supported\n- [ ] Durability: WAL ensures committed data survives crashes\n- [ ] Savepoints implemented\n- [ ] Performance benchmarks show \u003c 5ms overhead per transaction\n\n## Dependencies\n\n- WAL (Write-Ahead Log) implementation\n- Lock manager for isolation\n- Version tracking for MVCC","status":"closed","priority":0,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:35.37015-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:32:57.941801-06:00","closed_at":"2026-01-21T19:32:57.941801-06:00","close_reason":"Closed"}
{"id":"evodb-6cm","title":"TDD: Bound routing cache with LRU eviction","description":"writer/shard-router.ts - routingCache grows unboundedly. Implement LRU cache with max 10000 entries.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:10:06.068165-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.476622-06:00","closed_at":"2026-01-20T16:49:22.476622-06:00","close_reason":"Closed","external_ref":"gh-35"}
{"id":"evodb-6ek","title":"TDD: Add tests for RPC server.ts module","description":"## Problem\nThe RPC `server.ts` file (27KB) handles incoming connections and message routing but lacks comprehensive test coverage. Server-side concerns like connection pooling, request routing, and error responses need testing.\n\n## Coverage Gap\n- `server.ts`: 27KB source, 0 direct tests\n- Connection acceptance and management\n- Request routing to handlers\n- Error response formatting\n- Concurrent connection handling\n\n## Acceptance Criteria\n- [ ] Create `server.unit.test.ts` with tests for server lifecycle\n- [ ] Test connection acceptance and tracking\n- [ ] Test message routing to registered handlers\n- [ ] Test error response generation\n- [ ] Test connection cleanup on client disconnect\n- [ ] Test max connections limit enforcement\n\n## TDD Approach\nWrite failing tests first that define expected behavior for:\n1. Server startup and shutdown\n2. Handler registration and routing\n3. Error handling and response codes\n4. Connection limits and backpressure","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:17.78814-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.253223-06:00","closed_at":"2026-01-21T20:14:08.253223-06:00","close_reason":"Duplicate of already-implemented issues","labels":["rpc","tdd","testing"]}
{"id":"evodb-6hn","title":"Pre-compile column accessors for filter evaluation","description":"## Problem\nEach filter evaluation calls getNestedValue() which does string parsing and regex validation per row.\n\n## TDD Approach\n1. Write benchmark comparing compiled vs interpreted accessors\n2. Implement accessor compilation that runs once per query\n3. Use compiled accessors in filter loop\n4. Verify performance improvement\n\n## Expected Impact\n- 40-60% query speedup\n- Eliminate 500K regex ops for 100K rows × 5 filters\n\n## Current Code (query-ops.ts:393-412)\n```typescript\nfor (const filter of filters) {\n  validateColumnName(filter.column);  // Per row!\n  const value = getNestedValue(row, filter.column);  // String parsing per row\n}\n```\n\n## Fix\n```typescript\n// Compile once\nconst accessors = filters.map(f =\u003e ({\n  get: compileAccessor(f.column),\n  filter: f\n}));\n\n// Use compiled\nfor (const row of rows) {\n  for (const {get, filter} of accessors) {\n    if (!matches(get(row), filter)) return false;\n  }\n}\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:39.575884-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:21:40.660347-06:00","closed_at":"2026-01-21T12:21:40.660347-06:00","close_reason":"Closed"}
{"id":"evodb-6i2","title":"Improve error messages with consistent formatting and context","description":"## Problem\n\nError messages across the codebase have inconsistent formatting and sometimes lack context:\n\n**Good Examples** (from `core/src/errors.ts`):\n- Well-structured error hierarchy with `EvoDBError`, `QueryError`, `ValidationError`, etc.\n- Error codes for programmatic handling\n- Details object for debugging info\n\n**Issues Found**:\n\n1. **Generic Error usage instead of typed errors** (`core/src/storage.ts`):\n```typescript\nthrow new Error(`Object not found: ${path}`);  // Line 321\n```\nShould use `StorageError` with `StorageErrorCode.NOT_FOUND`\n\n2. **Missing context in error messages** (`core/src/merge.ts:72`):\n```typescript\nthrow new Error('No blocks to merge');\n```\nShould include why this is an error, expected input, actual input\n\n3. **Inconsistent error formats** - some use template literals, some use string concatenation\n\n4. **Error codes not used consistently** - `StorageErrorCode` enum exists but isn't always used\n\n## Recommendation\n1. Create an error factory utility to ensure consistent formatting\n2. Always use typed errors (EvoDBError subclasses) instead of generic Error\n3. Include context: what operation failed, expected vs actual, how to fix\n4. Use error codes consistently for programmatic error handling\n\n## Files affected\n- `core/src/storage.ts` - Multiple generic Error throws\n- `core/src/merge.ts` - Missing context\n- `core/src/encode.ts` - Some generic errors\n- `core/src/shred.ts` - Generic errors","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:56.123972-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:30:52.790516-06:00","closed_at":"2026-01-21T20:30:52.790516-06:00","close_reason":"Closed","labels":["DX","code-quality","error-handling"]}
{"id":"evodb-6mx","title":"Merge @evodb/reader into @evodb/query","description":"## Problem\nTwo separate query engines (@evodb/reader and @evodb/query) duplicate 80% of logic. QueryExecutorAdapter bridges them with boilerplate.\n\n## TDD Approach\n1. Write integration tests covering both engines' functionality\n2. Merge reader logic into query package\n3. Remove QueryExecutorAdapter\n4. Verify single unified API works\n5. Remove @evodb/reader package\n\n## Expected Impact\n- 30-50KB bundle reduction\n- Simpler mental model for users\n- Less type duplication\n\n## Files\n- reader/src/* → query/src/\n- Remove reader package entirely","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:24.760331-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T17:14:37.235363-06:00","closed_at":"2026-01-21T17:14:37.235363-06:00","close_reason":"Merge already complete. @evodb/reader is now a deprecated wrapper that re-exports from @evodb/query. Analysis shows: (1) reader/src/index.ts re-exports everything from @evodb/query with backward-compatible type aliases, (2) @evodb/query contains both SimpleQueryEngine (former reader) and full QueryEngine, (3) shared code (evaluateFilter, sortRows, computeAggregations) moved to @evodb/core, (4) QueryExecutorAdapter bridges both engines. Remaining cleanup: legacy files in reader/src (types.ts, validation.ts, cache.ts) are unused and can be deleted. No code changes needed - merge is architecturally complete."}
{"id":"evodb-6n1","title":"TDD: Add atomic flush for Writer+Manifest","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:16.246936-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:04:48.069312-06:00","closed_at":"2026-01-20T14:04:48.069312-06:00","close_reason":"Closed"}
{"id":"evodb-6nt4","title":"WRITE: Create comprehensive @evodb/observability README","description":"## Overview\nCreate a comprehensive README for the @evodb/observability package covering metrics, tracing, and logging setup.\n\n## Current State\nThe observability package needs a complete README documenting all observability features.\n\n## Required Sections\n\n### 1. Metrics Setup with Prometheus\n```typescript\nimport { createMetrics, PrometheusExporter } from '@evodb/observability'\n\nconst metrics = createMetrics({\n  exporter: new PrometheusExporter({ port: 9090 }),\n  prefix: 'evodb_',\n})\n\n// Track query performance\nmetrics.queryDuration.observe(duration)\nmetrics.queryCount.inc({ status: 'success' })\n\n// Track storage metrics\nmetrics.storageBytes.set(totalBytes)\nmetrics.documentCount.set(count)\n```\n\nDocument available metrics:\n- Query metrics (latency, count, errors)\n- Storage metrics (bytes, documents, partitions)\n- Writer metrics (writes/sec, compaction time)\n- Cache metrics (hits, misses, evictions)\n\n### 2. Tracing Setup with OpenTelemetry\n```typescript\nimport { createTracer, OTLPExporter } from '@evodb/observability'\n\nconst tracer = createTracer({\n  serviceName: 'my-evodb-service',\n  exporter: new OTLPExporter({\n    endpoint: 'http://jaeger:4318/v1/traces'\n  }),\n})\n\n// Automatic instrumentation\nconst db = createDatabase({\n  tracer,\n  // ... other options\n})\n\n// Manual spans\nconst span = tracer.startSpan('custom-operation')\ntry {\n  // ... operation\n} finally {\n  span.end()\n}\n```\n\n### 3. Logging Configuration\n```typescript\nimport { createLogger, LogLevel } from '@evodb/observability'\n\nconst logger = createLogger({\n  level: LogLevel.INFO,\n  format: 'json', // or 'text'\n  destination: process.stdout,\n})\n\n// Structured logging\nlogger.info('Query executed', {\n  duration: 45,\n  documentsReturned: 100,\n  queryHash: 'abc123',\n})\n```\n\n### 4. Integration with Cloudflare Analytics\n```typescript\nimport { CloudflareAnalytics } from '@evodb/observability'\n\n// In Durable Object\nexport class MyDO extends DurableObject {\n  analytics = new CloudflareAnalytics(this.ctx)\n  \n  async fetch(request: Request) {\n    this.analytics.trackRequest(request)\n    // ... handle request\n    this.analytics.trackLatency('query', duration)\n  }\n}\n```\n\nDocument:\n- Workers Analytics Engine integration\n- Custom metrics in Durable Objects\n- Dashboard setup in Cloudflare\n\n## Acceptance Criteria\n- [ ] README covers all observability features\n- [ ] Code examples are accurate and runnable\n- [ ] Integration guides for common platforms (Grafana, Datadog, etc.)\n- [ ] Best practices for production monitoring\n- [ ] Troubleshooting section included","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:56.369342-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:59:55.38399-06:00","closed_at":"2026-01-21T19:59:55.38399-06:00","close_reason":"Closed"}
{"id":"evodb-6yz","title":"Remove or implement TODO items in query engine R2DataSource","description":"## Problem\n\nThe `query/src/engine.ts` file has unimplemented TODO items that need to be addressed:\n\n**Line 198**:\n```typescript\n/**\n * R2DataSource - Reads table data from R2 bucket using the manifest format.\n *\n * TODO: Integrate with @evodb/reader for production use.\n */\n```\n\n**Line 253**:\n```typescript\n/**\n * Parse columnar data from R2.\n * TODO: Implement proper columnar format parsing with @evodb/reader.\n */\nprivate parseColumnarData(_data: ArrayBuffer, partition: PartitionInfo): Record\u003cstring, unknown\u003e[] {\n```\n\nCurrently, `parseColumnarData` generates mock data from partition metadata instead of parsing actual R2 data.\n\n## Recommendation\nEither:\n1. Implement the proper integration with @evodb/reader\n2. Remove R2DataSource if it's not needed (MockDataSource exists for testing)\n3. Add clear documentation that this is a placeholder\n\n## Impact\nProduction queries using R2DataSource won't work correctly - they'll get mock data instead of actual data.\n\n## Files\n- `query/src/engine.ts:198-275`","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:02.849912-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:43:03.805534-06:00","closed_at":"2026-01-21T19:43:03.805534-06:00","close_reason":"Closed","labels":["query-engine","tech-debt"]}
{"id":"evodb-6zh","title":"Move chaos-testing.ts to test-utils package","description":"## Problem\nchaos-testing.ts (1,345 lines) is test-only code but included in production @evodb/core bundle.\n\n## TDD Approach\n1. Write test verifying chaos-testing exports work from test-utils\n2. Move file to @evodb/test-utils\n3. Update imports in tests\n4. Verify core bundle no longer includes chaos code\n\n## Expected Impact\n- 20-30KB gzipped reduction from core bundle\n\n## Files\n- core/src/chaos-testing.ts → test-utils/src/chaos-testing.ts\n- core/src/storage/index.ts (remove re-export)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:18.521087-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:23:57.880584-06:00","closed_at":"2026-01-21T12:23:57.880584-06:00","close_reason":"Closed"}
{"id":"evodb-719","title":"TDD: Add type guards before type assertions","description":"Replace 'as unknown[]' and 'as Record\u003cstring,unknown\u003e' with proper type guards throughout codebase.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:08.652257-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.456809-06:00","closed_at":"2026-01-20T16:23:39.456809-06:00","close_reason":"Closed","external_ref":"gh-36"}
{"id":"evodb-71gn","title":"TDD: Implement full-text search integration","description":"## Overview\n\nImplement full-text search capabilities with an inverted index, supporting text indexing, search queries with relevance scoring, stemming, and fuzzy matching.\n\n## TDD Red-Green-Refactor Cycle\n\n### RED Phase: Write Failing Tests First\n\n```typescript\n// tests/search/full-text-search.test.ts\nimport { describe, it, expect, beforeEach, afterEach, vi } from 'vitest'\nimport { EvoDB } from '../src/evodb'\n\ndescribe('Full-text Search', () =\u003e {\n  let db: EvoDB\n\n  beforeEach(async () =\u003e {\n    db = new EvoDB({ name: 'test-search' })\n    await db.clear()\n  })\n\n  afterEach(async () =\u003e {\n    await db.close()\n  })\n\n  describe('Text Indexing', () =\u003e {\n    it('should create a text index on a field', async () =\u003e {\n      const result = await db.createTextIndex('articles', 'content')\n\n      expect(result).toEqual({\n        name: 'articles_content_text',\n        field: 'content',\n        collection: 'articles',\n        type: 'text',\n        language: 'english'\n      })\n    })\n\n    it('should create a text index on multiple fields', async () =\u003e {\n      const result = await db.createTextIndex('articles', ['title', 'content', 'tags'])\n\n      expect(result.fields).toEqual(['title', 'content', 'tags'])\n    })\n\n    it('should support field weights', async () =\u003e {\n      const result = await db.createTextIndex('articles', {\n        title: { weight: 10 },\n        content: { weight: 1 },\n        tags: { weight: 5 }\n      })\n\n      expect(result.weights).toEqual({\n        title: 10,\n        content: 1,\n        tags: 5\n      })\n    })\n\n    it('should index existing documents', async () =\u003e {\n      await db.put('articles:1', {\n        title: 'Introduction to TypeScript',\n        content: 'TypeScript is a typed superset of JavaScript'\n      })\n      await db.put('articles:2', {\n        title: 'JavaScript Basics',\n        content: 'JavaScript is a dynamic programming language'\n      })\n\n      await db.createTextIndex('articles', ['title', 'content'])\n\n      const stats = await db.getTextIndexStats('articles_title_content_text')\n      expect(stats.documentCount).toBe(2)\n      expect(stats.termCount).toBeGreaterThan(0)\n    })\n\n    it('should update index on document changes', async () =\u003e {\n      await db.createTextIndex('articles', 'content')\n\n      await db.put('articles:1', { content: 'Hello world' })\n\n      let results = await db.search('articles', 'world')\n      expect(results).toHaveLength(1)\n\n      await db.put('articles:1', { content: 'Goodbye universe' })\n\n      results = await db.search('articles', 'world')\n      expect(results).toHaveLength(0)\n\n      results = await db.search('articles', 'universe')\n      expect(results).toHaveLength(1)\n    })\n\n    it('should remove from index on document delete', async () =\u003e {\n      await db.createTextIndex('articles', 'content')\n      await db.put('articles:1', { content: 'Searchable content' })\n\n      let results = await db.search('articles', 'searchable')\n      expect(results).toHaveLength(1)\n\n      await db.delete('articles:1')\n\n      results = await db.search('articles', 'searchable')\n      expect(results).toHaveLength(0)\n    })\n\n    it('should specify language for text processing', async () =\u003e {\n      const result = await db.createTextIndex('articles', 'content', {\n        language: 'spanish'\n      })\n\n      expect(result.language).toBe('spanish')\n    })\n  })\n\n  describe('Search Queries', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('articles:1', {\n        title: 'Getting Started with React',\n        content: 'React is a JavaScript library for building user interfaces. It uses a virtual DOM for efficient updates.',\n        tags: ['react', 'javascript', 'frontend']\n      })\n      await db.put('articles:2', {\n        title: 'Vue.js Tutorial',\n        content: 'Vue is a progressive JavaScript framework. It is designed to be incrementally adoptable.',\n        tags: ['vue', 'javascript', 'frontend']\n      })\n      await db.put('articles:3', {\n        title: 'Node.js Backend Development',\n        content: 'Node.js is a runtime environment for executing JavaScript on the server side.',\n        tags: ['node', 'javascript', 'backend']\n      })\n\n      await db.createTextIndex('articles', {\n        title: { weight: 10 },\n        content: { weight: 5 },\n        tags: { weight: 3 }\n      })\n    })\n\n    it('should search for single term', async () =\u003e {\n      const results = await db.search('articles', 'react')\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n\n    it('should search for multiple terms (AND by default)', async () =\u003e {\n      const results = await db.search('articles', 'javascript library')\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n\n    it('should support OR operator', async () =\u003e {\n      const results = await db.search('articles', 'react OR vue')\n\n      expect(results).toHaveLength(2)\n    })\n\n    it('should support NOT operator', async () =\u003e {\n      const results = await db.search('articles', 'javascript -react')\n\n      expect(results).toHaveLength(2)\n      expect(results.find(r =\u003e r.key === 'articles:1')).toBeUndefined()\n    })\n\n    it('should support phrase search', async () =\u003e {\n      const results = await db.search('articles', '\"virtual DOM\"')\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n\n    it('should support wildcard search', async () =\u003e {\n      const results = await db.search('articles', 'java*')\n\n      expect(results).toHaveLength(3) // All have JavaScript\n    })\n\n    it('should support prefix search', async () =\u003e {\n      const results = await db.search('articles', 'front*')\n\n      expect(results).toHaveLength(2) // React and Vue have frontend tag\n    })\n\n    it('should return empty for no matches', async () =\u003e {\n      const results = await db.search('articles', 'python')\n\n      expect(results).toHaveLength(0)\n    })\n\n    it('should support pagination', async () =\u003e {\n      const page1 = await db.search('articles', 'javascript', {\n        limit: 2,\n        offset: 0\n      })\n      const page2 = await db.search('articles', 'javascript', {\n        limit: 2,\n        offset: 2\n      })\n\n      expect(page1).toHaveLength(2)\n      expect(page2).toHaveLength(1)\n      expect(page1[0].key).not.toBe(page2[0].key)\n    })\n\n    it('should support field-specific search', async () =\u003e {\n      const results = await db.search('articles', 'title:React')\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n  })\n\n  describe('Relevance Scoring', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('articles:1', {\n        title: 'JavaScript JavaScript JavaScript',\n        content: 'A short article'\n      })\n      await db.put('articles:2', {\n        title: 'Programming',\n        content: 'JavaScript is a programming language. JavaScript is popular.'\n      })\n      await db.put('articles:3', {\n        title: 'Other Topic',\n        content: 'This article mentions JavaScript once'\n      })\n\n      await db.createTextIndex('articles', {\n        title: { weight: 2 },\n        content: { weight: 1 }\n      })\n    })\n\n    it('should return results sorted by relevance score', async () =\u003e {\n      const results = await db.search('articles', 'javascript')\n\n      expect(results[0].key).toBe('articles:1') // Most occurrences + in title\n      expect(results[0].score).toBeGreaterThan(results[1].score)\n      expect(results[1].score).toBeGreaterThan(results[2].score)\n    })\n\n    it('should include score in results', async () =\u003e {\n      const results = await db.search('articles', 'javascript')\n\n      expect(results[0]).toHaveProperty('score')\n      expect(typeof results[0].score).toBe('number')\n      expect(results[0].score).toBeGreaterThan(0)\n    })\n\n    it('should apply field weights to scoring', async () =\u003e {\n      const results = await db.search('articles', 'programming')\n\n      // articles:2 has Programming in title (weight 2) AND programming in content\n      expect(results[0].key).toBe('articles:2')\n    })\n\n    it('should use TF-IDF scoring', async () =\u003e {\n      // Add more documents to test IDF\n      for (let i = 10; i \u003c 20; i++) {\n        await db.put(`articles:${i}`, {\n          title: 'Common Topic',\n          content: 'This is common content'\n        })\n      }\n\n      const results = await db.search('articles', 'javascript')\n\n      // javascript is rare, so it should have high IDF\n      expect(results[0].score).toBeGreaterThan(0)\n    })\n\n    it('should boost exact matches', async () =\u003e {\n      await db.put('articles:exact', {\n        title: 'JavaScript',\n        content: 'Exact title match'\n      })\n\n      const results = await db.search('articles', 'javascript', {\n        exactMatchBoost: 2.0\n      })\n\n      expect(results.find(r =\u003e r.key === 'articles:exact')?.score).toBeGreaterThan(0)\n    })\n\n    it('should support custom scoring function', async () =\u003e {\n      const results = await db.search('articles', 'javascript', {\n        scoringFunction: (doc, termFreq, docFreq, totalDocs) =\u003e {\n          // Custom BM25-like scoring\n          const k1 = 1.2\n          const b = 0.75\n          const avgDl = 100\n          const dl = doc.content.length\n          const idf = Math.log((totalDocs - docFreq + 0.5) / (docFreq + 0.5))\n          return idf * (termFreq * (k1 + 1)) / (termFreq + k1 * (1 - b + b * dl / avgDl))\n        }\n      })\n\n      expect(results).toHaveLength(3)\n    })\n  })\n\n  describe('Stemming', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('articles:1', { content: 'running fast' })\n      await db.put('articles:2', { content: 'the runner ran' })\n      await db.put('articles:3', { content: 'runs quickly' })\n\n      await db.createTextIndex('articles', 'content', {\n        stemming: true,\n        language: 'english'\n      })\n    })\n\n    it('should match stemmed variants', async () =\u003e {\n      const results = await db.search('articles', 'run')\n\n      // Should match: running, runner, ran, runs\n      expect(results).toHaveLength(3)\n    })\n\n    it('should stem search query', async () =\u003e {\n      const results = await db.search('articles', 'running')\n\n      // running stems to run, should match all\n      expect(results).toHaveLength(3)\n    })\n\n    it('should support disabling stemming for exact match', async () =\u003e {\n      const results = await db.search('articles', '\"running\"', {\n        stemming: false\n      })\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n\n    it('should use language-specific stemmer', async () =\u003e {\n      await db.put('spanish:1', { content: 'corriendo rapido' })\n      await db.put('spanish:2', { content: 'el corredor corrio' })\n\n      await db.createTextIndex('spanish', 'content', {\n        stemming: true,\n        language: 'spanish'\n      })\n\n      const results = await db.search('spanish', 'correr')\n\n      expect(results).toHaveLength(2)\n    })\n  })\n\n  describe('Fuzzy Matching', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('articles:1', { content: 'TypeScript programming' })\n      await db.put('articles:2', { content: 'JavaScript development' })\n      await db.put('articles:3', { content: 'Python scripting' })\n\n      await db.createTextIndex('articles', 'content')\n    })\n\n    it('should find matches with typos (edit distance 1)', async () =\u003e {\n      const results = await db.search('articles', 'Typescript', {\n        fuzzy: { maxEdits: 1 }\n      })\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:1')\n    })\n\n    it('should find matches with edit distance 2', async () =\u003e {\n      const results = await db.search('articles', 'JavaScrip', {\n        fuzzy: { maxEdits: 2 }\n      })\n\n      expect(results).toHaveLength(1)\n      expect(results[0].key).toBe('articles:2')\n    })\n\n    it('should respect fuzzy threshold', async () =\u003e {\n      const results = await db.search('articles', 'Pythn', {\n        fuzzy: { maxEdits: 1 }\n      })\n\n      expect(results).toHaveLength(1)\n    })\n\n    it('should not match beyond max edit distance', async () =\u003e {\n      const results = await db.search('articles', 'Pytho', {\n        fuzzy: { maxEdits: 0 }\n      })\n\n      expect(results).toHaveLength(0)\n    })\n\n    it('should apply fuzzy matching with prefix', async () =\u003e {\n      const results = await db.search('articles', 'progr~', {\n        fuzzy: { maxEdits: 2, prefixLength: 4 }\n      })\n\n      expect(results).toHaveLength(1)\n    })\n\n    it('should lower score for fuzzy matches', async () =\u003e {\n      await db.put('articles:exact', { content: 'TypeScript exact' })\n\n      const results = await db.search('articles', 'Typescript', {\n        fuzzy: { maxEdits: 1 }\n      })\n\n      const exactMatch = results.find(r =\u003e r.key === 'articles:exact')\n      const fuzzyMatch = results.find(r =\u003e r.key === 'articles:1')\n\n      expect(exactMatch?.score).toBeGreaterThan(fuzzyMatch?.score || 0)\n    })\n  })\n\n  describe('Highlighting', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('articles:1', {\n        title: 'Introduction to React',\n        content: 'React is a JavaScript library for building user interfaces. React components are reusable.'\n      })\n\n      await db.createTextIndex('articles', ['title', 'content'])\n    })\n\n    it('should highlight matching terms', async () =\u003e {\n      const results = await db.search('articles', 'react', {\n        highlight: true\n      })\n\n      expect(results[0].highlights).toBeDefined()\n      expect(results[0].highlights.title).toContain('\u003cmark\u003eReact\u003c/mark\u003e')\n      expect(results[0].highlights.content).toContain('\u003cmark\u003eReact\u003c/mark\u003e')\n    })\n\n    it('should support custom highlight tags', async () =\u003e {\n      const results = await db.search('articles', 'react', {\n        highlight: {\n          preTag: '\u003cem class=\"highlight\"\u003e',\n          postTag: '\u003c/em\u003e'\n        }\n      })\n\n      expect(results[0].highlights.title).toContain('\u003cem class=\"highlight\"\u003eReact\u003c/em\u003e')\n    })\n\n    it('should return snippets around matches', async () =\u003e {\n      const results = await db.search('articles', 'components', {\n        highlight: {\n          snippetLength: 50\n        }\n      })\n\n      expect(results[0].highlights.content.length).toBeLessThanOrEqual(100)\n      expect(results[0].highlights.content).toContain('\u003cmark\u003ecomponents\u003c/mark\u003e')\n    })\n\n    it('should highlight multiple matches', async () =\u003e {\n      const results = await db.search('articles', 'react javascript', {\n        highlight: true\n      })\n\n      expect(results[0].highlights.content).toContain('\u003cmark\u003eReact\u003c/mark\u003e')\n      expect(results[0].highlights.content).toContain('\u003cmark\u003eJavaScript\u003c/mark\u003e')\n    })\n  })\n\n  describe('Filters and Facets', () =\u003e {\n    beforeEach(async () =\u003e {\n      await db.put('products:1', {\n        name: 'Blue Running Shoes',\n        category: 'shoes',\n        price: 99.99,\n        inStock: true\n      })\n      await db.put('products:2', {\n        name: 'Red Running Shorts',\n        category: 'clothing',\n        price: 49.99,\n        inStock: true\n      })\n      await db.put('products:3', {\n        name: 'Blue Hiking Boots',\n        category: 'shoes',\n        price: 149.99,\n        inStock: false\n      })\n\n      await db.createTextIndex('products', 'name')\n    })\n\n    it('should combine text search with filters', async () =\u003e {\n      const results = await db.search('products', 'blue', {\n        filter: { category: 'shoes' }\n      })\n\n      expect(results).toHaveLength(2)\n      expect(results.every(r =\u003e r.document.category === 'shoes')).toBe(true)\n    })\n\n    it('should combine text search with range filters', async () =\u003e {\n      const results = await db.search('products', 'running', {\n        filter: { price: { $lt: 100 } }\n      })\n\n      expect(results).toHaveLength(2)\n    })\n\n    it('should return facet counts', async () =\u003e {\n      const results = await db.search('products', '*', {\n        facets: ['category', 'inStock']\n      })\n\n      expect(results.facets).toEqual({\n        category: {\n          shoes: 2,\n          clothing: 1\n        },\n        inStock: {\n          true: 2,\n          false: 1\n        }\n      })\n    })\n\n    it('should return facet counts respecting search filter', async () =\u003e {\n      const results = await db.search('products', 'blue', {\n        facets: ['category']\n      })\n\n      expect(results.facets.category).toEqual({\n        shoes: 2\n      })\n    })\n  })\n})\n```\n\n### GREEN Phase: Minimal Implementation to Pass Tests\n\n```typescript\n// src/search/text-index.ts\nexport interface TextIndexDefinition {\n  name: string\n  collection: string\n  fields: string[]\n  weights: Record\u003cstring, number\u003e\n  language: string\n  stemming: boolean\n}\n\nexport interface SearchOptions {\n  limit?: number\n  offset?: number\n  fuzzy?: { maxEdits: number; prefixLength?: number }\n  stemming?: boolean\n  highlight?: boolean | HighlightOptions\n  filter?: Record\u003cstring, any\u003e\n  facets?: string[]\n  exactMatchBoost?: number\n  scoringFunction?: ScoringFunction\n}\n\nexport interface SearchResult {\n  key: string\n  score: number\n  document: any\n  highlights?: Record\u003cstring, string\u003e\n}\n\nexport class TextIndex {\n  // Inverted index: term -\u003e { docId -\u003e positions[] }\n  private invertedIndex: Map\u003cstring, Map\u003cstring, number[]\u003e\u003e = new Map()\n  private documentLengths: Map\u003cstring, number\u003e = new Map()\n  private totalDocuments = 0\n  private avgDocumentLength = 0\n\n  constructor(\n    public definition: TextIndexDefinition,\n    private tokenizer: Tokenizer,\n    private stemmer: Stemmer | null\n  ) {}\n\n  async indexDocument(docId: string, document: any): Promise\u003cvoid\u003e {\n    // Remove old entries if updating\n    await this.removeDocument(docId)\n\n    let totalTerms = 0\n\n    for (const field of this.definition.fields) {\n      const text = this.getFieldValue(document, field)\n      if (!text) continue\n\n      const tokens = this.tokenizer.tokenize(text)\n      const processedTokens = this.stemmer\n        ? tokens.map(t =\u003e this.stemmer!.stem(t))\n        : tokens\n\n      totalTerms += processedTokens.length\n\n      processedTokens.forEach((token, position) =\u003e {\n        const termKey = `${field}:${token}`\n\n        if (!this.invertedIndex.has(termKey)) {\n          this.invertedIndex.set(termKey, new Map())\n        }\n\n        const docMap = this.invertedIndex.get(termKey)!\n        if (!docMap.has(docId)) {\n          docMap.set(docId, [])\n        }\n        docMap.get(docId)!.push(position)\n      })\n    }\n\n    this.documentLengths.set(docId, totalTerms)\n    this.totalDocuments++\n    this.updateAvgDocLength()\n  }\n\n  async removeDocument(docId: string): Promise\u003cvoid\u003e {\n    for (const [, docMap] of this.invertedIndex) {\n      docMap.delete(docId)\n    }\n\n    if (this.documentLengths.has(docId)) {\n      this.documentLengths.delete(docId)\n      this.totalDocuments--\n      this.updateAvgDocLength()\n    }\n  }\n\n  async search(query: string, options: SearchOptions = {}): Promise\u003cSearchResult[]\u003e {\n    const parsedQuery = this.parseQuery(query)\n    const candidateDocs = this.findCandidates(parsedQuery)\n\n    // Apply filters\n    let filteredDocs = candidateDocs\n    if (options.filter) {\n      filteredDocs = await this.applyFilters(candidateDocs, options.filter)\n    }\n\n    // Score documents\n    const scoredResults = this.scoreDocuments(filteredDocs, parsedQuery, options)\n\n    // Sort by score\n    scoredResults.sort((a, b) =\u003e b.score - a.score)\n\n    // Apply pagination\n    const start = options.offset || 0\n    const end = start + (options.limit || 10)\n    const paginatedResults = scoredResults.slice(start, end)\n\n    // Add highlights if requested\n    if (options.highlight) {\n      await this.addHighlights(paginatedResults, parsedQuery, options.highlight)\n    }\n\n    return paginatedResults\n  }\n\n  private parseQuery(query: string): ParsedQuery {\n    // Handle operators: AND, OR, NOT (-), phrases (\"\"), wildcards (*), field:term\n    const tokens: QueryToken[] = []\n\n    // Parse and return structured query\n    return { tokens, operator: 'AND' }\n  }\n\n  private findCandidates(query: ParsedQuery): Set\u003cstring\u003e {\n    const candidates = new Set\u003cstring\u003e()\n\n    for (const token of query.tokens) {\n      const term = this.stemmer ? this.stemmer.stem(token.value) : token.value\n\n      for (const field of this.definition.fields) {\n        const termKey = `${field}:${term}`\n        const docMap = this.invertedIndex.get(termKey)\n\n        if (docMap) {\n          for (const docId of docMap.keys()) {\n            candidates.add(docId)\n          }\n        }\n      }\n    }\n\n    return candidates\n  }\n\n  private scoreDocuments(\n    docIds: Set\u003cstring\u003e,\n    query: ParsedQuery,\n    options: SearchOptions\n  ): SearchResult[] {\n    const results: SearchResult[] = []\n\n    for (const docId of docIds) {\n      let score = 0\n\n      for (const token of query.tokens) {\n        const term = this.stemmer ? this.stemmer.stem(token.value) : token.value\n\n        for (const field of this.definition.fields) {\n          const termKey = `${field}:${term}`\n          const docMap = this.invertedIndex.get(termKey)\n\n          if (docMap?.has(docId)) {\n            const tf = docMap.get(docId)!.length\n            const df = docMap.size\n            const weight = this.definition.weights[field] || 1\n\n            // TF-IDF scoring\n            const idf = Math.log(this.totalDocuments / (df + 1))\n            const tfNormalized = 1 + Math.log(tf)\n\n            score += tfNormalized * idf * weight\n          }\n        }\n      }\n\n      if (score \u003e 0) {\n        results.push({\n          key: docId,\n          score,\n          document: {} // Will be populated\n        })\n      }\n    }\n\n    return results\n  }\n\n  private async addHighlights(\n    results: SearchResult[],\n    query: ParsedQuery,\n    options: HighlightOptions | boolean\n  ): Promise\u003cvoid\u003e {\n    const opts: HighlightOptions = typeof options === 'boolean'\n      ? { preTag: '\u003cmark\u003e', postTag: '\u003c/mark\u003e' }\n      : options\n\n    for (const result of results) {\n      result.highlights = {}\n\n      for (const field of this.definition.fields) {\n        const text = result.document[field]\n        if (!text) continue\n\n        let highlighted = text\n        for (const token of query.tokens) {\n          const regex = new RegExp(`(${this.escapeRegex(token.value)})`, 'gi')\n          highlighted = highlighted.replace(regex, `${opts.preTag}$1${opts.postTag}`)\n        }\n\n        result.highlights[field] = highlighted\n      }\n    }\n  }\n\n  private escapeRegex(str: string): string {\n    return str.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$\u0026')\n  }\n\n  private getFieldValue(doc: any, field: string): string | null {\n    return doc[field] ?? null\n  }\n\n  private updateAvgDocLength(): void {\n    if (this.totalDocuments === 0) {\n      this.avgDocumentLength = 0\n      return\n    }\n    let total = 0\n    for (const length of this.documentLengths.values()) {\n      total += length\n    }\n    this.avgDocumentLength = total / this.totalDocuments\n  }\n}\n\n// Tokenizer\nclass Tokenizer {\n  tokenize(text: string): string[] {\n    return text\n      .toLowerCase()\n      .split(/[\\s\\-_.,!?;:'\"()\\[\\]{}]+/)\n      .filter(t =\u003e t.length \u003e 0)\n  }\n}\n\n// Porter Stemmer (simplified)\nclass PorterStemmer implements Stemmer {\n  stem(word: string): string {\n    // Implement Porter stemming algorithm\n    // Step 1: Remove plurals, -ed, -ing\n    // Step 2: Turn terminal y to i\n    // Step 3-5: Map double suffixes to single, remove suffixes\n    return word // Placeholder\n  }\n}\n```\n\n### REFACTOR Phase: Clean Up and Optimize\n\n1. **Implement Efficient Inverted Index Storage**\n   ```typescript\n   // Use compressed posting lists\n   class CompressedPostingList {\n     // Delta encoding for document IDs\n     // Variable-byte encoding for positions\n     // Skip pointers for fast intersection\n   }\n   ```\n\n2. **Add Advanced Stemming**\n   ```typescript\n   // Full Porter2 stemmer\n   // Language-specific stemmers (Snowball)\n   // Custom stemming rules\n   ```\n\n3. **Optimize Fuzzy Matching**\n   ```typescript\n   // Levenshtein automaton for efficient fuzzy search\n   // Trigram index for fast candidate generation\n   class FuzzyMatcher {\n     private trigramIndex: Map\u003cstring, Set\u003cstring\u003e\u003e\n\n     generateCandidates(term: string, maxEdits: number): string[]\n     calculateEditDistance(a: string, b: string): number\n   }\n   ```\n\n4. **Add Relevance Tuning**\n   ```typescript\n   // BM25 scoring\n   // Custom boosting rules\n   // Learning to rank integration\n   ```\n\n5. **Performance Optimizations**\n   - Lazy loading of posting lists\n   - Query result caching\n   - Parallel query execution\n\n## Acceptance Criteria\n\n- [ ] All RED phase tests pass\n- [ ] Text indexing on single and multiple fields works\n- [ ] Basic search queries return correct results\n- [ ] Boolean operators (AND, OR, NOT) work\n- [ ] Phrase search works\n- [ ] Relevance scoring orders results correctly\n- [ ] Field weights affect scoring\n- [ ] Stemming matches word variants\n- [ ] Fuzzy matching finds typos\n- [ ] Highlighting shows matches\n- [ ] Filters combine with text search\n- [ ] Facet counts returned correctly\n\n## Dependencies\n\n- Tokenizer implementation\n- Stemmer (Porter or Snowball)\n- Edit distance algorithm for fuzzy matching\n- Query parser","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:49:02.585033-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:25:33.859469-06:00","closed_at":"2026-01-21T20:25:33.859469-06:00","close_reason":"Closed"}
{"id":"evodb-73e","title":"TDD: Add cache invalidation hooks","description":"Edge cache has no invalidation tied to CDC commits. Add hooks to invalidate stale data on writes.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:10:07.877589-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:32:02.466932-06:00","closed_at":"2026-01-20T13:32:02.466932-06:00","close_reason":"Closed"}
{"id":"evodb-75n","title":"TDD: Add RPC package tests (23% coverage)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:41.937841-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.35619-06:00","closed_at":"2026-01-20T16:49:22.35619-06:00","close_reason":"Closed"}
{"id":"evodb-7d8","title":"Simplify circuit-breaker.ts for edge execution model","description":"## Problem\ncircuit-breaker.ts (497 lines) implements full state machine pattern that may be overkill for Cloudflare Workers' ephemeral model.\n\n## TDD Approach\n1. Analyze actual circuit breaker usage patterns\n2. Implement simplified failure counter + exponential backoff\n3. Remove full state machine if not needed\n4. Verify functionality preserved\n\n## Expected Impact\n- Reduce from 497 to ~150 lines\n- ~8KB bundle reduction\n\n## Current\nFull CLOSED → OPEN → HALF_OPEN state machine with MonotonicTimeProvider abstraction.\n\n## Simplified\n```typescript\nclass SimpleCircuitBreaker {\n  private failures = 0;\n  private lastFailure = 0;\n  \n  async call\u003cT\u003e(fn: () =\u003e Promise\u003cT\u003e): Promise\u003cT\u003e {\n    if (this.isOpen()) throw new CircuitOpenError();\n    try {\n      const result = await fn();\n      this.failures = 0;\n      return result;\n    } catch (e) {\n      this.failures++;\n      this.lastFailure = Date.now();\n      throw e;\n    }\n  }\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:32.491867-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:29:06.240831-06:00","closed_at":"2026-01-21T12:29:06.240831-06:00","close_reason":"Closed"}
{"id":"evodb-7e4","title":"TDD: Add exhaustiveness checks in switch statements","description":"reader/src/index.ts:375-407 - evaluateFilter switch has silent default. Add never case or throw for unhandled types.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:45.380566-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:25:06.978637-06:00","closed_at":"2026-01-20T13:25:06.978637-06:00","close_reason":"Closed"}
{"id":"evodb-7i9","title":"TDD: Add lakehouse package tests (21% coverage)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:24.790895-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.44582-06:00","closed_at":"2026-01-20T16:49:22.44582-06:00","close_reason":"Closed"}
{"id":"evodb-7lmk","title":"Unify path validation between storage-provider and storage modules","description":"## Problem\n\nPath validation logic is duplicated between two files with slightly different implementations:\n\n**`core/src/storage-provider.ts:39-94`**:\n```typescript\n// Validates storage paths for:\n// - Empty paths\n// - Null bytes\n// - Control characters\n// - Absolute paths\n// - Path traversal (.. sequences)\n```\n\n**`core/src/storage.ts:753-808`**:\n```typescript\n// Similar validation with minor differences\n// Both throw with 'Storage path validation failed:' prefix\n```\n\n**Issues**:\n1. Code duplication - same logic in two places\n2. Potential for divergence if only one is updated\n3. Makes security auditing harder\n\n## Recommendation\n1. Extract path validation to a single utility function\n2. Create `validateStoragePath(path: string): void` in a shared location\n3. Have both modules import and use the shared function\n4. Add comprehensive test coverage for the shared function\n\n## Proposed location\n`core/src/path-utils.ts` or add to existing `core/src/guards.ts`\n\n## Files\n- `core/src/storage-provider.ts:39-94`\n- `core/src/storage.ts:753-808`","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:52.008648-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:48:52.281162-06:00","closed_at":"2026-01-21T19:48:52.281162-06:00","close_reason":"Closed","labels":["DRY","code-quality","security"]}
{"id":"evodb-7p1o","title":"DOCS: Cloudflare Workers deployment guide","description":"## Problem\nREADME mentions Cloudflare but lacks step-by-step deployment guide.\n\n## Solution\nCreate comprehensive deployment documentation:\n\n1. **Quick Start**\n   - Create R2 bucket\n   - Configure wrangler.toml\n   - Deploy first worker\n\n2. **Production Setup**\n   - Durable Objects configuration\n   - Multiple environments (dev/staging/prod)\n   - Custom domains\n   - Secrets management\n\n3. **Scaling Guide**\n   - DO sharding strategies\n   - Cache warming\n   - Cost optimization\n\n4. **Monitoring**\n   - Workers analytics integration\n   - Custom metrics\n   - Alert setup\n\n## Deliverables\n- DEPLOYMENT.md in repo root\n- Example wrangler.toml configurations\n- GitHub Actions deployment workflow\n\n## Impact\nCritical for users deploying to production","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:08.335532-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.72959-06:00","closed_at":"2026-01-21T20:08:17.72959-06:00","close_reason":"Already implemented"}
{"id":"evodb-7zu","title":"Create Unified Configuration Schema","description":"## Current State\nEach package has its own configuration types with inconsistent patterns:\n\n**@evodb/query**:\n- QueryEngineConfig, CacheConfig, QueryExecutionOptions, QueryHints\n\n**@evodb/writer**:\n- WriterOptions, PartitionModeConfig, BufferOptions, CompactionConfig\n\n**@evodb/rpc**:\n- ParentConfig, ChildConfig, ClientCapabilities, DedupConfig\n\n**@evodb/lakehouse**:\n- CreateTableOptions, CompactOptions, RecoveryConfig\n\nConfiguration is scattered and naming is inconsistent.\n\n## Proposed Improvement\n1. Create @evodb/config or add to @evodb/core:\n```typescript\nexport interface EvoDBConfig {\n  storage: StorageConfig;\n  query: QueryConfig;\n  writer: WriterConfig;\n  rpc: RpcConfig;\n  observability: ObservabilityConfig;\n}\n```\n\n2. Use consistent naming patterns:\n- All timeouts: *TimeoutMs\n- All sizes: *Bytes or *KB/MB/GB\n- All counts: max*, min*, *Count\n\n3. Provide defaults at config level:\n```typescript\nexport const DEFAULT_CONFIG: EvoDBConfig = { ... };\nexport function createConfig(overrides?: DeepPartial\u003cEvoDBConfig\u003e): EvoDBConfig;\n```\n\n4. Add validation with clear error messages:\n```typescript\nexport function validateConfig(config: EvoDBConfig): ValidationResult;\n```\n\n## Migration Path\n- Non-breaking: New config alongside existing\n- Map old config to new config internally\n- Deprecate old config types over time\n\n## Benefits\n- Single place to configure entire EvoDB instance\n- Better IDE autocomplete\n- Easier documentation\n- Environment-based configuration","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:41.60592-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:12:12.014708-06:00","closed_at":"2026-01-22T03:12:12.014708-06:00","close_reason":"Created @evodb/config package with unified configuration schema.\n\n## Changes\n\n### New Package: @evodb/config\n- Created unified EvoDBConfig interface with sections for storage, query, writer, rpc, and observability\n- Consistent naming conventions: *TimeoutMs for timeouts, *Bytes for sizes, max*/min* for limits\n- Deep partial overrides with createConfig()\n- Environment variable support with getConfigFromEnv()\n- Configuration validation with validateConfig() providing clear error messages\n\n### Configuration Schema\n- StorageConfig: Cache settings (enabled, ttlSeconds, maxSizeBytes, keyPrefix)\n- QueryConfig: maxParallelism, defaultTimeoutMs, memoryLimitBytes, enableStats, enablePlanCache\n- WriterConfig: partitionMode, bufferSize, bufferTimeoutMs, minCompactBlocks, maxRetries\n- RpcConfig: maxBatchSize, maxMessageSizeBytes, autoReconnect, heartbeatIntervalMs, enableDeduplication\n- ObservabilityConfig: logLevel, logFormat, tracingEnabled, metricsEnabled\n\n### Features\n- DEFAULT_CONFIG with sensible defaults from @evodb/core constants\n- Deep freeze prevents mutation of returned configs\n- Type-safe DeepPartial for overrides\n- Comprehensive validation with errors and warnings\n- Support for custom environment variable prefixes\n\n### TDD Approach\n- 41 unit tests passing covering all functionality\n- Tests written before implementation per issue requirements\n\n### Files Added\n- config/src/types.ts - All type definitions\n- config/src/defaults.ts - Default configuration values  \n- config/src/config.ts - createConfig, mergeConfigs, getConfigFromEnv\n- config/src/validation.ts - validateConfig with validation rules\n- config/src/index.ts - Public API exports\n- config/src/__tests__/config.unit.test.ts - Comprehensive tests\n- config/README.md - Documentation","labels":["DX","architecture","configuration"]}
{"id":"evodb-80q","title":"Make sparse null bitmap the default representation","description":"## Problem\nDense boolean arrays are still default for null bitmaps. SparseNullSet exists but requires opt-in.\n\n## TDD Approach\n1. Write tests for automatic sparse/dense selection\n2. Implement heuristic: if null rate \u003c 10%, use SparseNullSet\n3. Update unpackBits() to return appropriate representation\n4. Verify memory savings\n\n## Expected Impact\n- 100KB saved per 100K elements when sparse\n- Automatic optimization\n\n## Implementation\n```typescript\nfunction unpackBits(bytes: Uint8Array, count: number) {\n  const nullCount = countSetBits(bytes, count);\n  if (nullCount / count \u003c 0.1) {\n    return SparseNullSet.fromBitmap(bytes, count);\n  }\n  return denseUnpack(bytes, count);\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:33.761648-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:56:55.766833-06:00","closed_at":"2026-01-21T12:56:55.766833-06:00","close_reason":"Closed"}
{"id":"evodb-85mc","title":"TDD: Unit tests for RPC client.ts","description":"## Overview\nAdd comprehensive unit tests for the RPC client module to ensure reliable client-side communication.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\nWrite unit tests covering all client functionality:\n\n```typescript\ndescribe('RPC Client', () =\u003e {\n  describe('connection management', () =\u003e {\n    it('should establish WebSocket connection on init', async () =\u003e {\n      // Verify connection handshake\n    });\n    \n    it('should emit connected event on successful connection', async () =\u003e {\n      // Event emission verification\n    });\n    \n    it('should handle connection timeout', async () =\u003e {\n      // Timeout after configured duration\n    });\n    \n    it('should validate server identity on connect', async () =\u003e {\n      // Authentication/handshake verification\n    });\n  });\n  \n  describe('send/receive', () =\u003e {\n    it('should serialize and send RPC request', async () =\u003e {\n      // Request formatting and transmission\n    });\n    \n    it('should match response to pending request', async () =\u003e {\n      // Request ID correlation\n    });\n    \n    it('should handle response timeout', async () =\u003e {\n      // Pending request cleanup after timeout\n    });\n    \n    it('should support streaming responses', async () =\u003e {\n      // Multi-part response handling\n    });\n    \n    it('should handle server push messages', async () =\u003e {\n      // Unsolicited message handling\n    });\n  });\n  \n  describe('error handling', () =\u003e {\n    it('should reject pending requests on disconnect', async () =\u003e {\n      // Clean rejection of in-flight requests\n    });\n    \n    it('should parse and propagate server errors', async () =\u003e {\n      // Error deserialization and re-throwing\n    });\n    \n    it('should handle malformed responses gracefully', async () =\u003e {\n      // Invalid JSON, missing fields, etc.\n    });\n    \n    it('should emit error events for connection issues', async () =\u003e {\n      // Observable error stream\n    });\n  });\n});\n```\n\n### GREEN Phase - Add Tests with Mocked WebSocket\n\n1. Create mock WebSocket implementation:\n   ```typescript\n   class MockWebSocket {\n     onopen: () =\u003e void;\n     onmessage: (e: MessageEvent) =\u003e void;\n     onclose: () =\u003e void;\n     onerror: (e: Error) =\u003e void;\n     \n     simulateOpen(): void;\n     simulateMessage(data: any): void;\n     simulateClose(code: number): void;\n     simulateError(error: Error): void;\n   }\n   ```\n\n2. Implement test fixtures:\n   - Pre-configured client instances\n   - Common request/response patterns\n   - Error scenario helpers\n\n3. Make all unit tests pass\n\n### REFACTOR Phase - Advanced Tests\n\n1. Add timeout tests:\n   ```typescript\n   describe('timeouts', () =\u003e {\n     it('should respect per-request timeout override', async () =\u003e {});\n     it('should clean up resources after timeout', async () =\u003e {});\n     it('should support timeout cancellation', async () =\u003e {});\n   });\n   ```\n\n2. Add reconnection tests:\n   ```typescript\n   describe('reconnection', () =\u003e {\n     it('should auto-reconnect after disconnect', async () =\u003e {});\n     it('should use exponential backoff', async () =\u003e {});\n     it('should restore subscriptions after reconnect', async () =\u003e {});\n     it('should respect max retry limit', async () =\u003e {});\n   });\n   ```\n\n3. Code quality improvements:\n   - Extract reusable test utilities\n   - Add test documentation\n   - Improve assertion messages\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] Mock WebSocket implementation complete\n- [ ] All GREEN phase tests passing\n- [ ] Timeout and reconnection tests added\n- [ ] 100% branch coverage for client.ts\n- [ ] Tests run in \u003c 5 seconds","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:19.372938-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:10:25.208343-06:00","closed_at":"2026-01-21T20:10:25.208343-06:00","close_reason":"Closed"}
{"id":"evodb-880","title":"TDD: Add integration tests for writer flush atomicity","description":"## Problem\nThe writer package has unit tests for individual components but lacks integration tests verifying atomic flush behavior end-to-end. The `atomic-flush.ts` module is critical for data integrity but integration scenarios are undertested.\n\n## Coverage Gap\n- Atomic flush with concurrent CDC ingestion\n- Recovery after partial flush failure\n- Consistency between DO storage and R2\n- Block manifest updates during compaction\n\n## Acceptance Criteria\n- [ ] Create `atomic-flush.integration.test.ts` \n- [ ] Test flush completes atomically under concurrent CDC writes\n- [ ] Test recovery path when flush fails mid-operation\n- [ ] Test that DO state and R2 blocks remain consistent\n- [ ] Test compaction doesn't lose data during concurrent writes\n- [ ] Use chaos testing utilities to simulate failures\n\n## TDD Approach\nDefine test scenarios for:\n1. Happy path: CDC -\u003e buffer -\u003e flush -\u003e R2 consistency\n2. Failure during flush: verify rollback or recovery\n3. Concurrent flushes from multiple sources\n4. Compaction during active writes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:38.780512-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:53:15.475512-06:00","closed_at":"2026-01-21T19:53:15.475512-06:00","close_reason":"Closed","labels":["integration","tdd","testing","writer"]}
{"id":"evodb-8dmc","title":"TDD: Add lance-reader vector search accuracy tests","description":"## Problem\nThe lance-reader has tests for IVF-PQ and HNSW index operations but lacks tests verifying search result accuracy (recall). Vector search implementations can have subtle bugs that return incorrect results.\n\n## Coverage Gap\n- No recall measurement tests\n- No tests comparing approximate vs exact search\n- No tests for edge cases in distance functions\n- No tests for search with filters\n\n## Acceptance Criteria\n- [ ] Create `vector-search-accuracy.unit.test.ts`\n- [ ] Test IVF-PQ recall at different nprobe values\n- [ ] Test HNSW recall at different efSearch values\n- [ ] Test distance function edge cases (zero vectors, identical vectors)\n- [ ] Test filtered vector search returns correct results\n- [ ] Establish minimum recall thresholds for different configurations\n\n## TDD Approach\n1. Generate known test vectors with predictable neighbors\n2. Define expected recall at different accuracy/speed tradeoffs\n3. Test distance functions with edge cases\n4. Verify filtered search doesn't miss valid results","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:58.571204-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:17:47.7241-06:00","closed_at":"2026-01-21T20:17:47.7241-06:00","close_reason":"Closed","labels":["lance-reader","tdd","testing","vector"]}
{"id":"evodb-8ee","title":"Add sideEffects: false to all packages","description":"## Problem\nOnly 2 of 13 packages declare sideEffects: false. Bundlers can't fully eliminate unused code.\n\n## TDD Approach\n1. Write test that verifies all package.json files have sideEffects: false\n2. Add the field to all packages\n3. Verify tree-shaking works with sample consumer\n\n## Expected Impact\n- 10-20% additional bundle reduction for consumers\n- Better tree-shaking support\n\n## Implementation\nAdd to each package.json:\n```json\n\"sideEffects\": false\n```","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:15.806434-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:08:40.271764-06:00","closed_at":"2026-01-21T12:08:40.271764-06:00","close_reason":"Closed"}
{"id":"evodb-8mb","title":"TDD: Add subrequest budget tracking with clear errors","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:06.744007-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.656803-06:00","closed_at":"2026-01-20T17:49:16.656803-06:00","close_reason":"Closed","external_ref":"gh-111"}
{"id":"evodb-93h","title":"Extract Shared Query Operations to Core","description":"## Current State\nQuery-related operations are partially shared but still duplicated:\n\n- Filter evaluation exists in both QueryEngine and PartitionScanner\n- Aggregation logic is in QueryEngine but also in AggregationEngine\n- Sorting/comparison logic is used in multiple places\n- @evodb/core exports `evaluateFilter`, `compareValues`, `compareForSort` but usage is inconsistent\n\nThe query package imports from core:\n```typescript\nimport { getNestedValue, setNestedValue, compareValues, evaluateFilter, compareForSort } from '@evodb/core';\n```\n\nBut then has local wrappers that add direction handling.\n\n## Proposed Improvement\n1. Move ALL query operations to @evodb/core/query:\n   - Filter operators (eq, gt, gte, lt, lte, between, in, like, isNull, isNotNull)\n   - Aggregation functions (count, sum, avg, min, max, stddev, variance, etc.)\n   - Sort comparators with direction and null handling\n   - Projection logic\n2. Have @evodb/query use ONLY core query operations\n3. Remove local implementations in query/engine.ts\n\n## Files to Modify\n- core/src/query-ops.ts: Add all aggregation functions\n- core/src/query/index.ts: Export complete query toolkit\n- query/src/engine.ts: Remove local implementations\n- query/src/simple-engine.ts: Remove duplicated logic\n\n## Benefits\n- True single source of truth for query semantics\n- Easier to test edge cases in one place\n- Consistent behavior across all query paths\n\n## Edge Computing Impact\n- Smaller bundle size (shared code)\n- Same behavior in snippets and workers","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:33.719467-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:20:32.380103-06:00","closed_at":"2026-01-21T20:20:32.380103-06:00","close_reason":"Closed","labels":["DRY","architecture","query"]}
{"id":"evodb-95e","title":"TDD: Add vitest project stratification (unit/integration/e2e)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:53.036517-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.68336-06:00","closed_at":"2026-01-20T17:49:16.68336-06:00","close_reason":"Closed","external_ref":"gh-112"}
{"id":"evodb-988","title":"TDD: Fix dependency references (file: to workspace:*)","description":"query/package.json and benchmark/package.json use file:../ instead of workspace:*. Causes build/publishing issues.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:20.823419-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:20:51.081991-06:00","closed_at":"2026-01-20T13:20:51.081991-06:00","close_reason":"Closed"}
{"id":"evodb-9ba","title":"Add manifest versioning support","description":"TDD: No manifest versioning for schema evolution. Add version field and migration support to manifest format.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:01.981587-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:27:54.10693-06:00","closed_at":"2026-01-20T18:27:54.106932-06:00"}
{"id":"evodb-9fsh","title":"PRODUCT: Query caching layer","description":"## Problem\nCache API is mentioned but there's no high-level query caching abstraction.\n\n## Solution\nAdd query result caching:\n```typescript\n// Enable query caching\nconst db = new EvoDB({\n  cache: {\n    enabled: true,\n    ttl: 3600,\n    invalidation: 'auto'\n  }\n});\n\n// Cached queries\nconst users = await db.query('users')\n  .where('active', '=', true)\n  .cache({ ttl: 300 })  // Cache for 5 minutes\n  .execute();\n\n// Cache keys based on query + params\nconst stats = await db.query('orders')\n  .aggregate('sum', 'total', 'revenue')\n  .cache({ key: 'daily-revenue', ttl: 60 })\n  .execute();\n\n// Manual invalidation\nawait db.cache.invalidate('users');\nawait db.cache.invalidate({ key: 'daily-revenue' });\n```\n\n## Impact\nReduces R2 reads and improves latency","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:09.440143-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.891492-06:00","closed_at":"2026-01-21T20:08:17.891492-06:00","close_reason":"Already implemented"}
{"id":"evodb-9gs","title":"TDD: Decompose writer monolith with strategy pattern","description":"writer package has 5600+ LOC. Extract buffer manager, R2 writer, compactor into separate strategy implementations.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:17.938767-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:17:28.456785-06:00","closed_at":"2026-01-20T13:17:28.456785-06:00","close_reason":"Closed"}
{"id":"evodb-9ie","title":"Add Circuit Breaker and Retry Patterns to RPC","description":"## Current State\nThe @evodb/rpc package handles DO-to-DO WebSocket communication for CDC but lacks:\n1. Circuit breaker for failing connections\n2. Retry patterns with exponential backoff\n3. Bulkhead isolation for preventing cascade failures\n4. Health checks for parent DO availability\n\n@evodb/core has a circuit-breaker.ts file but it's not integrated with RPC.\n\n## Proposed Improvement\n1. Integrate circuit breaker pattern into LakehouseRpcClient:\n   - Track failure rates per connection\n   - Open circuit after N consecutive failures\n   - Half-open state for recovery probing\n   - Configurable thresholds\n\n2. Add retry with exponential backoff:\n   - Configurable max retries\n   - Jitter to prevent thundering herd\n   - Retry budget per time window\n\n3. Add bulkhead pattern:\n   - Limit concurrent connections per child DO\n   - Queue overflow handling\n\n4. Add health check endpoint to parent DO:\n   - /health endpoint\n   - Connection warmup on reconnect\n\n## Files to Modify\n- rpc/src/client.ts: Add circuit breaker, retry logic\n- rpc/src/server.ts: Add health endpoint\n- rpc/src/types.ts: Add config types\n\n## Edge Computing Impact\n- Better reliability under Durable Object constraints\n- Prevents connection storms during outages\n- Respects DO hibernation patterns","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:18.456216-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:11:01.680199-06:00","closed_at":"2026-01-21T20:11:01.680199-06:00","close_reason":"Closed","labels":["architecture","reliability","rpc"]}
{"id":"evodb-9ld","title":"Extract logging.ts to optional @evodb/observability plugin","description":"## Problem\nlogging.ts (501 lines, ~12KB) provides structured logging but defaults to noop anyway.\n\n## TDD Approach\n1. Write tests for logging in @evodb/observability\n2. Move logging.ts to observability package\n3. Keep minimal Logger interface in core\n4. Verify core bundle reduction\n\n## Expected Impact\n- ~12KB reduction from core bundle\n\n## Implementation\n- @evodb/observability/logging entry point\n- Core keeps 20-line Logger interface\n- Plugin provides JSON formatter, child loggers, etc.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:23.100813-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:31:15.694918-06:00","closed_at":"2026-01-21T12:31:15.694918-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-9ld","depends_on_id":"evodb-cpx","type":"blocks","created_at":"2026-01-21T12:05:47.331815-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-9s8","title":"Create focused entry points instead of barrel exports","description":"## Problem\nRoot index.ts uses 'export *' from 18 modules. Users importing from @evodb/core get 460KB when they may only need 8KB.\n\n## TDD Approach\n1. Write tests verifying each entry point exports correct subset\n2. Create focused entry points in package.json exports map\n3. Document recommended imports in README\n4. Verify tree-shaking works with focused imports\n\n## Expected Impact\n- 90% bundle reduction for minimal consumers\n- Better developer experience\n\n## Implementation\n```json\n{\n  \"exports\": {\n    \".\": \"./dist/index.js\",\n    \"./types\": \"./dist/types/index.js\",\n    \"./encoding\": \"./dist/encoding/index.js\",\n    \"./shredding\": \"./dist/shred.js\",\n    \"./query\": \"./dist/query-ops.js\",\n    \"./storage\": \"./dist/storage/index.js\"\n  }\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:14.43387-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:45:46.504-06:00","closed_at":"2026-01-21T12:45:46.504-06:00","close_reason":"Closed"}
{"id":"evodb-9t3","title":"TDD: Complete RPC idempotency/deduplication","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:36.211104-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:09:54.263874-06:00","closed_at":"2026-01-20T14:09:54.263874-06:00","close_reason":"Closed"}
{"id":"evodb-9t6","title":"TDD: Add R2 circuit breakers","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:18.745185-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.127139-06:00","closed_at":"2026-01-20T16:49:22.127139-06:00","close_reason":"Closed"}
{"id":"evodb-9yyz","title":"Implement Streaming Architecture for Large Result Sets","description":"## Current State\nThe QueryEngine has executeStream() but:\n1. It loads ALL rows into memory first, then streams\n2. Memory tracking happens after full load\n3. No backpressure mechanism for slow consumers\n\n```typescript\n// Current implementation (simplified)\nasync executeStream\u003cT\u003e(query: Query): Promise\u003cStreamingQueryResult\u003cT\u003e\u003e {\n  // Loads everything into memory!\n  let rows: Record\u003cstring, unknown\u003e[];\n  rows = streamDataSourceWithRows.getTableRows(query.table) ?? [];\n  // Then streams from memory\n  return { rows: rowIterator, ... };\n}\n```\n\n## Proposed Improvement\n1. True streaming from storage:\n```typescript\nasync *executeStream(query: Query): AsyncIterableIterator\u003cRow\u003e {\n  for (const partition of partitions) {\n    for await (const row of streamPartition(partition)) {\n      if (matchesPredicates(row, query.predicates)) {\n        yield row;\n      }\n    }\n  }\n}\n```\n\n2. Backpressure support:\n```typescript\ninterface StreamConfig {\n  highWaterMark: number;  // Pause reading when buffer exceeds\n  lowWaterMark: number;   // Resume reading when buffer drops below\n}\n```\n\n3. Streaming aggregations:\n```typescript\n// Stream-friendly aggregations (single-pass)\nconst countAgg = createStreamingCount();\nconst sumAgg = createStreamingSum('amount');\n\nfor await (const row of stream) {\n  countAgg.update(row);\n  sumAgg.update(row);\n}\n```\n\n## Files to Modify\n- query/src/engine.ts: True streaming implementation\n- core/src/query-ops.ts: Streaming aggregation functions\n- lakehouse/src/r2.ts: Streaming block reader\n\n## Edge Computing Impact\n- Handle larger-than-memory queries in Workers\n- Better memory utilization (128MB limit)\n- Enables real-time CDC processing","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:19.550011-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:32:22.570732-06:00","closed_at":"2026-01-21T20:32:22.570732-06:00","close_reason":"Closed","labels":["architecture","query","scalability","streaming"]}
{"id":"evodb-a0q","title":"TDD: Add corrupted block error handling","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:47.186813-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.040975-06:00","closed_at":"2026-01-20T16:55:12.040975-06:00","close_reason":"Closed","external_ref":"gh-75"}
{"id":"evodb-a1r6","title":"WRITE: Create CONTRIBUTING.md for open source contributors","description":"## Overview\nCreate comprehensive contributing guide to enable open source contributors to effectively participate in the project.\n\n## Tasks\n\n### Development Setup Instructions\n- Prerequisites:\n  - Node.js version requirements\n  - pnpm installation\n  - Git configuration\n- Repository setup:\n  ```bash\n  git clone https://github.com/evodb/evodb.git\n  cd evodb\n  pnpm install\n  pnpm build\n  pnpm test\n  ```\n- IDE setup recommendations:\n  - VS Code extensions\n  - ESLint/Prettier configuration\n  - Debugging setup\n\n### TDD Workflow Explanation\n- Test-driven development process:\n  1. Write failing test first\n  2. Implement minimal code to pass\n  3. Refactor with confidence\n- Test file organization\n- Running specific tests\n- Coverage requirements\n- Using `bd` for issue tracking during development\n\n### Code Style Guide\n- TypeScript conventions:\n  - Naming conventions\n  - Type annotation requirements\n  - Module organization\n- Documentation standards:\n  - JSDoc requirements\n  - README maintenance\n  - Changelog updates\n- Commit message format:\n  - Conventional commits\n  - Scope definitions\n  - Breaking change notation\n\n### PR Process and Review Guidelines\n- Branch naming conventions\n- PR template usage\n- Review checklist:\n  - [ ] Tests pass\n  - [ ] Coverage maintained\n  - [ ] Documentation updated\n  - [ ] Changeset added\n- Review response expectations\n- Merge requirements\n\n## Acceptance Criteria\n- [ ] New contributor can set up environment in \u003c 15 minutes\n- [ ] TDD workflow is clearly explained with examples\n- [ ] Code style is enforceable via linting\n- [ ] PR process has no ambiguity","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:33.766161-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:59:42.000656-06:00","closed_at":"2026-01-21T19:59:42.000656-06:00","close_reason":"Closed"}
{"id":"evodb-a2x","title":"Implement lazy bitmap unpacking with iterator","description":"## Problem\nunpackBits() always creates full boolean[] array even when only checking a few values.\n\n## TDD Approach\n1. Write benchmark for sparse bitmap access patterns\n2. Implement lazy iterator that reads bits on demand\n3. Keep eager unpack for dense access patterns\n4. Verify memory reduction\n\n## Expected Impact\n- Avoid 100KB allocation for 100K element column\n- Faster for sparse access patterns\n\n## Current Code (encode.ts:335-341)\n```typescript\nexport function unpackBits(bytes: Uint8Array, count: number): boolean[] {\n  const bits: boolean[] = new Array(count);  // Always allocates\n  for (let i = 0; i \u003c count; i++) {\n    bits[i] = (bytes[i \u003e\u003e\u003e 3] \u0026 (1 \u003c\u003c (i \u0026 7))) !== 0;\n  }\n  return bits;\n}\n```\n\n## Fix\n```typescript\nexport function unpackBitsLazy(bytes: Uint8Array, count: number) {\n  return {\n    get(i: number): boolean {\n      return (bytes[i \u003e\u003e\u003e 3] \u0026 (1 \u003c\u003c (i \u0026 7))) !== 0;\n    },\n    toArray(): boolean[] { /* eager if needed */ }\n  };\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:38.025324-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:37:00.45985-06:00","closed_at":"2026-01-21T12:37:00.45985-06:00","close_reason":"Closed"}
{"id":"evodb-a5p","title":"TDD: Add hard buffer size limits with overflow error","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:33.756513-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.673644-06:00","closed_at":"2026-01-20T17:49:16.673644-06:00","close_reason":"Closed","external_ref":"gh-113"}
{"id":"evodb-a7h","title":"Query engine unit test coverage","description":"TDD: Query engine lacks unit tests. Add comprehensive unit tests for query planning, optimization, and execution.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:04.137445-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:27:49.609734-06:00","closed_at":"2026-01-20T18:27:49.609737-06:00"}
{"id":"evodb-a9o","title":"Exclude source maps from npm distribution","description":"## Problem\nSource maps account for 43% of core package size (556KB). They're included in npm distribution but not needed by consumers.\n\n## TDD Approach\n1. Write test that verifies published package excludes .map files\n2. Add 'files' field to package.json or .npmignore\n3. Verify bundle size reduction\n\n## Expected Impact\n- 556KB reduction from core alone\n- ~800KB+ total across all packages\n\n## Implementation\nAdd to each package.json:\n```json\n\"files\": [\"dist/**/*.js\", \"dist/**/*.d.ts\", \"README.md\"]\n```","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:14.453447-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:08:57.086568-06:00","closed_at":"2026-01-21T12:08:57.086568-06:00","close_reason":"Closed"}
{"id":"evodb-ag1s","title":"TDD: Add observability integration tests","description":"## Problem\nThe observability package has unit tests for logging, tracing, and metrics in isolation, but no integration tests verifying they work together correctly in a realistic scenario.\n\n## Coverage Gap\n- No tests for trace context propagation across async boundaries\n- No tests for metrics collection during actual operations\n- No tests for log correlation with trace IDs\n- No tests for observability overhead impact\n\n## Acceptance Criteria\n- [ ] Create `observability.integration.test.ts`\n- [ ] Test trace context propagates through async operations\n- [ ] Test metrics are collected during query execution\n- [ ] Test logs include correct trace/span IDs\n- [ ] Test disabling observability has minimal overhead\n- [ ] Test sampling configuration works correctly\n\n## TDD Approach\n1. Define expected trace propagation behavior\n2. Write tests for metric collection accuracy\n3. Verify log-trace correlation\n4. Measure observability overhead","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:34.872507-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T02:27:17.445889-06:00","closed_at":"2026-01-22T02:27:17.445889-06:00","close_reason":"Closed","labels":["observability","tdd","testing"]}
{"id":"evodb-ala","title":"TDD: Consolidate storage interfaces in core","description":"ObjectStorageAdapter defined in both core and lakehouse. Move all storage types to core, re-export from others.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:10:08.945735-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:30:55.109529-06:00","closed_at":"2026-01-20T13:30:55.109529-06:00","close_reason":"Closed"}
{"id":"evodb-amb","title":"Consolidate Storage Interface Fragmentation","description":"## Current State\nThe codebase has 5+ overlapping storage abstractions:\n\n1. **Storage** (core/storage.ts): `read/write/list/delete`\n2. **ObjectStorageAdapter** (core/storage.ts): `put/get/list/head/delete`\n3. **StorageAdapter** (core/types.ts): `writeBlock/readBlock/listBlocks/deleteBlock`\n4. **R2StorageAdapter** (lakehouse/types.ts): `readJson/writeJson/readBinary/writeBinary`\n5. **StorageAdapter** (lance-reader/types.ts): `get/getRange/list/exists`\n6. **StorageProvider** (core/storage-provider.ts): `get/put/delete/list/exists` (newest)\n\nThe StorageProvider is the intended canonical interface (issue evodb-v3l), but the others remain.\n\n## Proposed Improvement\n1. Complete migration to StorageProvider as the single interface\n2. Remove legacy interfaces with deprecation period\n3. Update all packages to use StorageProvider:\n   - @evodb/lakehouse: Replace R2StorageAdapter\n   - @evodb/lance-reader: Replace StorageAdapter\n   - @evodb/writer: Use StorageProvider\n   - @evodb/query: Use StorageProvider\n4. Keep adapter functions for backward compatibility during migration\n\n## Migration Path\n1. Mark Storage, ObjectStorageAdapter, StorageAdapter as @deprecated\n2. Add console.warn on first usage in dev mode\n3. Update all internal code to use StorageProvider\n4. Remove deprecated interfaces in v0.3.0\n\n## Breaking Changes\n- External code using StorageAdapter will need to migrate\n- Method naming changes: read-\u003eget, write-\u003eput, listBlocks-\u003elist\n\n## Edge Computing Impact\n- Reduced bundle size (single interface)\n- No runtime impact","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:11.808066-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:51:42.042666-06:00","closed_at":"2026-01-21T19:51:42.042666-06:00","close_reason":"Closed","labels":["architecture","breaking-change","storage"]}
{"id":"evodb-amp7","title":"TDD: Implement row-level security (RLS)","description":"## Overview\nImplement row-level security (RLS) to enable fine-grained access control at the row level based on user context and policies.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. Policy Definition Tests\n```typescript\ndescribe('RLS Policy Definition', () =\u003e {\n  it('should define a SELECT policy', async () =\u003e {\n    await db.execute(`\n      CREATE POLICY user_select_own ON users\n      FOR SELECT\n      USING (id = current_user_id())\n    `);\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE table_name = ?', ['users']);\n    expect(policies).toContainEqual({\n      name: 'user_select_own',\n      table_name: 'users',\n      operation: 'SELECT',\n      using_expr: 'id = current_user_id()',\n      check_expr: null,\n    });\n  });\n\n  it('should define an INSERT policy with CHECK', async () =\u003e {\n    await db.execute(`\n      CREATE POLICY user_insert_own ON users\n      FOR INSERT\n      WITH CHECK (created_by = current_user_id())\n    `);\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE name = ?', ['user_insert_own']);\n    expect(policies[0].check_expr).toBe('created_by = current_user_id()');\n  });\n\n  it('should define an UPDATE policy with USING and CHECK', async () =\u003e {\n    await db.execute(`\n      CREATE POLICY user_update_own ON users\n      FOR UPDATE\n      USING (owner_id = current_user_id())\n      WITH CHECK (owner_id = current_user_id())\n    `);\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE name = ?', ['user_update_own']);\n    expect(policies[0].using_expr).toBe('owner_id = current_user_id()');\n    expect(policies[0].check_expr).toBe('owner_id = current_user_id()');\n  });\n\n  it('should define a DELETE policy', async () =\u003e {\n    await db.execute(`\n      CREATE POLICY user_delete_own ON users\n      FOR DELETE\n      USING (owner_id = current_user_id() OR is_admin())\n    `);\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE name = ?', ['user_delete_own']);\n    expect(policies[0].operation).toBe('DELETE');\n  });\n\n  it('should define policy for ALL operations', async () =\u003e {\n    await db.execute(`\n      CREATE POLICY user_all ON users\n      FOR ALL\n      USING (tenant_id = current_tenant_id())\n    `);\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE name = ?', ['user_all']);\n    expect(policies[0].operation).toBe('ALL');\n  });\n\n  it('should enable RLS on a table', async () =\u003e {\n    await db.execute('ALTER TABLE users ENABLE ROW LEVEL SECURITY');\n    \n    const tables = await db.query('SELECT * FROM evodb_rls_tables WHERE table_name = ?', ['users']);\n    expect(tables[0].rls_enabled).toBe(true);\n  });\n\n  it('should drop a policy', async () =\u003e {\n    await db.execute('CREATE POLICY temp_policy ON users FOR SELECT USING (true)');\n    await db.execute('DROP POLICY temp_policy ON users');\n    \n    const policies = await db.query('SELECT * FROM evodb_policies WHERE name = ?', ['temp_policy']);\n    expect(policies).toHaveLength(0);\n  });\n});\n```\n\n#### 2. Policy Enforcement Tests\n```typescript\ndescribe('RLS Policy Enforcement', () =\u003e {\n  beforeEach(async () =\u003e {\n    // Setup: Create table with RLS\n    await db.execute('CREATE TABLE documents (id INTEGER PRIMARY KEY, owner_id INTEGER, content TEXT)');\n    await db.execute('ALTER TABLE documents ENABLE ROW LEVEL SECURITY');\n    await db.execute(`\n      CREATE POLICY doc_owner_select ON documents\n      FOR SELECT USING (owner_id = current_user_id())\n    `);\n    await db.execute(`\n      CREATE POLICY doc_owner_modify ON documents\n      FOR ALL USING (owner_id = current_user_id())\n      WITH CHECK (owner_id = current_user_id())\n    `);\n    \n    // Insert test data as admin\n    await db.withBypassRLS(async () =\u003e {\n      await db.execute('INSERT INTO documents (id, owner_id, content) VALUES (1, 100, ?)', ['User 100 doc']);\n      await db.execute('INSERT INTO documents (id, owner_id, content) VALUES (2, 200, ?)', ['User 200 doc']);\n    });\n  });\n\n  it('should filter SELECT results based on policy', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    const docs = await db.query('SELECT * FROM documents');\n    \n    expect(docs).toHaveLength(1);\n    expect(docs[0].owner_id).toBe(100);\n  });\n\n  it('should prevent SELECT of unauthorized rows', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    const docs = await db.query('SELECT * FROM documents WHERE id = ?', [2]);\n    \n    expect(docs).toHaveLength(0); // User 100 cannot see user 200's doc\n  });\n\n  it('should allow INSERT when CHECK passes', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    await db.execute('INSERT INTO documents (id, owner_id, content) VALUES (3, 100, ?)', ['New doc']);\n    \n    const docs = await db.query('SELECT * FROM documents WHERE id = ?', [3]);\n    expect(docs).toHaveLength(1);\n  });\n\n  it('should prevent INSERT when CHECK fails', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    \n    await expect(\n      db.execute('INSERT INTO documents (id, owner_id, content) VALUES (4, 200, ?)', ['Sneaky'])\n    ).rejects.toThrow('RLS policy violation');\n  });\n\n  it('should allow UPDATE of owned rows', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    await db.execute('UPDATE documents SET content = ? WHERE id = ?', ['Updated', 1]);\n    \n    const docs = await db.query('SELECT * FROM documents WHERE id = ?', [1]);\n    expect(docs[0].content).toBe('Updated');\n  });\n\n  it('should prevent UPDATE of unauthorized rows', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    const result = await db.execute('UPDATE documents SET content = ? WHERE id = ?', ['Hacked', 2]);\n    \n    expect(result.changes).toBe(0); // No rows updated\n  });\n\n  it('should prevent UPDATE that violates CHECK', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    \n    await expect(\n      db.execute('UPDATE documents SET owner_id = ? WHERE id = ?', [999, 1])\n    ).rejects.toThrow('RLS policy violation');\n  });\n\n  it('should allow DELETE of owned rows', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    const result = await db.execute('DELETE FROM documents WHERE id = ?', [1]);\n    \n    expect(result.changes).toBe(1);\n  });\n\n  it('should prevent DELETE of unauthorized rows', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    const result = await db.execute('DELETE FROM documents WHERE id = ?', [2]);\n    \n    expect(result.changes).toBe(0); // No rows deleted\n  });\n\n  it('should apply RLS to JOINs', async () =\u003e {\n    await db.execute('CREATE TABLE comments (id INTEGER PRIMARY KEY, doc_id INTEGER, text TEXT)');\n    await db.withBypassRLS(async () =\u003e {\n      await db.execute('INSERT INTO comments VALUES (1, 1, ?)', ['Comment on doc 1']);\n      await db.execute('INSERT INTO comments VALUES (2, 2, ?)', ['Comment on doc 2']);\n    });\n    \n    await db.setContext({ user_id: 100 });\n    const results = await db.query(`\n      SELECT d.content, c.text \n      FROM documents d \n      JOIN comments c ON d.id = c.doc_id\n    `);\n    \n    expect(results).toHaveLength(1);\n    expect(results[0].text).toBe('Comment on doc 1');\n  });\n});\n```\n\n#### 3. Policy Bypass Tests\n```typescript\ndescribe('RLS Policy Bypass', () =\u003e {\n  it('should bypass RLS with explicit permission', async () =\u003e {\n    await db.setContext({ user_id: 100 });\n    \n    await db.withBypassRLS(async () =\u003e {\n      const docs = await db.query('SELECT * FROM documents');\n      expect(docs.length).toBeGreaterThan(1); // See all docs\n    });\n    \n    // After block, RLS is re-enabled\n    const docs = await db.query('SELECT * FROM documents');\n    expect(docs).toHaveLength(1); // Back to filtered\n  });\n\n  it('should allow admin role to bypass RLS', async () =\u003e {\n    await db.setContext({ user_id: 100, role: 'admin' });\n    \n    const docs = await db.query('SELECT * FROM documents');\n    expect(docs.length).toBeGreaterThan(1);\n  });\n\n  it('should not allow bypass without proper privileges', async () =\u003e {\n    await db.setContext({ user_id: 100, role: 'user' });\n    \n    await expect(\n      db.withBypassRLS(async () =\u003e {\n        await db.query('SELECT * FROM documents');\n      })\n    ).rejects.toThrow('Insufficient privileges to bypass RLS');\n  });\n\n  it('should audit RLS bypass usage', async () =\u003e {\n    await db.setContext({ user_id: 1, role: 'admin' });\n    \n    await db.withBypassRLS(async () =\u003e {\n      await db.query('SELECT * FROM documents');\n    });\n    \n    const audit = await db.query('SELECT * FROM evodb_rls_audit ORDER BY timestamp DESC LIMIT 1');\n    expect(audit[0]).toMatchObject({\n      user_id: 1,\n      action: 'BYPASS_RLS',\n      table_name: 'documents',\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/core/src/rls/policy-engine.ts\nexport class RLSPolicyEngine {\n  private policies = new Map\u003cstring, RLSPolicy[]\u003e();\n  private enabledTables = new Set\u003cstring\u003e();\n  private contextStack: RLSContext[] = [];\n\n  async createPolicy(policy: CreatePolicyOptions): Promise\u003cvoid\u003e {\n    const { name, tableName, operation, using, check } = policy;\n    \n    // Parse and validate expressions\n    const usingAst = using ? this.parseExpression(using) : null;\n    const checkAst = check ? this.parseExpression(check) : null;\n    \n    // Store policy\n    const policies = this.policies.get(tableName) ?? [];\n    policies.push({\n      name,\n      tableName,\n      operation,\n      usingExpr: using,\n      usingAst,\n      checkExpr: check,\n      checkAst,\n    });\n    this.policies.set(tableName, policies);\n    \n    // Persist to catalog\n    await this.db.execute(\n      'INSERT INTO evodb_policies (name, table_name, operation, using_expr, check_expr) VALUES (?, ?, ?, ?, ?)',\n      [name, tableName, operation, using, check]\n    );\n  }\n\n  enableRLS(tableName: string): void {\n    this.enabledTables.add(tableName);\n    this.db.execute('UPDATE evodb_rls_tables SET rls_enabled = 1 WHERE table_name = ?', [tableName]);\n  }\n\n  setContext(context: RLSContext): void {\n    this.contextStack[0] = context;\n  }\n\n  async withBypassRLS\u003cT\u003e(fn: () =\u003e Promise\u003cT\u003e): Promise\u003cT\u003e {\n    const context = this.getCurrentContext();\n    if (!context.role || !['admin', 'superuser'].includes(context.role)) {\n      throw new Error('Insufficient privileges to bypass RLS');\n    }\n    \n    // Audit bypass\n    await this.db.execute(\n      'INSERT INTO evodb_rls_audit (user_id, action, table_name, timestamp) VALUES (?, ?, ?, ?)',\n      [context.user_id, 'BYPASS_RLS', '*', Date.now()]\n    );\n    \n    this.contextStack.push({ ...context, bypassRLS: true });\n    try {\n      return await fn();\n    } finally {\n      this.contextStack.pop();\n    }\n  }\n\n  rewriteQuery(sql: string, operation: 'SELECT' | 'UPDATE' | 'DELETE'): string {\n    const context = this.getCurrentContext();\n    if (context.bypassRLS || context.role === 'admin') {\n      return sql;\n    }\n\n    const ast = this.parser.parse(sql);\n    const tables = this.extractTables(ast);\n    \n    for (const table of tables) {\n      if (!this.enabledTables.has(table)) continue;\n      \n      const policies = this.getApplicablePolicies(table, operation);\n      if (policies.length === 0) continue;\n      \n      // Combine policies with OR (permissive by default)\n      const combined = policies\n        .map(p =\u003e this.substituteContext(p.usingExpr!, context))\n        .join(' OR ');\n      \n      ast.where = ast.where \n        ? { type: 'AND', left: ast.where, right: this.parser.parseExpr(`(${combined})`) }\n        : this.parser.parseExpr(`(${combined})`);\n    }\n    \n    return this.generator.generate(ast);\n  }\n\n  validateWrite(tableName: string, row: Record\u003cstring, unknown\u003e, operation: 'INSERT' | 'UPDATE'): void {\n    const context = this.getCurrentContext();\n    if (context.bypassRLS || context.role === 'admin') return;\n    \n    const policies = this.getApplicablePolicies(tableName, operation);\n    \n    for (const policy of policies) {\n      if (!policy.checkExpr) continue;\n      \n      const expr = this.substituteContext(policy.checkExpr, context);\n      const result = this.evaluateExpression(expr, row);\n      \n      if (!result) {\n        throw new Error(`RLS policy violation: ${policy.name}`);\n      }\n    }\n  }\n\n  private substituteContext(expr: string, context: RLSContext): string {\n    return expr\n      .replace(/current_user_id\\(\\)/g, String(context.user_id))\n      .replace(/current_tenant_id\\(\\)/g, String(context.tenant_id ?? 'NULL'))\n      .replace(/is_admin\\(\\)/g, context.role === 'admin' ? 'TRUE' : 'FALSE');\n  }\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. Policy Composition\n```typescript\n// packages/core/src/rls/policy-composer.ts\nexport class PolicyComposer {\n  // Combine policies from multiple sources\n  compose(policies: RLSPolicy[], mode: 'permissive' | 'restrictive' = 'permissive'): string {\n    if (policies.length === 0) return 'FALSE'; // Deny by default\n    \n    const exprs = policies.map(p =\u003e `(${p.usingExpr})`);\n    \n    return mode === 'permissive' \n      ? exprs.join(' OR ')\n      : exprs.join(' AND ');\n  }\n\n  // Support policy inheritance\n  inherit(childTable: string, parentTable: string, foreignKey: string): RLSPolicy {\n    const parentPolicies = this.engine.getPolicies(parentTable);\n    \n    return {\n      name: `${childTable}_inherits_${parentTable}`,\n      tableName: childTable,\n      operation: 'ALL',\n      usingExpr: `${foreignKey} IN (SELECT id FROM ${parentTable} WHERE ${this.compose(parentPolicies)})`,\n    };\n  }\n\n  // Support policy templates\n  createFromTemplate(template: PolicyTemplate, params: Record\u003cstring, unknown\u003e): RLSPolicy {\n    let expr = template.expression;\n    for (const [key, value] of Object.entries(params)) {\n      expr = expr.replace(new RegExp(`\\\\$\\\\{${key}\\\\}`, 'g'), String(value));\n    }\n    \n    return {\n      name: template.name,\n      tableName: params.tableName as string,\n      operation: template.operation,\n      usingExpr: expr,\n    };\n  }\n}\n\n// Built-in templates\nexport const POLICY_TEMPLATES = {\n  ownerOnly: {\n    name: 'owner_only',\n    operation: 'ALL',\n    expression: '${ownerColumn} = current_user_id()',\n  },\n  tenantIsolation: {\n    name: 'tenant_isolation',\n    operation: 'ALL',\n    expression: '${tenantColumn} = current_tenant_id()',\n  },\n  publicRead: {\n    name: 'public_read',\n    operation: 'SELECT',\n    expression: '${visibilityColumn} = \\'public\\'',\n  },\n};\n```\n\n#### 2. Policy Debugging\n```typescript\n// packages/core/src/rls/policy-debugger.ts\nexport class RLSDebugger {\n  // Explain why a query returned specific results\n  async explainQuery(sql: string, context: RLSContext): Promise\u003cRLSExplanation\u003e {\n    const rewritten = this.engine.rewriteQuery(sql, 'SELECT');\n    const policies = this.engine.getAppliedPolicies(sql);\n    \n    return {\n      originalQuery: sql,\n      rewrittenQuery: rewritten,\n      appliedPolicies: policies.map(p =\u003e ({\n        name: p.name,\n        expression: p.usingExpr,\n        evaluatedTo: this.evaluateForContext(p.usingExpr!, context),\n      })),\n      context: {\n        user_id: context.user_id,\n        tenant_id: context.tenant_id,\n        role: context.role,\n      },\n    };\n  }\n\n  // Check if a specific row would be visible\n  async checkRowAccess(\n    tableName: string, \n    rowId: unknown, \n    context: RLSContext\n  ): Promise\u003cRowAccessResult\u003e {\n    const row = await this.db.withBypassRLS(() =\u003e \n      this.db.queryOne(`SELECT * FROM ${tableName} WHERE id = ?`, [rowId])\n    );\n    \n    if (!row) return { accessible: false, reason: 'Row does not exist' };\n    \n    const policies = this.engine.getPolicies(tableName, 'SELECT');\n    const results: PolicyEvaluation[] = [];\n    \n    for (const policy of policies) {\n      const result = this.evaluatePolicy(policy, row, context);\n      results.push({\n        policy: policy.name,\n        expression: policy.usingExpr!,\n        result,\n        substituted: this.engine.substituteContext(policy.usingExpr!, context),\n      });\n    }\n    \n    const accessible = results.some(r =\u003e r.result); // Permissive mode\n    \n    return {\n      accessible,\n      reason: accessible \n        ? `Allowed by policy: ${results.find(r =\u003e r.result)?.policy}`\n        : 'No policy grants access',\n      evaluations: results,\n    };\n  }\n\n  // Simulate policy changes\n  async simulatePolicyChange(\n    change: PolicyChange,\n    testQueries: string[]\n  ): Promise\u003cSimulationResult[]\u003e {\n    const results: SimulationResult[] = [];\n    \n    for (const query of testQueries) {\n      const before = await this.engine.rewriteQuery(query, 'SELECT');\n      \n      // Temporarily apply change\n      if (change.type === 'add') {\n        this.engine.addPolicy(change.policy);\n      } else {\n        this.engine.removePolicy(change.policyName);\n      }\n      \n      const after = this.engine.rewriteQuery(query, 'SELECT');\n      \n      // Revert change\n      if (change.type === 'add') {\n        this.engine.removePolicy(change.policy.name);\n      } else {\n        this.engine.addPolicy(change.policy);\n      }\n      \n      results.push({\n        query,\n        beforeRewrite: before,\n        afterRewrite: after,\n        changed: before !== after,\n      });\n    }\n    \n    return results;\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] Policy definition syntax matches PostgreSQL RLS\n- [ ] Policies enforced on SELECT, INSERT, UPDATE, DELETE\n- [ ] Support for USING and WITH CHECK clauses\n- [ ] Context functions (current_user_id, etc.) work correctly\n- [ ] RLS bypass with proper authorization\n- [ ] Audit logging for security events\n- [ ] Policy composition and templates\n- [ ] Debug tools for policy troubleshooting\n\n## Labels\nsecurity, rls, access-control, policies","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:39.674484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:58:35.769439-06:00","closed_at":"2026-01-21T19:58:35.769439-06:00","close_reason":"Closed"}
{"id":"evodb-amq","title":"TDD: Add buffer config validation bounds","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:33.261654-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.162625-06:00","closed_at":"2026-01-20T16:55:12.162625-06:00","close_reason":"Closed","external_ref":"gh-76"}
{"id":"evodb-aqap","title":"TDD: Add generic constraints for better type inference","description":"## Overview\nAdd proper generic constraints (extends clauses) to improve TypeScript type inference and provide better error messages.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests for type inference in QueryBuilder\n  - Ensure field names are properly inferred\n  - Ensure return types are correct\n  - Test chained method type inference\n- [ ] Write tests for Collection generics\n  - Document type inference\n  - Field access type checking\n- [ ] Write tests for index type inference\n  - Index field types match document fields\n  - Query constraints match index capabilities\n\n### GREEN Phase - Make Tests Pass\n- [ ] Add `extends` constraints to QueryBuilder generics\n  - `T extends Record\u003cstring, unknown\u003e`\n  - `K extends keyof T`\n- [ ] Add constraints to Collection generics\n  - Document type constraints\n  - Field type constraints\n- [ ] Add constraints to index generics\n  - Ensure index fields exist on document\n- [ ] Update method signatures for better inference\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Improve error messages with template literal types\n  - `Field '${K}' does not exist on type...`\n- [ ] Add type tests using expect-type or similar\n- [ ] Document generic patterns for contributors\n- [ ] Consider conditional types for advanced inference\n\n## Acceptance Criteria\n- Generic constraints catch errors at compile time\n- Type inference works without explicit type annotations\n- Error messages clearly indicate the problem\n- IDE autocomplete works correctly","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:44.359755-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:27:26.286664-06:00","closed_at":"2026-01-21T20:27:26.286664-06:00","close_reason":"Closed"}
{"id":"evodb-as0h","title":"TDD: Add edge-cache eviction and TTL tests","description":"## Problem\nThe edge-cache package tests basic cache operations but lacks comprehensive testing of eviction policies and TTL behavior, which are critical for production cache correctness.\n\n## Coverage Gap\n- No tests for LRU eviction order\n- No tests for TTL expiration accuracy\n- No tests for cache behavior under memory pressure\n- No tests for concurrent access during eviction\n\n## Acceptance Criteria\n- [ ] Create `cache-eviction.unit.test.ts`\n- [ ] Test LRU eviction maintains correct order\n- [ ] Test TTL expiration removes entries at correct time\n- [ ] Test cache handles concurrent reads during eviction\n- [ ] Test memory limit enforcement triggers eviction\n- [ ] Test stale-while-revalidate behavior if implemented\n\n## TDD Approach\n1. Write tests defining expected LRU behavior\n2. Test time-based expiration with mocked clock\n3. Test concurrent access patterns\n4. Verify memory accounting accuracy","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:41.765598-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:10:29.229982-06:00","closed_at":"2026-01-21T20:10:29.229982-06:00","close_reason":"Closed","labels":["edge-cache","tdd","testing"]}
{"id":"evodb-asde","title":"Optimize Core Package Bundle Size with Subpath Exports","description":"## Current State\nThe @evodb/core package has 17 subpath exports in package.json:\n```json\n\"exports\": {\n  \".\": { ... },\n  \"./types\": { ... },\n  \"./encoding\": { ... },\n  \"./shredding\": { ... },\n  \"./block\": { ... },\n  // ... 12 more\n}\n```\n\nHowever, the main index.ts re-exports EVERYTHING:\n```typescript\nexport * from './errors/index.js';\nexport * from './evodb/index.js';\nexport * from './types/index.js';\n// ... all modules\n```\n\nThis means importing from '@evodb/core' brings in the entire package, defeating tree-shaking.\n\n## Proposed Improvement\n1. Audit actual import patterns across packages\n2. Make main export minimal (types only?):\n```typescript\n// @evodb/core (main)\nexport type { ... } from './types/index.js';\nexport { Type, Encoding, WalOp } from './types/index.js';\n```\n\n3. Force consumers to use subpath imports for implementations:\n```typescript\nimport { shred, unshred } from '@evodb/core/shredding';\nimport { encode, decode } from '@evodb/core/encoding';\n```\n\n4. Use package.json sideEffects: false for all subpaths\n\n## Bundle Size Analysis Needed\n- Current: Measure full bundle\n- Target: \u003c50KB for minimal import\n- Measure each subpath independently\n\n## Migration Path\n- Phase 1: Add deprecation warnings for main export\n- Phase 2: Document recommended subpath imports\n- Phase 3: Reduce main export in v0.2.0\n\n## Edge Computing Impact\n- Critical for snippets (32MB RAM limit)\n- Faster cold starts\n- Better Cloudflare bundler optimization","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:06.424618-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:19:26.269218-06:00","closed_at":"2026-01-21T20:19:26.269218-06:00","close_reason":"Closed","labels":["architecture","bundle","edge","performance"]}
{"id":"evodb-atn","title":"Add comprehensive edge case handling to partition pruning","description":"## Problem\n\nThe partition pruning logic in `lakehouse/src/partition.ts` handles most cases but could be more robust:\n\n**Potential edge cases**:\n\n1. **Empty partition values** (line 324):\n```typescript\nif (value === null || value === undefined) {\n  if ('eq' in filter) return filter.eq === null;\n  if ('in' in filter) return filter.in.includes(null as never);\n  return false;\n}\n```\nThe `null as never` cast suggests a typing issue that should be resolved.\n\n2. **Type coercion in numeric comparisons** (lines 340-358):\n```typescript\nconst numValue = typeof value === 'number' ? value : parseFloat(String(value));\n```\n- `parseFloat('abc')` returns `NaN` - this isn't handled\n- `parseFloat('')` returns `NaN`\n- Should validate the conversion result\n\n3. **Missing filter types handling** (line 361):\n```typescript\nreturn true; // Fallback for unknown filter types\n```\nShould log a warning when encountering unknown filter types.\n\n4. **Date comparisons in zone maps**:\nThe current implementation converts dates to timestamps but doesn't handle timezone edge cases.\n\n## Recommendation\n1. Add NaN checks after parseFloat\n2. Handle unknown filter types with warnings\n3. Add comprehensive tests for edge cases\n4. Consider using a stricter type for partition values\n\n## Files\n- `lakehouse/src/partition.ts:320-362`\n- `lakehouse/src/partition.ts:402-458` (zone map pruning)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:47.585597-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:23:24.230077-06:00","closed_at":"2026-01-21T20:23:24.230077-06:00","close_reason":"Closed","labels":["edge-cases","lakehouse","robustness"]}
{"id":"evodb-av9","title":"Remove console.log/error statements from production code","description":"## Problem\n\nThere are direct `console.log/error` statements scattered throughout production code instead of using the structured logging from `@evodb/observability`:\n\n**RPC Server** (`rpc/src/server.ts`):\n- Line 401: `console.error('Error handling WebSocket message:', error);`\n- Line 441: `console.error('WebSocket error:', error);`\n- Line 757: `console.error('R2 flush failed, using fallback:', error);`\n- Line 879: `console.error('Fallback recovery failed:', error);`\n\n**Writer** (`writer/src/sharded-parent-do.ts`):\n- Line 245: `console.error('Error handling WebSocket message:', error);`\n- Line 520: `console.error('WebSocket error for ${routedWs.sourceDoId}:', error);`\n\n**Snippets Chain** (`snippets-chain/src/executor.ts`):\n- Line 596: `console.log(...)` in debug logger\n\n## Recommendation\n1. Use the structured logger from `@evodb/observability` for all error logging\n2. Consider using the tracing module for debug/info logs\n3. Remove or conditionally compile console statements\n\n## Benefits\n- Consistent log format across all packages\n- Better log aggregation and querying\n- Can be disabled in production for performance\n- Structured data for debugging\n\n## Files to modify\n- `rpc/src/server.ts`\n- `writer/src/sharded-parent-do.ts`\n- `snippets-chain/src/executor.ts`","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:37.816715-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:32:05.275713-06:00","closed_at":"2026-01-21T20:32:05.275713-06:00","close_reason":"Closed","labels":["code-quality","logging"]}
{"id":"evodb-ax7","title":"TDD: Document type assertions in performance paths","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:54.397636-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.681752-06:00","closed_at":"2026-01-20T17:49:16.681752-06:00","close_reason":"Closed","external_ref":"gh-114"}
{"id":"evodb-azl","title":"TDD: Add async/await test patterns","description":"0 async tests in main suite. Add async tests for R2 operations, query execution, CDC pipeline.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:31.046599-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:38:16.419064-06:00","closed_at":"2026-01-20T13:38:16.419064-06:00","close_reason":"Closed"}
{"id":"evodb-b4y","title":"Consolidate duplicate getNestedValue and compareValues functions across packages","description":"## Problem\n\nThere are duplicate implementations of `getNestedValue`, `compareValues`, and `simpleHash` functions across multiple packages:\n\n**getNestedValue**:\n- `core/src/query-ops.ts:234`\n- `lakehouse/src/partition.ts:155`\n- `query/src/engine.ts` (imports from core)\n\n**compareValues**:\n- `core/src/query-ops.ts:325`\n- `lakehouse/src/partition.ts:464`\n- `query/src/engine.ts:616`\n\n**simpleHash**:\n- `query/src/engine.ts:603` (\\_simpleHash)\n- `lakehouse/src/partition.ts:259`\n\n## Recommendation\nConsolidate all implementations into `@evodb/core` and re-export from there. The lakehouse package should import from core rather than having its own implementation.\n\n## Files to modify\n- `lakehouse/src/partition.ts` - Remove duplicate implementations, import from core\n- `core/src/query-ops.ts` - Ensure exports are available\n- `core/src/index.ts` - Export utility functions\n\n## Acceptance Criteria\n- Single source of truth for utility functions in @evodb/core\n- All packages import from core\n- No functionality regression\n- Tests pass in all packages","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:52.187988-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:45:28.538343-06:00","closed_at":"2026-01-21T19:45:28.538343-06:00","close_reason":"Closed","labels":["DRY","code-quality","tech-debt"]}
{"id":"evodb-bjfg","title":"TDD: Implement offline-first local-first sync","description":"## Overview\nImplement offline-first synchronization with local persistence, conflict resolution, and eventual consistency for distributed data access.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. Offline Queue Tests\n```typescript\ndescribe('Offline Queue', () =\u003e {\n  it('should queue mutations when offline', async () =\u003e {\n    await db.goOffline();\n    \n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Buy milk']);\n    await db.execute('UPDATE todos SET title = ? WHERE id = 1', ['Buy eggs']);\n    \n    const queue = await db.getOfflineQueue();\n    expect(queue).toHaveLength(2);\n    expect(queue[0]).toMatchObject({\n      type: 'INSERT',\n      table: 'todos',\n      data: { id: 1, title: 'Buy milk' },\n    });\n  });\n\n  it('should persist queue across restarts', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Task 1']);\n    \n    // Simulate restart\n    await db.close();\n    const db2 = await openDatabase(dbPath);\n    \n    const queue = await db2.getOfflineQueue();\n    expect(queue).toHaveLength(1);\n  });\n\n  it('should apply mutations locally while offline', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Task 1']);\n    \n    const todos = await db.query('SELECT * FROM todos');\n    expect(todos).toHaveLength(1);\n    expect(todos[0].title).toBe('Task 1');\n  });\n\n  it('should track mutation timestamps', async () =\u003e {\n    await db.goOffline();\n    const before = Date.now();\n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Task 1']);\n    const after = Date.now();\n    \n    const queue = await db.getOfflineQueue();\n    expect(queue[0].timestamp).toBeGreaterThanOrEqual(before);\n    expect(queue[0].timestamp).toBeLessThanOrEqual(after);\n  });\n\n  it('should assign unique mutation IDs', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Task 1']);\n    await db.execute('INSERT INTO todos (id, title) VALUES (2, ?)', ['Task 2']);\n    \n    const queue = await db.getOfflineQueue();\n    expect(queue[0].mutationId).not.toBe(queue[1].mutationId);\n  });\n\n  it('should coalesce redundant mutations', async () =\u003e {\n    await db.goOffline();\n    await db.execute('UPDATE todos SET title = ? WHERE id = 1', ['V1']);\n    await db.execute('UPDATE todos SET title = ? WHERE id = 1', ['V2']);\n    await db.execute('UPDATE todos SET title = ? WHERE id = 1', ['V3']);\n    \n    const queue = await db.getOfflineQueue();\n    // Should coalesce to single update with final value\n    expect(queue).toHaveLength(1);\n    expect(queue[0].data.title).toBe('V3');\n  });\n\n  it('should handle delete after insert as no-op', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos (id, title) VALUES (1, ?)', ['Task 1']);\n    await db.execute('DELETE FROM todos WHERE id = 1');\n    \n    const queue = await db.getOfflineQueue();\n    expect(queue).toHaveLength(0); // Both cancelled out\n  });\n});\n```\n\n#### 2. Conflict Resolution Tests\n```typescript\ndescribe('Conflict Resolution', () =\u003e {\n  describe('Last-Write-Wins', () =\u003e {\n    it('should use server version when server is newer', async () =\u003e {\n      // Local: updated at t=100\n      // Server: updated at t=200\n      const conflict = createConflict({\n        local: { title: 'Local', _updated_at: 100 },\n        server: { title: 'Server', _updated_at: 200 },\n      });\n      \n      const resolved = await db.resolveConflict(conflict, 'last-write-wins');\n      expect(resolved.title).toBe('Server');\n    });\n\n    it('should use local version when local is newer', async () =\u003e {\n      const conflict = createConflict({\n        local: { title: 'Local', _updated_at: 200 },\n        server: { title: 'Server', _updated_at: 100 },\n      });\n      \n      const resolved = await db.resolveConflict(conflict, 'last-write-wins');\n      expect(resolved.title).toBe('Local');\n    });\n  });\n\n  describe('Field-Level Merge', () =\u003e {\n    it('should merge non-conflicting field changes', async () =\u003e {\n      const conflict = createConflict({\n        base: { title: 'Original', description: 'Desc', priority: 1 },\n        local: { title: 'Local Title', description: 'Desc', priority: 1 },\n        server: { title: 'Original', description: 'Server Desc', priority: 2 },\n      });\n      \n      const resolved = await db.resolveConflict(conflict, 'field-merge');\n      expect(resolved).toEqual({\n        title: 'Local Title',\n        description: 'Server Desc',\n        priority: 2,\n      });\n    });\n\n    it('should use LWW for conflicting fields', async () =\u003e {\n      const conflict = createConflict({\n        base: { title: 'Original' },\n        local: { title: 'Local', _field_updated: { title: 100 } },\n        server: { title: 'Server', _field_updated: { title: 200 } },\n      });\n      \n      const resolved = await db.resolveConflict(conflict, 'field-merge');\n      expect(resolved.title).toBe('Server'); // Server field is newer\n    });\n  });\n\n  describe('Custom Resolution', () =\u003e {\n    it('should call custom resolver function', async () =\u003e {\n      const customResolver = vi.fn().mockReturnValue({ title: 'Custom' });\n      \n      const conflict = createConflict({\n        local: { title: 'Local' },\n        server: { title: 'Server' },\n      });\n      \n      await db.resolveConflict(conflict, customResolver);\n      \n      expect(customResolver).toHaveBeenCalledWith({\n        local: { title: 'Local' },\n        server: { title: 'Server' },\n        base: undefined,\n      });\n    });\n\n    it('should allow keeping both versions', async () =\u003e {\n      const resolver = () =\u003e ({ action: 'keep-both' });\n      \n      const conflict = createConflict({\n        local: { id: 'local-1', title: 'Local' },\n        server: { id: 'server-1', title: 'Server' },\n      });\n      \n      const result = await db.resolveConflict(conflict, resolver);\n      expect(result.action).toBe('keep-both');\n    });\n  });\n\n  describe('Delete Conflicts', () =\u003e {\n    it('should handle local delete vs server update', async () =\u003e {\n      const conflict = createConflict({\n        local: null, // Deleted\n        server: { id: 1, title: 'Updated' },\n        type: 'delete-update',\n      });\n      \n      // Default: server wins (resurrect)\n      const resolved = await db.resolveConflict(conflict, 'server-wins');\n      expect(resolved).toEqual({ id: 1, title: 'Updated' });\n    });\n\n    it('should handle local update vs server delete', async () =\u003e {\n      const conflict = createConflict({\n        local: { id: 1, title: 'Updated' },\n        server: null, // Deleted\n        type: 'update-delete',\n      });\n      \n      // Can configure to keep local\n      const resolved = await db.resolveConflict(conflict, 'local-wins');\n      expect(resolved).toEqual({ id: 1, title: 'Updated' });\n    });\n  });\n});\n```\n\n#### 3. Sync Protocol Tests\n```typescript\ndescribe('Sync Protocol', () =\u003e {\n  it('should sync when coming online', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos VALUES (1, ?)', ['Task 1']);\n    \n    const syncResult = await db.goOnline();\n    \n    expect(syncResult.pushed).toBe(1);\n    expect(syncResult.conflicts).toBe(0);\n    expect(await db.getOfflineQueue()).toHaveLength(0);\n  });\n\n  it('should pull remote changes during sync', async () =\u003e {\n    // Remote has new data\n    await remoteDb.execute('INSERT INTO todos VALUES (2, ?)', ['Remote Task']);\n    \n    await db.sync();\n    \n    const todos = await db.query('SELECT * FROM todos');\n    expect(todos).toContainEqual({ id: 2, title: 'Remote Task' });\n  });\n\n  it('should use vector clocks for ordering', async () =\u003e {\n    // Client A and B both modify same row\n    const clockA = { A: 1, B: 0 };\n    const clockB = { A: 0, B: 1 };\n    \n    // Neither dominates - concurrent modification\n    expect(isConcurrent(clockA, clockB)).toBe(true);\n  });\n\n  it('should track sync state per table', async () =\u003e {\n    await db.sync({ tables: ['todos'] });\n    \n    const syncState = await db.getSyncState('todos');\n    expect(syncState.lastSyncedAt).toBeDefined();\n    expect(syncState.serverVersion).toBeDefined();\n  });\n\n  it('should support incremental sync', async () =\u003e {\n    // First sync - full\n    await db.sync();\n    const state1 = await db.getSyncState('todos');\n    \n    // Add more data remotely\n    await remoteDb.execute('INSERT INTO todos VALUES (3, ?)', ['New Task']);\n    \n    // Second sync - incremental (only changes since lastSyncedAt)\n    const result = await db.sync();\n    expect(result.pulled).toBe(1); // Only the new row\n  });\n\n  it('should handle sync interruption gracefully', async () =\u003e {\n    await db.goOffline();\n    await db.execute('INSERT INTO todos VALUES (1, ?)', ['Task 1']);\n    await db.execute('INSERT INTO todos VALUES (2, ?)', ['Task 2']);\n    \n    // Simulate interrupted sync (fails after first mutation)\n    mockSyncError(1);\n    \n    await expect(db.goOnline()).rejects.toThrow();\n    \n    // Queue should still have unpushed mutation\n    const queue = await db.getOfflineQueue();\n    expect(queue).toHaveLength(1); // One was pushed before failure\n  });\n\n  it('should batch mutations for efficient sync', async () =\u003e {\n    await db.goOffline();\n    for (let i = 0; i \u003c 100; i++) {\n      await db.execute('INSERT INTO todos VALUES (?, ?)', [i, `Task ${i}`]);\n    }\n    \n    const spy = vi.spyOn(syncClient, 'pushMutations');\n    await db.goOnline();\n    \n    // Should batch rather than 100 individual calls\n    expect(spy).toHaveBeenCalledTimes(1);\n    expect(spy.mock.calls[0][0]).toHaveLength(100);\n  });\n\n  it('should emit sync events', async () =\u003e {\n    const events: SyncEvent[] = [];\n    db.on('sync', (event) =\u003e events.push(event));\n    \n    await db.goOffline();\n    await db.execute('INSERT INTO todos VALUES (1, ?)', ['Task']);\n    await db.goOnline();\n    \n    expect(events).toContainEqual({ type: 'sync:start' });\n    expect(events).toContainEqual({ type: 'sync:push', count: 1 });\n    expect(events).toContainEqual({ type: 'sync:complete', pushed: 1, pulled: 0 });\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/core/src/sync/offline-queue.ts\nexport class OfflineQueue {\n  private queue: QueuedMutation[] = [];\n  private storage: PersistentStorage;\n\n  constructor(storage: PersistentStorage) {\n    this.storage = storage;\n  }\n\n  async enqueue(mutation: Mutation): Promise\u003cvoid\u003e {\n    const queued: QueuedMutation = {\n      mutationId: generateId(),\n      timestamp: Date.now(),\n      ...mutation,\n    };\n    \n    // Try to coalesce with existing mutations\n    const coalesced = this.tryCoalesce(queued);\n    if (!coalesced) {\n      this.queue.push(queued);\n    }\n    \n    await this.persist();\n  }\n\n  private tryCoalesce(mutation: QueuedMutation): boolean {\n    // Find existing mutation for same row\n    const existing = this.queue.findIndex(\n      m =\u003e m.table === mutation.table \u0026\u0026 m.rowId === mutation.rowId\n    );\n    \n    if (existing === -1) return false;\n    \n    const prev = this.queue[existing];\n    \n    // INSERT + DELETE = remove both\n    if (prev.type === 'INSERT' \u0026\u0026 mutation.type === 'DELETE') {\n      this.queue.splice(existing, 1);\n      return true;\n    }\n    \n    // UPDATE + UPDATE = merge\n    if (prev.type === 'UPDATE' \u0026\u0026 mutation.type === 'UPDATE') {\n      this.queue[existing] = {\n        ...prev,\n        data: { ...prev.data, ...mutation.data },\n        timestamp: mutation.timestamp,\n      };\n      return true;\n    }\n    \n    return false;\n  }\n\n  async dequeue(count: number): Promise\u003cQueuedMutation[]\u003e {\n    const mutations = this.queue.splice(0, count);\n    await this.persist();\n    return mutations;\n  }\n\n  async restore(): Promise\u003cvoid\u003e {\n    const data = await this.storage.get('offline_queue');\n    this.queue = data ? JSON.parse(data) : [];\n  }\n\n  private async persist(): Promise\u003cvoid\u003e {\n    await this.storage.set('offline_queue', JSON.stringify(this.queue));\n  }\n}\n\n// packages/core/src/sync/conflict-resolver.ts\nexport class ConflictResolver {\n  resolve(conflict: Conflict, strategy: ConflictStrategy): ResolvedValue {\n    if (typeof strategy === 'function') {\n      return strategy(conflict);\n    }\n    \n    switch (strategy) {\n      case 'last-write-wins':\n        return this.lastWriteWins(conflict);\n      case 'field-merge':\n        return this.fieldMerge(conflict);\n      case 'server-wins':\n        return conflict.server ?? conflict.local;\n      case 'local-wins':\n        return conflict.local ?? conflict.server;\n      default:\n        throw new Error(`Unknown strategy: ${strategy}`);\n    }\n  }\n\n  private lastWriteWins(conflict: Conflict): unknown {\n    const localTime = conflict.local?._updated_at ?? 0;\n    const serverTime = conflict.server?._updated_at ?? 0;\n    return localTime \u003e serverTime ? conflict.local : conflict.server;\n  }\n\n  private fieldMerge(conflict: Conflict): Record\u003cstring, unknown\u003e {\n    const { base, local, server } = conflict;\n    const result: Record\u003cstring, unknown\u003e = { ...base };\n    \n    const allKeys = new Set([\n      ...Object.keys(local ?? {}),\n      ...Object.keys(server ?? {}),\n    ]);\n    \n    for (const key of allKeys) {\n      if (key.startsWith('_')) continue; // Skip metadata\n      \n      const baseVal = base?.[key];\n      const localVal = local?.[key];\n      const serverVal = server?.[key];\n      \n      // No change\n      if (localVal === baseVal \u0026\u0026 serverVal === baseVal) {\n        result[key] = baseVal;\n      }\n      // Only local changed\n      else if (localVal !== baseVal \u0026\u0026 serverVal === baseVal) {\n        result[key] = localVal;\n      }\n      // Only server changed\n      else if (serverVal !== baseVal \u0026\u0026 localVal === baseVal) {\n        result[key] = serverVal;\n      }\n      // Both changed - use field timestamps\n      else {\n        const localFieldTime = local?._field_updated?.[key] ?? 0;\n        const serverFieldTime = server?._field_updated?.[key] ?? 0;\n        result[key] = localFieldTime \u003e serverFieldTime ? localVal : serverVal;\n      }\n    }\n    \n    return result;\n  }\n}\n\n// packages/core/src/sync/sync-engine.ts\nexport class SyncEngine {\n  private queue: OfflineQueue;\n  private resolver: ConflictResolver;\n  private isOnline: boolean = true;\n  private emitter: EventEmitter;\n\n  async sync(options: SyncOptions = {}): Promise\u003cSyncResult\u003e {\n    this.emitter.emit('sync', { type: 'sync:start' });\n    \n    const tables = options.tables ?? await this.getTrackedTables();\n    let totalPushed = 0;\n    let totalPulled = 0;\n    let totalConflicts = 0;\n    \n    // Push local changes\n    const mutations = await this.queue.getAll();\n    if (mutations.length \u003e 0) {\n      const pushResult = await this.pushMutations(mutations);\n      totalPushed = pushResult.pushed;\n      totalConflicts = pushResult.conflicts;\n      this.emitter.emit('sync', { type: 'sync:push', count: totalPushed });\n    }\n    \n    // Pull remote changes\n    for (const table of tables) {\n      const syncState = await this.getSyncState(table);\n      const changes = await this.client.pullChanges(table, syncState.serverVersion);\n      \n      for (const change of changes) {\n        await this.applyRemoteChange(table, change);\n        totalPulled++;\n      }\n      \n      await this.updateSyncState(table, changes.version);\n    }\n    \n    this.emitter.emit('sync', { \n      type: 'sync:complete', \n      pushed: totalPushed, \n      pulled: totalPulled \n    });\n    \n    return { pushed: totalPushed, pulled: totalPulled, conflicts: totalConflicts };\n  }\n\n  private async pushMutations(mutations: QueuedMutation[]): Promise\u003cPushResult\u003e {\n    const response = await this.client.pushMutations(mutations);\n    \n    let pushed = 0;\n    let conflicts = 0;\n    \n    for (const result of response.results) {\n      if (result.status === 'ok') {\n        await this.queue.remove(result.mutationId);\n        pushed++;\n      } else if (result.status === 'conflict') {\n        const resolved = this.resolver.resolve(result.conflict, this.options.conflictStrategy);\n        await this.applyResolution(result.conflict.table, result.conflict.rowId, resolved);\n        conflicts++;\n      }\n    }\n    \n    return { pushed, conflicts };\n  }\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. CRDT Support\n```typescript\n// packages/core/src/sync/crdt/lww-register.ts\nexport class LWWRegister\u003cT\u003e {\n  constructor(\n    private value: T,\n    private timestamp: number,\n    private nodeId: string\n  ) {}\n\n  set(value: T, timestamp: number, nodeId: string): LWWRegister\u003cT\u003e {\n    if (timestamp \u003e this.timestamp || \n        (timestamp === this.timestamp \u0026\u0026 nodeId \u003e this.nodeId)) {\n      return new LWWRegister(value, timestamp, nodeId);\n    }\n    return this;\n  }\n\n  get(): T {\n    return this.value;\n  }\n\n  merge(other: LWWRegister\u003cT\u003e): LWWRegister\u003cT\u003e {\n    return this.set(other.value, other.timestamp, other.nodeId);\n  }\n}\n\n// packages/core/src/sync/crdt/lww-map.ts\nexport class LWWMap\u003cV\u003e {\n  private registers = new Map\u003cstring, LWWRegister\u003cV | null\u003e\u003e();\n\n  set(key: string, value: V, timestamp: number, nodeId: string): void {\n    const existing = this.registers.get(key);\n    if (existing) {\n      this.registers.set(key, existing.set(value, timestamp, nodeId));\n    } else {\n      this.registers.set(key, new LWWRegister(value, timestamp, nodeId));\n    }\n  }\n\n  delete(key: string, timestamp: number, nodeId: string): void {\n    this.set(key, null as any, timestamp, nodeId);\n  }\n\n  get(key: string): V | undefined {\n    const register = this.registers.get(key);\n    const value = register?.get();\n    return value === null ? undefined : value;\n  }\n\n  merge(other: LWWMap\u003cV\u003e): LWWMap\u003cV\u003e {\n    const result = new LWWMap\u003cV\u003e();\n    \n    for (const [key, register] of this.registers) {\n      result.registers.set(key, register);\n    }\n    \n    for (const [key, register] of other.registers) {\n      const existing = result.registers.get(key);\n      result.registers.set(\n        key, \n        existing ? existing.merge(register) : register\n      );\n    }\n    \n    return result;\n  }\n}\n\n// packages/core/src/sync/crdt/counter.ts\nexport class GCounter {\n  private counts = new Map\u003cstring, number\u003e();\n\n  increment(nodeId: string, amount: number = 1): void {\n    const current = this.counts.get(nodeId) ?? 0;\n    this.counts.set(nodeId, current + amount);\n  }\n\n  value(): number {\n    let sum = 0;\n    for (const count of this.counts.values()) {\n      sum += count;\n    }\n    return sum;\n  }\n\n  merge(other: GCounter): GCounter {\n    const result = new GCounter();\n    \n    for (const [nodeId, count] of this.counts) {\n      result.counts.set(nodeId, count);\n    }\n    \n    for (const [nodeId, count] of other.counts) {\n      const existing = result.counts.get(nodeId) ?? 0;\n      result.counts.set(nodeId, Math.max(existing, count));\n    }\n    \n    return result;\n  }\n}\n```\n\n#### 2. Selective Sync\n```typescript\n// packages/core/src/sync/selective-sync.ts\nexport class SelectiveSync {\n  private syncConfig: SyncConfig = {\n    tables: new Map(),\n    defaultMode: 'full',\n  };\n\n  configure(table: string, config: TableSyncConfig): void {\n    this.syncConfig.tables.set(table, config);\n  }\n\n  // Only sync rows matching filter\n  setFilter(table: string, filter: SyncFilter): void {\n    const config = this.getConfig(table);\n    config.filter = filter;\n    this.syncConfig.tables.set(table, config);\n  }\n\n  // Sync specific columns only\n  setColumns(table: string, columns: string[]): void {\n    const config = this.getConfig(table);\n    config.columns = columns;\n    this.syncConfig.tables.set(table, config);\n  }\n\n  // Priority-based sync (high priority first)\n  setPriority(table: string, priority: number): void {\n    const config = this.getConfig(table);\n    config.priority = priority;\n    this.syncConfig.tables.set(table, config);\n  }\n\n  // Lazy sync (only when accessed)\n  setLazy(table: string, lazy: boolean): void {\n    const config = this.getConfig(table);\n    config.lazy = lazy;\n    this.syncConfig.tables.set(table, config);\n  }\n\n  async sync(): Promise\u003cSyncResult\u003e {\n    // Sort tables by priority\n    const tables = [...this.syncConfig.tables.entries()]\n      .filter(([_, config]) =\u003e !config.lazy)\n      .sort((a, b) =\u003e (b[1].priority ?? 0) - (a[1].priority ?? 0));\n    \n    const results: SyncResult[] = [];\n    \n    for (const [table, config] of tables) {\n      const result = await this.syncTable(table, config);\n      results.push(result);\n    }\n    \n    return this.mergeResults(results);\n  }\n\n  private async syncTable(table: string, config: TableSyncConfig): Promise\u003cSyncResult\u003e {\n    const options: PullOptions = {};\n    \n    if (config.filter) {\n      options.where = this.buildWhereClause(config.filter);\n    }\n    \n    if (config.columns) {\n      options.select = config.columns;\n    }\n    \n    return this.engine.syncTable(table, options);\n  }\n}\n\n// Usage\nsync.configure('users', { priority: 10, columns: ['id', 'name', 'avatar'] });\nsync.configure('messages', { filter: { user_id: currentUserId }, lazy: true });\nsync.configure('analytics', { mode: 'none' }); // Don't sync locally\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] Offline mutation queue with persistence\n- [ ] Multiple conflict resolution strategies\n- [ ] Sync protocol with incremental updates\n- [ ] CRDT support (LWW-Register, LWW-Map, G-Counter)\n- [ ] Selective sync with filters and priorities\n- [ ] Sync event emission for UI updates\n- [ ] Graceful interruption handling\n- [ ] Mutation batching for efficiency\n\n## Labels\nsync, offline-first, local-first, crdt, conflict-resolution","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:39:07.752292-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:44:26.007761-06:00","closed_at":"2026-01-21T20:44:26.007761-06:00","close_reason":"Closed"}
{"id":"evodb-bka","title":"P2 Code Quality: Consistency \u0026 Testing","description":"Epic for P2 code quality improvements: workspace dependencies, test utilities, E2E integration, and documentation alignment.","status":"closed","priority":2,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:27:45.292472-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:05:24.24988-06:00","closed_at":"2026-01-20T11:05:24.24988-06:00","close_reason":"Closed"}
{"id":"evodb-bka.1","title":"Standardize workspace dependencies to workspace:*","description":"Query and benchmark packages use file:../core instead of workspace:*.\n\nThis causes version resolution inconsistencies and breaks pnpm workspace linking.\n\nUpdate all internal dependencies to use workspace:* syntax.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:52.350283-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:35:25.702079-06:00","closed_at":"2026-01-20T10:35:25.702079-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.1","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:29:52.351072-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.2","title":"Extract mock data to test utilities package","description":"Query engine has ~200 lines of mock data generation in production code (initializeMockData).\n\nExtract to a separate @evodb/test-utils package or test fixtures directory.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:58.909741-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:47:09.386895-06:00","closed_at":"2026-01-20T10:47:09.386895-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.2","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:29:58.910607-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.3","title":"Add E2E integration test with real R2","description":"No complete 'write data -\u003e query data' example that actually works.\n\nCreate E2E test that:\n1. Writes CDC entries via writer\n2. Flushes to R2 (or miniflare mock)\n3. Queries back via reader\n4. Validates round-trip","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:04.930966-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:40:30.070145-06:00","closed_at":"2026-01-20T10:40:30.070145-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.3","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:04.931612-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.4","title":"Make snippets-lance import from lance-reader","description":"snippets-lance duplicates vector math from lance-reader:\n- normalizeVector, computeL2Distance duplicated\n- CachedLanceReader doesn't use lance-reader\n\nDRY violation - add proper dependency and imports.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:11.14332-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:57:56.26371-06:00","closed_at":"2026-01-20T10:57:56.26371-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.4","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:11.143943-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.5","title":"Replace magic numbers with named constants","description":"Multiple magic numbers in snippets-lance for binary format sizes.\n\nExtract to named constants for readability and maintainability:\n- Header sizes\n- Offset calculations\n- Buffer sizes","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:15.175353-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:44:56.629044-06:00","closed_at":"2026-01-20T10:44:56.629044-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.5","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:15.176005-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.6","title":"Add exhaustiveness check to codegen switch statements","description":"Switch statements in codegen (diff.ts, generator.ts) missing exhaustiveness checks.\n\nAdd assertNever() in default cases to catch unhandled enum values at compile time.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:19.356286-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:06:37.647031-06:00","closed_at":"2026-01-20T11:06:37.647031-06:00","close_reason":"Closed","external_ref":"gh-7","dependencies":[{"issue_id":"evodb-bka.6","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:19.356934-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.7","title":"Add input validation to codegen CLI JSON parsing","description":"CLI in codegen/src/cli.ts parses JSON from files without validation.\n\nMalformed JSON could cause confusing errors. Add proper validation with helpful error messages.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:22.771411-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:41:09.358434-06:00","closed_at":"2026-01-20T10:41:09.358434-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.7","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:22.772008-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.8","title":"Implement actual bloom filter instead of Map simulation","description":"BloomFilterManager in query engine uses Map\u003cstring, Set\u003cstring\u003e\u003e instead of actual bloom filter bit arrays.\n\nImplement proper bloom filter for space efficiency and probabilistic behavior.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:25.934737-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:47:08.208649-06:00","closed_at":"2026-01-20T10:47:08.208649-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.8","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:25.93552-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bka.9","title":"Define QueryExecutor interface for reader/query unification","description":"reader and query packages both implement query execution but don't share an interface.\n\nDefine QueryExecutor interface in core that both implement for consistency.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:29.744773-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:43:06.407675-06:00","closed_at":"2026-01-20T10:43:06.407675-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-bka.9","depends_on_id":"evodb-bka","type":"parent-child","created_at":"2026-01-20T10:30:29.745313-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-bkw","title":"TDD: Reduce non-null assertions in production","description":"119 occurrences of ! in 24 files. Replace with explicit null checks in writer/sharded-parent-do.ts and reader/index.ts.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:41.837366-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.372276-06:00","closed_at":"2026-01-20T16:23:39.372276-06:00","close_reason":"Closed","external_ref":"gh-43"}
{"id":"evodb-bm6","title":"TDD: Add cross-package integration tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:24.063159-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:36:42.795844-06:00","closed_at":"2026-01-20T17:36:42.795844-06:00","close_reason":"Closed"}
{"id":"evodb-bnv8","title":"TDD: Add circuit breaker to RPC client","description":"## Overview\n\nImplement a circuit breaker pattern for the RPC client to prevent cascading failures and provide graceful degradation when downstream services are unhealthy.\n\n## TDD Approach\n\n### RED Phase - Write Failing Tests\n\n```typescript\n// tests/core/resilience/circuit-breaker.test.ts\n\ndescribe('Circuit Breaker', () =\u003e {\n  describe('State Transitions', () =\u003e {\n    describe('Closed State (Normal Operation)', () =\u003e {\n      it('should start in closed state', () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 5 });\n        expect(breaker.state).toBe('closed');\n      });\n\n      it('should allow calls when closed', async () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 5 });\n        const operation = vi.fn().mockResolvedValue('success');\n        \n        const result = await breaker.execute(operation);\n        \n        expect(result).toBe('success');\n        expect(operation).toHaveBeenCalled();\n      });\n\n      it('should track failures but stay closed under threshold', async () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 5 });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        \n        for (let i = 0; i \u003c 4; i++) {\n          await breaker.execute(failingOp).catch(() =\u003e {});\n        }\n        \n        expect(breaker.state).toBe('closed');\n        expect(breaker.failureCount).toBe(4);\n      });\n\n      it('should reset failure count on success', async () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 5 });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        const successOp = vi.fn().mockResolvedValue('ok');\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        await breaker.execute(failingOp).catch(() =\u003e {});\n        await breaker.execute(successOp);\n        \n        expect(breaker.failureCount).toBe(0);\n      });\n    });\n\n    describe('Open State (Failing Fast)', () =\u003e {\n      it('should open after failure threshold reached', async () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 3 });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        \n        for (let i = 0; i \u003c 3; i++) {\n          await breaker.execute(failingOp).catch(() =\u003e {});\n        }\n        \n        expect(breaker.state).toBe('open');\n      });\n\n      it('should reject calls immediately when open', async () =\u003e {\n        const breaker = new CircuitBreaker({ failureThreshold: 1 });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        const operation = vi.fn().mockResolvedValue('success');\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        expect(breaker.state).toBe('open');\n        \n        await expect(breaker.execute(operation))\n          .rejects.toThrow(CircuitBreakerOpenError);\n        \n        expect(operation).not.toHaveBeenCalled();\n      });\n\n      it('should include circuit info in rejection error', async () =\u003e {\n        const breaker = new CircuitBreaker({ \n          failureThreshold: 1,\n          name: 'test-breaker'\n        });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        \n        try {\n          await breaker.execute(vi.fn());\n        } catch (error) {\n          expect(error).toBeInstanceOf(CircuitBreakerOpenError);\n          expect((error as CircuitBreakerOpenError).circuitName).toBe('test-breaker');\n          expect((error as CircuitBreakerOpenError).openedAt).toBeInstanceOf(Date);\n        }\n      });\n    });\n\n    describe('Half-Open State (Testing Recovery)', () =\u003e {\n      beforeEach(() =\u003e {\n        vi.useFakeTimers();\n      });\n\n      afterEach(() =\u003e {\n        vi.useRealTimers();\n      });\n\n      it('should transition to half-open after reset timeout', async () =\u003e {\n        const breaker = new CircuitBreaker({\n          failureThreshold: 1,\n          resetTimeoutMs: 5000,\n        });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        expect(breaker.state).toBe('open');\n        \n        await vi.advanceTimersByTimeAsync(5000);\n        \n        expect(breaker.state).toBe('half-open');\n      });\n\n      it('should allow limited calls in half-open state', async () =\u003e {\n        const breaker = new CircuitBreaker({\n          failureThreshold: 1,\n          resetTimeoutMs: 1000,\n          halfOpenMaxCalls: 2,\n        });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        const operation = vi.fn().mockResolvedValue('ok');\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        await vi.advanceTimersByTimeAsync(1000);\n        \n        // First two calls allowed\n        await breaker.execute(operation);\n        await breaker.execute(operation);\n        \n        // Third call should be rejected\n        await expect(breaker.execute(operation))\n          .rejects.toThrow(/half-open limit/i);\n      });\n\n      it('should close on success in half-open state', async () =\u003e {\n        const breaker = new CircuitBreaker({\n          failureThreshold: 1,\n          resetTimeoutMs: 1000,\n          successThreshold: 2,\n        });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        const successOp = vi.fn().mockResolvedValue('ok');\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        await vi.advanceTimersByTimeAsync(1000);\n        \n        await breaker.execute(successOp);\n        await breaker.execute(successOp);\n        \n        expect(breaker.state).toBe('closed');\n      });\n\n      it('should re-open on failure in half-open state', async () =\u003e {\n        const breaker = new CircuitBreaker({\n          failureThreshold: 1,\n          resetTimeoutMs: 1000,\n        });\n        const failingOp = vi.fn().mockRejectedValue(new Error('fail'));\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        await vi.advanceTimersByTimeAsync(1000);\n        expect(breaker.state).toBe('half-open');\n        \n        await breaker.execute(failingOp).catch(() =\u003e {});\n        \n        expect(breaker.state).toBe('open');\n      });\n    });\n  });\n\n  describe('Failure Counting Strategies', () =\u003e {\n    it('should support consecutive failure counting', async () =\u003e {\n      const breaker = new CircuitBreaker({\n        failureThreshold: 3,\n        countingStrategy: 'consecutive',\n      });\n      \n      const fail = vi.fn().mockRejectedValue(new Error('fail'));\n      const success = vi.fn().mockResolvedValue('ok');\n      \n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(success); // Resets consecutive count\n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(fail).catch(() =\u003e {});\n      \n      expect(breaker.state).toBe('closed'); // Only 2 consecutive\n    });\n\n    it('should support sliding window failure rate', async () =\u003e {\n      const breaker = new CircuitBreaker({\n        failureThreshold: 0.5, // 50% failure rate\n        windowSizeMs: 10000,\n        minimumCalls: 5,\n        countingStrategy: 'rate',\n      });\n      \n      const fail = vi.fn().mockRejectedValue(new Error('fail'));\n      const success = vi.fn().mockResolvedValue('ok');\n      \n      // 3 successes, 3 failures = 50% rate\n      await breaker.execute(success);\n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(success);\n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(success);\n      await breaker.execute(fail).catch(() =\u003e {}); // Triggers open\n      \n      expect(breaker.state).toBe('open');\n    });\n  });\n\n  describe('RPC Client Integration', () =\u003e {\n    it('should wrap RPC calls with circuit breaker', async () =\u003e {\n      const rpcClient = new RpcClient({\n        circuitBreaker: {\n          enabled: true,\n          failureThreshold: 3,\n          resetTimeoutMs: 5000,\n        },\n      });\n      \n      // Simulate 3 failures\n      vi.spyOn(rpcClient, 'rawCall').mockRejectedValue(new Error('timeout'));\n      \n      for (let i = 0; i \u003c 3; i++) {\n        await rpcClient.call('method', {}).catch(() =\u003e {});\n      }\n      \n      await expect(rpcClient.call('method', {}))\n        .rejects.toThrow(CircuitBreakerOpenError);\n    });\n\n    it('should have separate breakers for different services', async () =\u003e {\n      const rpcClient = new RpcClient({\n        circuitBreaker: {\n          enabled: true,\n          perService: true,\n          failureThreshold: 2,\n        },\n      });\n      \n      // Fail service A\n      vi.spyOn(rpcClient, 'rawCall')\n        .mockImplementation(async (service) =\u003e {\n          if (service === 'serviceA') throw new Error('fail');\n          return 'ok';\n        });\n      \n      await rpcClient.call('serviceA.method', {}).catch(() =\u003e {});\n      await rpcClient.call('serviceA.method', {}).catch(() =\u003e {});\n      \n      // Service A is open\n      await expect(rpcClient.call('serviceA.method', {}))\n        .rejects.toThrow(CircuitBreakerOpenError);\n      \n      // Service B still works\n      await expect(rpcClient.call('serviceB.method', {}))\n        .resolves.toBe('ok');\n    });\n  });\n\n  describe('Events and Metrics', () =\u003e {\n    it('should emit state change events', async () =\u003e {\n      const breaker = new CircuitBreaker({ failureThreshold: 1 });\n      const stateChanges: string[] = [];\n      \n      breaker.on('stateChange', ({ from, to }) =\u003e {\n        stateChanges.push(`${from}-\u003e${to}`);\n      });\n      \n      const fail = vi.fn().mockRejectedValue(new Error('fail'));\n      await breaker.execute(fail).catch(() =\u003e {});\n      \n      expect(stateChanges).toContain('closed-\u003eopen');\n    });\n\n    it('should track metrics', async () =\u003e {\n      const breaker = new CircuitBreaker({ failureThreshold: 5 });\n      \n      const success = vi.fn().mockResolvedValue('ok');\n      const fail = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      await breaker.execute(success);\n      await breaker.execute(fail).catch(() =\u003e {});\n      await breaker.execute(success);\n      \n      const metrics = breaker.getMetrics();\n      \n      expect(metrics.totalCalls).toBe(3);\n      expect(metrics.successCount).toBe(2);\n      expect(metrics.failureCount).toBe(1);\n      expect(metrics.successRate).toBeCloseTo(0.667, 2);\n    });\n  });\n\n  describe('Fallback Support', () =\u003e {\n    it('should call fallback when circuit is open', async () =\u003e {\n      const breaker = new CircuitBreaker({ failureThreshold: 1 });\n      const fail = vi.fn().mockRejectedValue(new Error('fail'));\n      const fallback = vi.fn().mockResolvedValue('fallback-result');\n      \n      await breaker.execute(fail).catch(() =\u003e {});\n      \n      const result = await breaker.execute(fail, { fallback });\n      \n      expect(result).toBe('fallback-result');\n      expect(fallback).toHaveBeenCalled();\n    });\n\n    it('should pass error to fallback function', async () =\u003e {\n      const breaker = new CircuitBreaker({ failureThreshold: 1 });\n      const fail = vi.fn().mockRejectedValue(new Error('original error'));\n      const fallback = vi.fn((error) =\u003e `handled: ${error.message}`);\n      \n      await breaker.execute(fail).catch(() =\u003e {});\n      \n      const result = await breaker.execute(fail, { fallback });\n      \n      expect(result).toBe('handled: Circuit breaker is open');\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Circuit Breaker\n\n```typescript\n// src/core/resilience/circuit-breaker.ts\n\nexport type CircuitState = 'closed' | 'open' | 'half-open';\nexport type CountingStrategy = 'consecutive' | 'total' | 'rate';\n\nexport interface CircuitBreakerConfig {\n  name?: string;\n  failureThreshold: number;\n  successThreshold?: number;\n  resetTimeoutMs?: number;\n  halfOpenMaxCalls?: number;\n  countingStrategy?: CountingStrategy;\n  windowSizeMs?: number;\n  minimumCalls?: number;\n}\n\nexport class CircuitBreakerOpenError extends Error {\n  constructor(\n    message: string,\n    public circuitName: string,\n    public openedAt: Date\n  ) {\n    super(message);\n    this.name = 'CircuitBreakerOpenError';\n  }\n}\n\nexport class CircuitBreaker extends EventEmitter {\n  private _state: CircuitState = 'closed';\n  private _failureCount = 0;\n  private _successCount = 0;\n  private _halfOpenCalls = 0;\n  private _openedAt?: Date;\n  private _resetTimer?: NodeJS.Timeout;\n  private _callHistory: Array\u003c{ timestamp: number; success: boolean }\u003e = [];\n  \n  private readonly config: Required\u003cCircuitBreakerConfig\u003e;\n\n  constructor(config: CircuitBreakerConfig) {\n    super();\n    this.config = {\n      name: config.name ?? 'default',\n      failureThreshold: config.failureThreshold,\n      successThreshold: config.successThreshold ?? 1,\n      resetTimeoutMs: config.resetTimeoutMs ?? 30000,\n      halfOpenMaxCalls: config.halfOpenMaxCalls ?? 1,\n      countingStrategy: config.countingStrategy ?? 'consecutive',\n      windowSizeMs: config.windowSizeMs ?? 60000,\n      minimumCalls: config.minimumCalls ?? 10,\n    };\n  }\n\n  get state(): CircuitState {\n    return this._state;\n  }\n\n  get failureCount(): number {\n    return this._failureCount;\n  }\n\n  async execute\u003cT\u003e(\n    operation: () =\u003e Promise\u003cT\u003e,\n    options?: { fallback?: (error: Error) =\u003e T | Promise\u003cT\u003e }\n  ): Promise\u003cT\u003e {\n    // Check if we should reject immediately\n    if (this._state === 'open') {\n      const error = new CircuitBreakerOpenError(\n        `Circuit breaker '${this.config.name}' is open`,\n        this.config.name,\n        this._openedAt!\n      );\n      \n      if (options?.fallback) {\n        return options.fallback(error);\n      }\n      throw error;\n    }\n\n    // Check half-open limits\n    if (this._state === 'half-open') {\n      if (this._halfOpenCalls \u003e= this.config.halfOpenMaxCalls) {\n        throw new Error('Circuit breaker half-open limit reached');\n      }\n      this._halfOpenCalls++;\n    }\n\n    try {\n      const result = await operation();\n      this.recordSuccess();\n      return result;\n    } catch (error) {\n      this.recordFailure();\n      throw error;\n    }\n  }\n\n  private recordSuccess(): void {\n    this._callHistory.push({ timestamp: Date.now(), success: true });\n    \n    if (this._state === 'half-open') {\n      this._successCount++;\n      if (this._successCount \u003e= this.config.successThreshold) {\n        this.transitionTo('closed');\n      }\n    } else if (this._state === 'closed') {\n      this._failureCount = 0; // Reset on success\n    }\n  }\n\n  private recordFailure(): void {\n    this._callHistory.push({ timestamp: Date.now(), success: false });\n    \n    if (this._state === 'half-open') {\n      this.transitionTo('open');\n      return;\n    }\n\n    if (this._state === 'closed') {\n      this._failureCount++;\n      \n      if (this.shouldOpen()) {\n        this.transitionTo('open');\n      }\n    }\n  }\n\n  private shouldOpen(): boolean {\n    switch (this.config.countingStrategy) {\n      case 'consecutive':\n      case 'total':\n        return this._failureCount \u003e= this.config.failureThreshold;\n        \n      case 'rate':\n        return this.calculateFailureRate() \u003e= this.config.failureThreshold;\n        \n      default:\n        return this._failureCount \u003e= this.config.failureThreshold;\n    }\n  }\n\n  private calculateFailureRate(): number {\n    const now = Date.now();\n    const windowStart = now - this.config.windowSizeMs;\n    \n    const recentCalls = this._callHistory.filter(c =\u003e c.timestamp \u003e windowStart);\n    \n    if (recentCalls.length \u003c this.config.minimumCalls) {\n      return 0;\n    }\n    \n    const failures = recentCalls.filter(c =\u003e !c.success).length;\n    return failures / recentCalls.length;\n  }\n\n  private transitionTo(newState: CircuitState): void {\n    const oldState = this._state;\n    this._state = newState;\n    \n    this.emit('stateChange', { from: oldState, to: newState });\n    \n    switch (newState) {\n      case 'open':\n        this._openedAt = new Date();\n        this._resetTimer = setTimeout(() =\u003e {\n          this.transitionTo('half-open');\n        }, this.config.resetTimeoutMs);\n        break;\n        \n      case 'half-open':\n        this._halfOpenCalls = 0;\n        this._successCount = 0;\n        break;\n        \n      case 'closed':\n        this._failureCount = 0;\n        this._successCount = 0;\n        if (this._resetTimer) {\n          clearTimeout(this._resetTimer);\n        }\n        break;\n    }\n  }\n\n  getMetrics(): {\n    totalCalls: number;\n    successCount: number;\n    failureCount: number;\n    successRate: number;\n    state: CircuitState;\n  } {\n    const total = this._callHistory.length;\n    const successes = this._callHistory.filter(c =\u003e c.success).length;\n    \n    return {\n      totalCalls: total,\n      successCount: successes,\n      failureCount: total - successes,\n      successRate: total \u003e 0 ? successes / total : 1,\n      state: this._state,\n    };\n  }\n}\n```\n\n### REFACTOR Phase - Add Metrics and Configuration\n\n1. **Add metrics export**: Prometheus/OpenTelemetry compatible metrics\n2. **Add configurable thresholds**: Runtime-adjustable settings\n3. **Add health check integration**: Expose circuit health for monitoring\n4. **Add bulkhead pattern**: Limit concurrent calls\n\n```typescript\n// Enhanced circuit breaker with metrics and configuration\n\nexport class CircuitBreakerRegistry {\n  private breakers = new Map\u003cstring, CircuitBreaker\u003e();\n  \n  get(name: string, config?: CircuitBreakerConfig): CircuitBreaker {\n    let breaker = this.breakers.get(name);\n    if (!breaker \u0026\u0026 config) {\n      breaker = new CircuitBreaker({ ...config, name });\n      this.breakers.set(name, breaker);\n    }\n    return breaker!;\n  }\n  \n  getAllMetrics(): Record\u003cstring, ReturnType\u003cCircuitBreaker['getMetrics']\u003e\u003e {\n    const metrics: Record\u003cstring, any\u003e = {};\n    for (const [name, breaker] of this.breakers) {\n      metrics[name] = breaker.getMetrics();\n    }\n    return metrics;\n  }\n}\n\n// Global registry\nexport const circuitBreakers = new CircuitBreakerRegistry();\n```\n\n## Acceptance Criteria\n\n- [ ] Circuit starts in closed state\n- [ ] Opens after failure threshold reached\n- [ ] Rejects immediately when open\n- [ ] Transitions to half-open after timeout\n- [ ] Closes after success in half-open state\n- [ ] Re-opens on failure in half-open state\n- [ ] Supports consecutive and rate-based counting\n- [ ] RPC client integrates circuit breaker\n- [ ] Per-service circuit breakers work\n- [ ] State change events emit correctly\n- [ ] Metrics are tracked accurately\n- [ ] Fallback support works\n- [ ] Configurable thresholds work","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:37:54.704488-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:11:01.646083-06:00","closed_at":"2026-01-21T20:11:01.646083-06:00","close_reason":"Closed"}
{"id":"evodb-bpe","title":"TDD: Enable vitest coverage collection","description":"All coverage reports show 0%. Add coverage config to vitest.config.ts with provider v8, thresholds 80%.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:29.392485-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.429131-06:00","closed_at":"2026-01-20T16:23:39.429131-06:00","close_reason":"Closed","external_ref":"gh-44"}
{"id":"evodb-bqv","title":"TDD: Add cache-aware query planning","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T14:00:00.982586-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.749539-06:00","closed_at":"2026-01-20T16:54:57.749539-06:00","close_reason":"Closed","external_ref":"gh-77"}
{"id":"evodb-bs5","title":"TDD: Standardize test-utils adoption","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:56.830178-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.269878-06:00","closed_at":"2026-01-20T16:49:22.269878-06:00","close_reason":"Closed"}
{"id":"evodb-byzk","title":"WRITE: Create example projects and starter templates","description":"## Action: WRITE (Create from scratch)\n\nCreate example projects demonstrating EvoDB capabilities and starter templates for common use cases.\n\n## Examples Directory Structure\n```\nexamples/\n├── basic-crud/           # Simple CRUD operations\n├── realtime-app/         # Real-time subscriptions\n├── vector-search/        # AI/ML vector search use case\n└── multi-tenant/         # SaaS multi-tenancy pattern\n```\n\n## Example 1: basic-crud\n**Purpose**: Simplest possible EvoDB usage for beginners\n\n### Contents\n```\nexamples/basic-crud/\n├── package.json\n├── tsconfig.json\n├── README.md\n├── src/\n│   ├── index.ts          # Main entry point\n│   ├── schema.ts         # Database schema definition\n│   └── operations.ts     # CRUD operation examples\n└── test/\n    └── crud.test.ts      # Test examples\n```\n\n### Features Demonstrated\n- Database initialization\n- Schema definition\n- Create operations (single \u0026 batch)\n- Read operations (get, query, filter)\n- Update operations (partial \u0026 full)\n- Delete operations (single \u0026 batch)\n- Basic querying with filters\n\n---\n\n## Example 2: realtime-app\n**Purpose**: Real-time data synchronization and subscriptions\n\n### Contents\n```\nexamples/realtime-app/\n├── package.json\n├── tsconfig.json\n├── README.md\n├── src/\n│   ├── server.ts         # Server with EvoDB\n│   ├── client.ts         # Client subscription example\n│   ├── schema.ts         # Schema with reactive fields\n│   └── handlers.ts       # Real-time event handlers\n└── public/\n    └── index.html        # Browser client demo\n```\n\n### Features Demonstrated\n- Real-time subscriptions\n- Live query updates\n- Optimistic updates\n- Conflict resolution\n- Presence indicators\n- WebSocket integration\n\n---\n\n## Example 3: vector-search\n**Purpose**: AI/ML use case with semantic search and RAG\n\n### Contents\n```\nexamples/vector-search/\n├── package.json\n├── tsconfig.json\n├── README.md\n├── src/\n│   ├── index.ts          # Main entry point\n│   ├── embeddings.ts     # Embedding generation\n│   ├── search.ts         # Vector search queries\n│   ├── rag.ts            # RAG pipeline example\n│   └── schema.ts         # Schema with vector fields\n└── data/\n    └── sample-docs.json  # Sample documents for demo\n```\n\n### Features Demonstrated\n- Vector field definitions\n- Embedding storage\n- Similarity search (cosine, euclidean)\n- Hybrid search (vector + filter)\n- RAG (Retrieval Augmented Generation) pipeline\n- Integration with OpenAI embeddings\n\n---\n\n## Example 4: multi-tenant\n**Purpose**: SaaS multi-tenancy patterns\n\n### Contents\n```\nexamples/multi-tenant/\n├── package.json\n├── tsconfig.json\n├── README.md\n├── src/\n│   ├── index.ts          # Main entry point\n│   ├── tenant.ts         # Tenant management\n│   ├── isolation.ts      # Data isolation patterns\n│   ├── middleware.ts     # Tenant context middleware\n│   └── schema.ts         # Multi-tenant schema design\n└── test/\n    └── isolation.test.ts # Isolation verification tests\n```\n\n### Features Demonstrated\n- Tenant isolation strategies\n- Per-tenant databases\n- Shared database with tenant scoping\n- Tenant context propagation\n- Cross-tenant queries (admin)\n- Tenant provisioning/deprovisioning\n\n---\n\n## Common Requirements for All Examples\n\n### Each Example Must Have\n1. **README.md** with:\n   - What it demonstrates\n   - Prerequisites\n   - Quick start (copy-paste ready)\n   - Architecture diagram (ASCII or Mermaid)\n   - Code walkthrough\n\n2. **package.json** with:\n   - Correct EvoDB dependency versions\n   - npm scripts: start, build, test\n   - Type definitions\n\n3. **Working Code** that:\n   - Runs without modification\n   - Has inline comments explaining key concepts\n   - Follows TypeScript best practices\n\n4. **Tests** demonstrating:\n   - How to test EvoDB operations\n   - Mocking patterns if applicable\n\n## Acceptance Criteria\n- [ ] All 4 examples created with complete code\n- [ ] Each example runs with `npm install \u0026\u0026 npm start`\n- [ ] All examples have comprehensive READMEs\n- [ ] Code is well-commented and educational\n- [ ] Tests pass for all examples\n- [ ] No external API keys required for basic demo\n- [ ] Examples work with latest EvoDB version","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:51:21.787318-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:32:48.670064-06:00","closed_at":"2026-01-21T19:32:48.670064-06:00","close_reason":"Closed"}
{"id":"evodb-c1a","title":"PRODUCT: npm package publishing and versioning strategy","description":"## Problem\nPackages are at version 0.1.0-rc.1 but not published to npm. Users cannot install packages.\n\n## Solution\n1. Set up npm publishing workflow (GitHub Actions)\n2. Define versioning strategy (semver, prereleases)\n3. Create CHANGELOG.md for each package\n4. Set up canary/nightly releases for testing\n\n## Acceptance Criteria\n- [ ] All packages published to npm under @evodb scope\n- [ ] GitHub Actions workflow for publishing\n- [ ] Automated version bumping\n- [ ] Changelog generation\n\n## Impact\nCritical blocker for adoption - users cannot install the packages","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:24.532068-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.279859-06:00","closed_at":"2026-01-21T20:14:08.279859-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-c98m","title":"TDD: Implement @evodb/graphql adapter","description":"## Overview\nImplement a GraphQL adapter that automatically generates schema, resolvers, and subscriptions from EvoDb tables.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. Schema Generation Tests\n```typescript\ndescribe('GraphQL Schema Generation', () =\u003e {\n  beforeEach(async () =\u003e {\n    await db.execute(`\n      CREATE TABLE users (\n        id INTEGER PRIMARY KEY,\n        name TEXT NOT NULL,\n        email TEXT UNIQUE,\n        created_at TEXT DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n    await db.execute(`\n      CREATE TABLE posts (\n        id INTEGER PRIMARY KEY,\n        title TEXT NOT NULL,\n        content TEXT,\n        author_id INTEGER REFERENCES users(id),\n        published BOOLEAN DEFAULT FALSE\n      )\n    `);\n  });\n\n  it('should generate type for table', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('type User {');\n    expect(schema).toContain('id: Int!');\n    expect(schema).toContain('name: String!');\n    expect(schema).toContain('email: String');\n    expect(schema).toContain('createdAt: String');\n  });\n\n  it('should generate input types', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('input UserInput {');\n    expect(schema).toContain('input UserUpdateInput {');\n    expect(schema).toContain('input UserWhereInput {');\n  });\n\n  it('should generate query type', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('type Query {');\n    expect(schema).toContain('user(id: Int!): User');\n    expect(schema).toContain('users(where: UserWhereInput, limit: Int, offset: Int): [User!]!');\n  });\n\n  it('should generate mutation type', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('type Mutation {');\n    expect(schema).toContain('createUser(input: UserInput!): User!');\n    expect(schema).toContain('updateUser(id: Int!, input: UserUpdateInput!): User');\n    expect(schema).toContain('deleteUser(id: Int!): Boolean!');\n  });\n\n  it('should generate subscription type', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('type Subscription {');\n    expect(schema).toContain('userCreated: User!');\n    expect(schema).toContain('userUpdated: User!');\n    expect(schema).toContain('userDeleted: User!');\n  });\n\n  it('should generate relationship fields', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    // Post -\u003e User relationship\n    expect(schema).toContain('author: User');\n    // User -\u003e Post[] relationship\n    expect(schema).toContain('posts: [Post!]!');\n  });\n\n  it('should map SQLite types to GraphQL types', async () =\u003e {\n    await db.execute(`\n      CREATE TABLE typed (\n        int_col INTEGER,\n        real_col REAL,\n        text_col TEXT,\n        blob_col BLOB,\n        bool_col BOOLEAN\n      )\n    `);\n    \n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('intCol: Int');\n    expect(schema).toContain('realCol: Float');\n    expect(schema).toContain('textCol: String');\n    expect(schema).toContain('blobCol: String'); // Base64 encoded\n    expect(schema).toContain('boolCol: Boolean');\n  });\n\n  it('should handle NOT NULL constraint', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('name: String!'); // NOT NULL -\u003e non-nullable\n    expect(schema).toContain('email: String'); // Nullable\n  });\n\n  it('should generate filter input for where clauses', async () =\u003e {\n    const schema = await generateGraphQLSchema(db);\n    \n    expect(schema).toContain('input UserWhereInput {');\n    expect(schema).toContain('id: Int');\n    expect(schema).toContain('id_eq: Int');\n    expect(schema).toContain('id_ne: Int');\n    expect(schema).toContain('id_gt: Int');\n    expect(schema).toContain('id_gte: Int');\n    expect(schema).toContain('id_lt: Int');\n    expect(schema).toContain('id_lte: Int');\n    expect(schema).toContain('id_in: [Int!]');\n    expect(schema).toContain('name_contains: String');\n    expect(schema).toContain('name_startsWith: String');\n    expect(schema).toContain('AND: [UserWhereInput!]');\n    expect(schema).toContain('OR: [UserWhereInput!]');\n  });\n});\n```\n\n#### 2. Resolver Tests\n```typescript\ndescribe('GraphQL Resolvers', () =\u003e {\n  let server: GraphQLServer;\n\n  beforeEach(async () =\u003e {\n    server = await createGraphQLServer(db);\n    await db.execute('INSERT INTO users (id, name, email) VALUES (1, ?, ?)', ['Alice', 'alice@test.com']);\n    await db.execute('INSERT INTO posts (id, title, author_id) VALUES (1, ?, 1)', ['Hello World']);\n  });\n\n  describe('Query Resolvers', () =\u003e {\n    it('should resolve single record by ID', async () =\u003e {\n      const result = await server.execute({\n        query: `query { user(id: 1) { id name email } }`,\n      });\n      \n      expect(result.data.user).toEqual({\n        id: 1,\n        name: 'Alice',\n        email: 'alice@test.com',\n      });\n    });\n\n    it('should resolve list with filtering', async () =\u003e {\n      await db.execute('INSERT INTO users (id, name) VALUES (2, ?)', ['Bob']);\n      \n      const result = await server.execute({\n        query: `query { users(where: { name_contains: \"Ali\" }) { id name } }`,\n      });\n      \n      expect(result.data.users).toHaveLength(1);\n      expect(result.data.users[0].name).toBe('Alice');\n    });\n\n    it('should resolve with pagination', async () =\u003e {\n      for (let i = 2; i \u003c= 10; i++) {\n        await db.execute('INSERT INTO users (id, name) VALUES (?, ?)', [i, `User ${i}`]);\n      }\n      \n      const result = await server.execute({\n        query: `query { users(limit: 5, offset: 2) { id } }`,\n      });\n      \n      expect(result.data.users).toHaveLength(5);\n      expect(result.data.users[0].id).toBe(3);\n    });\n\n    it('should resolve relationships', async () =\u003e {\n      const result = await server.execute({\n        query: `query { \n          user(id: 1) { \n            name \n            posts { id title } \n          } \n        }`,\n      });\n      \n      expect(result.data.user.posts).toHaveLength(1);\n      expect(result.data.user.posts[0].title).toBe('Hello World');\n    });\n\n    it('should resolve nested relationships', async () =\u003e {\n      const result = await server.execute({\n        query: `query { \n          post(id: 1) { \n            title \n            author { name email } \n          } \n        }`,\n      });\n      \n      expect(result.data.post.author.name).toBe('Alice');\n    });\n  });\n\n  describe('Mutation Resolvers', () =\u003e {\n    it('should create record', async () =\u003e {\n      const result = await server.execute({\n        query: `mutation { \n          createUser(input: { name: \"Bob\", email: \"bob@test.com\" }) { \n            id name email \n          } \n        }`,\n      });\n      \n      expect(result.data.createUser.name).toBe('Bob');\n      expect(result.data.createUser.id).toBeDefined();\n    });\n\n    it('should update record', async () =\u003e {\n      const result = await server.execute({\n        query: `mutation { \n          updateUser(id: 1, input: { name: \"Alice Updated\" }) { \n            id name \n          } \n        }`,\n      });\n      \n      expect(result.data.updateUser.name).toBe('Alice Updated');\n    });\n\n    it('should delete record', async () =\u003e {\n      const result = await server.execute({\n        query: `mutation { deleteUser(id: 1) }`,\n      });\n      \n      expect(result.data.deleteUser).toBe(true);\n      \n      const user = await db.queryOne('SELECT * FROM users WHERE id = 1');\n      expect(user).toBeUndefined();\n    });\n\n    it('should return null for update on non-existent record', async () =\u003e {\n      const result = await server.execute({\n        query: `mutation { \n          updateUser(id: 999, input: { name: \"Nobody\" }) { id } \n        }`,\n      });\n      \n      expect(result.data.updateUser).toBeNull();\n    });\n\n    it('should handle validation errors', async () =\u003e {\n      const result = await server.execute({\n        query: `mutation { \n          createUser(input: { email: \"invalid\" }) { id } \n        }`,\n      });\n      \n      expect(result.errors).toBeDefined();\n      expect(result.errors[0].message).toContain('name'); // Required field\n    });\n  });\n});\n```\n\n#### 3. Subscription Tests\n```typescript\ndescribe('GraphQL Subscriptions', () =\u003e {\n  let server: GraphQLServer;\n  let subscriptions: AsyncIterator\u003cany\u003e[];\n\n  beforeEach(async () =\u003e {\n    server = await createGraphQLServer(db);\n    subscriptions = [];\n  });\n\n  afterEach(async () =\u003e {\n    for (const sub of subscriptions) {\n      await sub.return?.();\n    }\n  });\n\n  it('should emit on record creation', async () =\u003e {\n    const subscription = await server.subscribe({\n      query: `subscription { userCreated { id name } }`,\n    });\n    subscriptions.push(subscription);\n    \n    // Trigger creation via mutation\n    await server.execute({\n      query: `mutation { createUser(input: { name: \"Bob\" }) { id } }`,\n    });\n    \n    const { value } = await subscription.next();\n    expect(value.data.userCreated.name).toBe('Bob');\n  });\n\n  it('should emit on record update', async () =\u003e {\n    await db.execute('INSERT INTO users (id, name) VALUES (1, ?)', ['Alice']);\n    \n    const subscription = await server.subscribe({\n      query: `subscription { userUpdated { id name } }`,\n    });\n    subscriptions.push(subscription);\n    \n    await server.execute({\n      query: `mutation { updateUser(id: 1, input: { name: \"Alice Updated\" }) { id } }`,\n    });\n    \n    const { value } = await subscription.next();\n    expect(value.data.userUpdated.name).toBe('Alice Updated');\n  });\n\n  it('should emit on record deletion', async () =\u003e {\n    await db.execute('INSERT INTO users (id, name) VALUES (1, ?)', ['Alice']);\n    \n    const subscription = await server.subscribe({\n      query: `subscription { userDeleted { id } }`,\n    });\n    subscriptions.push(subscription);\n    \n    await server.execute({\n      query: `mutation { deleteUser(id: 1) }`,\n    });\n    \n    const { value } = await subscription.next();\n    expect(value.data.userDeleted.id).toBe(1);\n  });\n\n  it('should filter subscriptions', async () =\u003e {\n    const subscription = await server.subscribe({\n      query: `subscription { userUpdated(where: { id: 1 }) { id name } }`,\n    });\n    subscriptions.push(subscription);\n    \n    await db.execute('INSERT INTO users (id, name) VALUES (1, ?), (2, ?)', ['Alice', 'Bob']);\n    \n    // Update user 2 - should not emit\n    await server.execute({\n      query: `mutation { updateUser(id: 2, input: { name: \"Bob Updated\" }) { id } }`,\n    });\n    \n    // Update user 1 - should emit\n    await server.execute({\n      query: `mutation { updateUser(id: 1, input: { name: \"Alice Updated\" }) { id } }`,\n    });\n    \n    const { value } = await subscription.next();\n    expect(value.data.userUpdated.id).toBe(1);\n  });\n\n  it('should support WebSocket transport', async () =\u003e {\n    const client = createWebSocketClient('ws://localhost:4000/graphql');\n    \n    const messages: any[] = [];\n    const subscription = client.subscribe(\n      { query: `subscription { userCreated { id name } }` },\n      (msg) =\u003e messages.push(msg)\n    );\n    \n    await server.execute({\n      query: `mutation { createUser(input: { name: \"WebSocket User\" }) { id } }`,\n    });\n    \n    await waitFor(() =\u003e messages.length \u003e 0);\n    expect(messages[0].data.userCreated.name).toBe('WebSocket User');\n    \n    subscription.unsubscribe();\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/graphql/src/schema-generator.ts\nexport class GraphQLSchemaGenerator {\n  async generate(db: EvoDb): Promise\u003cGraphQLSchema\u003e {\n    const tables = await this.getTableInfo(db);\n    const relationships = await this.inferRelationships(db, tables);\n    \n    const types = tables.map(t =\u003e this.generateType(t, relationships));\n    const inputs = tables.flatMap(t =\u003e this.generateInputTypes(t));\n    const queries = tables.map(t =\u003e this.generateQueries(t));\n    const mutations = tables.map(t =\u003e this.generateMutations(t));\n    const subscriptions = tables.map(t =\u003e this.generateSubscriptions(t));\n    \n    const schema = `\n      ${types.join('\\n\\n')}\n      ${inputs.join('\\n\\n')}\n      \n      type Query {\n        ${queries.join('\\n        ')}\n      }\n      \n      type Mutation {\n        ${mutations.join('\\n        ')}\n      }\n      \n      type Subscription {\n        ${subscriptions.join('\\n        ')}\n      }\n    `;\n    \n    return buildSchema(schema);\n  }\n\n  private generateType(table: TableInfo, relationships: Relationship[]): string {\n    const fields = table.columns.map(col =\u003e {\n      const type = this.sqliteToGraphQL(col.type, col.notnull);\n      return `${this.toCamelCase(col.name)}: ${type}`;\n    });\n    \n    // Add relationship fields\n    const rels = relationships.filter(r =\u003e r.from === table.name || r.to === table.name);\n    for (const rel of rels) {\n      if (rel.from === table.name) {\n        fields.push(`${this.toCamelCase(rel.toTable)}: ${this.toPascalCase(rel.toTable)}`);\n      } else {\n        fields.push(`${this.toCamelCase(rel.fromTable)}s: [${this.toPascalCase(rel.fromTable)}!]!`);\n      }\n    }\n    \n    return `type ${this.toPascalCase(table.name)} {\\n  ${fields.join('\\n  ')}\\n}`;\n  }\n\n  private generateInputTypes(table: TableInfo): string[] {\n    const name = this.toPascalCase(table.name);\n    const fields = table.columns\n      .filter(c =\u003e c.name !== 'id' \u0026\u0026 !c.pk)\n      .map(c =\u003e `${this.toCamelCase(c.name)}: ${this.sqliteToGraphQL(c.type, false)}`);\n    \n    return [\n      `input ${name}Input {\\n  ${fields.join('\\n  ')}\\n}`,\n      `input ${name}UpdateInput {\\n  ${fields.join('\\n  ')}\\n}`,\n      this.generateWhereInput(table),\n    ];\n  }\n\n  private sqliteToGraphQL(sqliteType: string, required: boolean): string {\n    const baseType = {\n      'INTEGER': 'Int',\n      'REAL': 'Float',\n      'TEXT': 'String',\n      'BLOB': 'String',\n      'BOOLEAN': 'Boolean',\n    }[sqliteType.toUpperCase()] ?? 'String';\n    \n    return required ? `${baseType}!` : baseType;\n  }\n}\n\n// packages/graphql/src/resolver-generator.ts\nexport class ResolverGenerator {\n  generate(db: EvoDb, tables: TableInfo[]): Resolvers {\n    const resolvers: Resolvers = {\n      Query: {},\n      Mutation: {},\n      Subscription: {},\n    };\n    \n    for (const table of tables) {\n      const name = this.toPascalCase(table.name);\n      const camelName = this.toCamelCase(table.name);\n      \n      // Query resolvers\n      resolvers.Query[camelName] = this.createFindByIdResolver(db, table);\n      resolvers.Query[`${camelName}s`] = this.createFindManyResolver(db, table);\n      \n      // Mutation resolvers\n      resolvers.Mutation[`create${name}`] = this.createInsertResolver(db, table);\n      resolvers.Mutation[`update${name}`] = this.createUpdateResolver(db, table);\n      resolvers.Mutation[`delete${name}`] = this.createDeleteResolver(db, table);\n      \n      // Subscription resolvers\n      resolvers.Subscription[`${camelName}Created`] = this.createSubscriptionResolver(db, table, 'INSERT');\n      resolvers.Subscription[`${camelName}Updated`] = this.createSubscriptionResolver(db, table, 'UPDATE');\n      resolvers.Subscription[`${camelName}Deleted`] = this.createSubscriptionResolver(db, table, 'DELETE');\n    }\n    \n    return resolvers;\n  }\n\n  private createFindByIdResolver(db: EvoDb, table: TableInfo) {\n    return async (_: any, { id }: { id: number }) =\u003e {\n      return db.queryOne(`SELECT * FROM ${table.name} WHERE id = ?`, [id]);\n    };\n  }\n\n  private createFindManyResolver(db: EvoDb, table: TableInfo) {\n    return async (_: any, args: { where?: any; limit?: number; offset?: number }) =\u003e {\n      let sql = `SELECT * FROM ${table.name}`;\n      const params: unknown[] = [];\n      \n      if (args.where) {\n        const { clause, values } = this.buildWhereClause(args.where);\n        sql += ` WHERE ${clause}`;\n        params.push(...values);\n      }\n      \n      if (args.limit) {\n        sql += ` LIMIT ?`;\n        params.push(args.limit);\n      }\n      \n      if (args.offset) {\n        sql += ` OFFSET ?`;\n        params.push(args.offset);\n      }\n      \n      return db.query(sql, params);\n    };\n  }\n\n  private createSubscriptionResolver(db: EvoDb, table: TableInfo, operation: string) {\n    return {\n      subscribe: async function* (_: any, args: { where?: any }) {\n        const subscription = db.subscribe(table.name, {\n          operations: [operation],\n          filter: args.where,\n        });\n        \n        for await (const event of subscription) {\n          yield { [`${table.name}${operation === 'INSERT' ? 'Created' : operation === 'UPDATE' ? 'Updated' : 'Deleted'}`]: event.row };\n        }\n      },\n    };\n  }\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. Query Batching (DataLoader)\n```typescript\n// packages/graphql/src/dataloader.ts\nexport function createDataLoaders(db: EvoDb, tables: TableInfo[]): DataLoaders {\n  const loaders: DataLoaders = {};\n  \n  for (const table of tables) {\n    loaders[table.name] = new DataLoader(async (ids: readonly number[]) =\u003e {\n      const placeholders = ids.map(() =\u003e '?').join(', ');\n      const rows = await db.query(\n        `SELECT * FROM ${table.name} WHERE id IN (${placeholders})`,\n        [...ids]\n      );\n      \n      // Return in same order as requested ids\n      const rowMap = new Map(rows.map(r =\u003e [r.id, r]));\n      return ids.map(id =\u003e rowMap.get(id) ?? null);\n    }, {\n      cache: true,\n      maxBatchSize: 100,\n    });\n  }\n  \n  return loaders;\n}\n\n// Use in resolvers\nexport function createResolversWithDataLoader(db: EvoDb, tables: TableInfo[]): Resolvers {\n  return {\n    Query: {\n      user: (_, { id }, context) =\u003e context.loaders.users.load(id),\n      users: async (_, args, context) =\u003e {\n        const ids = await db.query(`SELECT id FROM users WHERE ...`);\n        return context.loaders.users.loadMany(ids.map(r =\u003e r.id));\n      },\n    },\n    Post: {\n      author: (post, _, context) =\u003e context.loaders.users.load(post.author_id),\n    },\n    User: {\n      posts: async (user, _, context) =\u003e {\n        // Use a separate loader for has-many relationships\n        return context.loaders.userPosts.load(user.id);\n      },\n    },\n  };\n}\n```\n\n#### 2. Query Caching\n```typescript\n// packages/graphql/src/cache.ts\nexport class GraphQLCache {\n  private cache: LRUCache\u003cstring, CacheEntry\u003e;\n  private invalidationRules: InvalidationRule[] = [];\n\n  constructor(options: CacheOptions = {}) {\n    this.cache = new LRUCache({\n      max: options.maxSize ?? 1000,\n      ttl: options.ttl ?? 60000,\n    });\n  }\n\n  // Cache query results\n  async cachedQuery\u003cT\u003e(\n    key: string,\n    query: () =\u003e Promise\u003cT\u003e,\n    options?: { ttl?: number; tags?: string[] }\n  ): Promise\u003cT\u003e {\n    const cached = this.cache.get(key);\n    if (cached) return cached.data as T;\n    \n    const result = await query();\n    this.cache.set(key, { data: result, tags: options?.tags ?? [] });\n    \n    return result;\n  }\n\n  // Invalidate on mutations\n  invalidate(tags: string[]): void {\n    for (const [key, entry] of this.cache.entries()) {\n      if (entry.tags.some(t =\u003e tags.includes(t))) {\n        this.cache.delete(key);\n      }\n    }\n  }\n\n  // Auto-invalidate based on table changes\n  setupAutoInvalidation(db: EvoDb): void {\n    db.on('change', (event) =\u003e {\n      this.invalidate([event.table, `${event.table}:${event.rowId}`]);\n    });\n  }\n}\n\n// Wrap resolvers with caching\nexport function withCache(resolver: Resolver, cache: GraphQLCache): Resolver {\n  return async (parent, args, context, info) =\u003e {\n    const key = createCacheKey(info.fieldName, args);\n    const tags = [info.returnType.toString()];\n    \n    return cache.cachedQuery(key, () =\u003e resolver(parent, args, context, info), { tags });\n  };\n}\n```\n\n#### 3. Schema Customization\n```typescript\n// packages/graphql/src/schema-customizer.ts\nexport class SchemaCustomizer {\n  private customTypes: Map\u003cstring, CustomTypeConfig\u003e = new Map();\n  private hiddenFields: Set\u003cstring\u003e = new Set();\n  private computedFields: Map\u003cstring, ComputedField\u003e = new Map();\n\n  // Hide sensitive fields\n  hideField(table: string, field: string): this {\n    this.hiddenFields.add(`${table}.${field}`);\n    return this;\n  }\n\n  // Add computed fields\n  addComputedField(table: string, field: string, resolver: FieldResolver): this {\n    this.computedFields.set(`${table}.${field}`, { resolver, type: 'String' });\n    return this;\n  }\n\n  // Rename fields\n  renameField(table: string, from: string, to: string): this {\n    this.customTypes.set(`${table}.${from}`, { rename: to });\n    return this;\n  }\n\n  // Add custom validation\n  addValidation(table: string, field: string, validator: Validator): this {\n    const key = `${table}.${field}`;\n    const existing = this.customTypes.get(key) ?? {};\n    this.customTypes.set(key, { ...existing, validator });\n    return this;\n  }\n\n  // Apply customizations to schema\n  apply(schema: GraphQLSchema): GraphQLSchema {\n    // Use schema transforms to apply customizations\n    return mapSchema(schema, {\n      [MapperKind.OBJECT_FIELD]: (fieldConfig, fieldName, typeName) =\u003e {\n        const key = `${typeName}.${fieldName}`;\n        \n        if (this.hiddenFields.has(key)) {\n          return null; // Remove field\n        }\n        \n        const custom = this.customTypes.get(key);\n        if (custom?.rename) {\n          return [custom.rename, fieldConfig];\n        }\n        \n        return fieldConfig;\n      },\n    });\n  }\n}\n\n// Usage\nconst customizer = new SchemaCustomizer()\n  .hideField('User', 'passwordHash')\n  .renameField('User', 'createdAt', 'joinedAt')\n  .addComputedField('User', 'fullName', (user) =\u003e `${user.firstName} ${user.lastName}`)\n  .addValidation('User', 'email', (value) =\u003e /^.+@.+$/.test(value));\n\nconst schema = customizer.apply(await generateGraphQLSchema(db));\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] Automatic schema generation from tables\n- [ ] Query, Mutation, and Subscription resolvers\n- [ ] Relationship resolution (one-to-one, one-to-many)\n- [ ] Filtering, pagination, and sorting\n- [ ] WebSocket subscriptions\n- [ ] DataLoader batching\n- [ ] Response caching with invalidation\n- [ ] Schema customization API\n\n## Labels\ngraphql, api, schema, subscriptions, dataloader","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:40:24.12643-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T02:25:28.275988-06:00","closed_at":"2026-01-22T02:25:28.275988-06:00","close_reason":"Closed"}
{"id":"evodb-can","title":"TypeScript: Unify duplicate R2 interface definitions across packages","description":"## Summary\n\nThe R2Bucket interface is defined multiple times across packages with slight variations. This creates maintenance burden and potential type incompatibilities.\n\n## Locations\n\n### Duplicate R2Bucket Definitions\n\n1. **query/src/types.ts:1463-1504** - R2Bucket, R2GetOptions, R2Range, R2Object, etc.\n2. **reader/src/types.ts:254-294** - R2Bucket (subset), R2Object, R2ListOptions, R2Objects\n3. **writer/src/types.ts:593-681** - R2Bucket, R2PutOptions, R2GetOptions, R2Object, etc.\n\n### Differences\n\n| Interface | query/types | reader/types | writer/types |\n|-----------|-------------|--------------|--------------|\n| R2Bucket.get | with options | no options | with options |\n| R2Bucket.put | missing | missing | present |\n| R2Object.body | ReadableStream\\|undefined | missing | ReadableStream |\n| R2Checksums | missing | missing | present |\n\n## Impact\n\n- Different packages may have incompatible R2 expectations\n- Changes to R2 API require updates in multiple files\n- Consumers may face type errors when using across packages\n\n## Recommendations\n\n### Option 1: Use @cloudflare/workers-types (Preferred)\n\nAll packages already depend on @cloudflare/workers-types. Use those types directly:\n\n```typescript\n// In each package's types.ts\nimport type { R2Bucket, R2Object, R2ListOptions } from '@cloudflare/workers-types';\nexport type { R2Bucket, R2Object, R2ListOptions };\n```\n\n### Option 2: Create shared types in @evodb/core\n\n```typescript\n// core/src/r2-types.ts\nexport interface R2Bucket {\n  get(key: string, options?: R2GetOptions): Promise\u003cR2ObjectBody | null\u003e;\n  put(key: string, value: R2PutValue, options?: R2PutOptions): Promise\u003cR2Object\u003e;\n  // ... full interface\n}\n```\n\n### Option 3: Minimal subset in core, extend per-package\n\nDefine minimal interface in core, let packages extend as needed.\n\n## References\n\n- @cloudflare/workers-types R2 definitions\n- TypeScript module augmentation","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:56.264212-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.144878-06:00","closed_at":"2026-01-21T20:14:08.144878-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-cl1","title":"TDD: Standardize error handling with typed exceptions","description":"Create QueryError, TimeoutError, ValidationError classes. Replace plain Error throws with typed exceptions.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:05.267982-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:19:33.023012-06:00","closed_at":"2026-01-20T13:19:33.023012-06:00","close_reason":"Closed"}
{"id":"evodb-cn6","title":"TypeScript: Simplify branded types - reduce to BlockId and TableId only","description":"## Summary\n\nThe codebase correctly uses branded types for critical identifiers but the current implementation has deprecated pass-through functions. This issue tracks cleanup.\n\n## Current State (core/src/types.ts)\n\n### Properly Branded (Keep)\n- `BlockId = Brand\u003cstring, 'BlockId'\u003e` - Critical for data integrity\n- `TableId = Brand\u003cstring, 'TableId'\u003e` - Critical for referential integrity\n\n### Simplified (Deprecated Functions)\n- `SnapshotId = string` - Plain string, deprecated validators\n- `BatchId = string` - Plain string, deprecated validators  \n- `WalId = string` - Plain string, deprecated validators\n- `SchemaId = number` - Plain number, deprecated validators\n\n## Issue\n\nThe deprecated functions (snapshotId, batchId, walId, schemaId) are still exported:\n\n```typescript\n/**\n * @deprecated No longer validates - SnapshotId is now a plain string type (evodb-3ju)\n */\nexport function snapshotId(id: string): SnapshotId {\n  return id;\n}\n```\n\nThese add no value and may confuse consumers.\n\n## Recommendations\n\n1. **Remove deprecated constructor functions** after migration period:\n   - `snapshotId()`, `unsafeSnapshotId()`\n   - `batchId()`, `unsafeBatchId()`\n   - `walId()`, `unsafeWalId()`\n   - `schemaId()`, `unsafeSchemaId()`\n\n2. **Keep branded types for critical IDs**:\n   - `blockId()` and `unsafeBlockId()` with validation\n   - `tableId()` and `unsafeTableId()` with validation\n\n3. **Document the decision**:\n   - Reference evodb-3ju in CHANGELOG\n   - Explain why some IDs don't need branding\n\n## Migration Path\n\n1. Search for usages of deprecated functions\n2. Replace with direct assignment:\n   ```typescript\n   // Before\n   const id = snapshotId('snap-123');\n   // After\n   const id: SnapshotId = 'snap-123';\n   // Or simply\n   const id = 'snap-123';\n   ```\n\n## References\n\n- TDD issue evodb-3ju (original simplification)\n- TypeScript branded types pattern","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:26.18409-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:25:34.313755-06:00","closed_at":"2026-01-21T20:25:34.313755-06:00","close_reason":"Closed"}
{"id":"evodb-cnt8","title":"TDD: Integration tests for writer flush atomicity","description":"## Overview\nAdd comprehensive integration tests for writer flush atomicity to ensure data integrity during flush operations.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\nWrite integration tests that verify flush atomicity guarantees:\n\n```typescript\ndescribe('Writer Flush Atomicity', () =\u003e {\n  describe('partial flush failure', () =\u003e {\n    it('should rollback on partial write failure', async () =\u003e {\n      // Test that if flush fails midway, no partial data is persisted\n    });\n    \n    it('should not corrupt existing data on flush failure', async () =\u003e {\n      // Verify existing data remains intact after failed flush\n    });\n    \n    it('should report accurate flush status after failure', async () =\u003e {\n      // Ensure flush state reflects actual persisted state\n    });\n  });\n  \n  describe('recovery', () =\u003e {\n    it('should recover from interrupted flush on restart', async () =\u003e {\n      // Simulate crash during flush, verify recovery\n    });\n    \n    it('should detect and clean up incomplete flush artifacts', async () =\u003e {\n      // Check for orphaned temporary files/state\n    });\n    \n    it('should resume from last successful checkpoint', async () =\u003e {\n      // Verify checkpoint-based recovery works correctly\n    });\n  });\n  \n  describe('idempotency', () =\u003e {\n    it('should produce same result when flush is retried', async () =\u003e {\n      // Multiple flush attempts should be safe\n    });\n    \n    it('should handle concurrent flush attempts gracefully', async () =\u003e {\n      // Race condition testing for duplicate flushes\n    });\n    \n    it('should deduplicate data on retry after partial success', async () =\u003e {\n      // No duplicate records after retry\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Atomic Flush\n\n1. Implement write-ahead logging for flush operations\n2. Add transactional semantics to multi-step flushes\n3. Implement checkpoint/recovery mechanism\n4. Add flush state tracking and reporting\n5. Make all tests pass with minimal implementation\n\n### REFACTOR Phase - Chaos Testing\n\n1. Add chaos testing scenarios:\n   - Random I/O failures during flush\n   - Network partition simulation\n   - Process crash injection\n   - Storage full conditions\n   \n2. Improve test infrastructure:\n   - Add fault injection framework\n   - Create reproducible failure scenarios\n   - Add metrics collection during chaos tests\n   \n3. Performance optimization:\n   - Profile flush hot paths\n   - Optimize recovery time\n   - Reduce flush latency while maintaining atomicity\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] GREEN phase implementation passes all tests\n- [ ] Chaos testing scenarios added and passing\n- [ ] Flush recovery time \u003c 1 second for typical workloads\n- [ ] Zero data loss in all failure scenarios","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:16.903415-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:53:15.444306-06:00","closed_at":"2026-01-21T19:53:15.444306-06:00","close_reason":"Closed"}
{"id":"evodb-cpx","title":"Extract metrics.ts to optional @evodb/observability plugin","description":"## Problem\nmetrics.ts (1,283 lines, 32KB) is bundled in core but most users don't need Prometheus metrics.\n\n## TDD Approach\n1. Create @evodb/observability package\n2. Write tests for metrics functionality in new package\n3. Move metrics.ts to new package\n4. Create minimal noop metrics interface in core\n5. Verify core bundle reduction\n\n## Expected Impact\n- 32KB reduction from core bundle\n- Users opt-in to metrics when needed\n\n## Implementation\n- Create @evodb/observability/metrics entry point\n- Core exports MetricsRegistry interface only\n- Plugin provides full Prometheus implementation","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:20.174817-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:22:06.595223-06:00","closed_at":"2026-01-21T12:22:06.595223-06:00","close_reason":"Closed"}
{"id":"evodb-cu1i","title":"WRITE: Create TYPESCRIPT.md documenting type patterns","description":"## Overview\n\nDocument TypeScript patterns and conventions used throughout the EvoDb codebase for contributor guidance and API consistency.\n\n## Content Requirements\n\n### 1. Branded Types Usage\n- What are branded types and why use them\n- EvoDb's branded type implementations\n- Examples: `TableId`, `RowId`, `Timestamp`\n- Type-safe ID handling\n- Runtime validation with branded types\n\n```typescript\ntype TableId = string \u0026 { readonly __brand: 'TableId' }\ntype RowId = string \u0026 { readonly __brand: 'RowId' }\n```\n\n### 2. Type Guards and Assertions\n- User-defined type guards (`is` predicates)\n- Assertion functions (`asserts` keyword)\n- Narrowing patterns in EvoDb\n- Error handling with type narrowing\n\n```typescript\nfunction isValidRow(row: unknown): row is Row { ... }\nfunction assertValidRow(row: unknown): asserts row is Row { ... }\n```\n\n### 3. Generic Constraints\n- Table schema generics\n- Query builder type inference\n- Conditional types usage\n- Mapped types for schema transformation\n- Template literal types for paths\n\n### 4. Discriminated Unions\n- Event types and CDC messages\n- Operation result types\n- Error handling unions\n- Exhaustive checking patterns\n\n```typescript\ntype Operation = \n  | { type: 'insert'; row: Row }\n  | { type: 'update'; old: Row; new: Row }\n  | { type: 'delete'; row: Row }\n```\n\n### 5. Advanced Patterns\n- Inference from schema definitions\n- Recursive types for nested data\n- Utility types (Pick, Omit, Partial, Required)\n- Strict null checking patterns\n\n## File Location\n\n`docs/TYPESCRIPT.md`\n\n## Acceptance Criteria\n\n- [ ] All patterns have runnable examples\n- [ ] Links to relevant source code\n- [ ] TypeScript version requirements noted\n- [ ] Common pitfalls documented","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:45.652266-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T02:25:06.626093-06:00","closed_at":"2026-01-22T02:25:06.626093-06:00","close_reason":"Closed"}
{"id":"evodb-cxq","title":"TDD: Add query cancellation support","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:55.517685-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.791401-06:00","closed_at":"2026-01-20T16:54:57.791401-06:00","close_reason":"Closed"}
{"id":"evodb-cxr","title":"PRODUCT: Add interactive CLI demo/tutorial","description":"## Problem\nThe CLI (codegen) exists but there's no interactive tutorial to walk new users through the workflow.\n\n## Solution\nCreate an interactive CLI tutorial:\n```bash\nnpx evodb init          # Initialize EvoDB in current project\nnpx evodb demo          # Run interactive demo\nnpx evodb tutorial      # Step-by-step guided setup\n```\n\n## Features\n- Interactive prompts for configuration\n- Create sample data and run queries\n- Demonstrate schema evolution workflow\n- Show dev -\u003e production transition\n\n## Impact\nReduces time to first value for new users","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:17.812079-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:01.038219-06:00","closed_at":"2026-01-21T20:18:01.038219-06:00","close_reason":"Closed"}
{"id":"evodb-cyig","title":"PRODUCT: Access control and row-level security","description":"## Problem\nNo access control or authorization layer. Multi-tenant apps need row-level security.\n\n## Solution\nAdd access control:\n```typescript\n// Define policies\nawait db.policies.create('users_own_data', {\n  table: 'posts',\n  operation: ['SELECT', 'UPDATE', 'DELETE'],\n  check: (row, context) =\u003e row.userId === context.userId\n});\n\n// Context injection\nconst db = new EvoDB({\n  context: {\n    userId: request.auth.userId,\n    role: request.auth.role\n  }\n});\n\n// Policies automatically applied\nconst posts = await db.query('posts');\n// Only returns posts where userId matches\n```\n\n## Impact\nEssential for multi-tenant SaaS applications","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:52.147284-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.811603-06:00","closed_at":"2026-01-21T20:08:17.811603-06:00","close_reason":"Already implemented"}
{"id":"evodb-d0x","title":"Fix array spread in sortRows() - use in-place sort","description":"## Problem\nsortRows() in query-ops.ts:458 uses [...rows].sort() which creates a full copy of the array before sorting.\n\n## TDD Approach\n1. Write benchmark test comparing current vs in-place sort\n2. Implement in-place sort or index-based non-destructive sort\n3. Verify correctness with existing tests\n4. Verify memory reduction\n\n## Expected Impact\n- 50% memory reduction during sort operations\n- Faster sort for large datasets\n\n## Current Code (query-ops.ts:458-484)\n```typescript\nreturn [...rows].sort((a, b) =\u003e { /* comparison */ });\n```\n\n## Fix\n```typescript\n// Option 1: In-place if mutation OK\nrows.sort((a, b) =\u003e { /* comparison */ });\nreturn rows;\n\n// Option 2: Index-based for non-destructive\nconst indices = Array.from({length: rows.length}, (_, i) =\u003e i);\nindices.sort((i, j) =\u003e compare(rows[i], rows[j]));\nreturn indices.map(i =\u003e rows[i]);\n```","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:26.147247-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:09:08.874734-06:00","closed_at":"2026-01-21T12:09:08.874734-06:00","close_reason":"Closed"}
{"id":"evodb-d8s","title":"TDD: Add tests for RPC client.ts module","description":"## Problem\nThe RPC `client.ts` file is the largest source file in the RPC package (41KB) but has no dedicated test file. The existing tests cover protocol, buffer, and other modules, but client connection management, reconnection logic, and error handling are undertested.\n\n## Coverage Gap\n- `client.ts`: 41KB source, 0 direct tests\n- Connection state management\n- WebSocket reconnection logic\n- Request/response correlation\n- Timeout handling\n\n## Acceptance Criteria\n- [ ] Create `client.unit.test.ts` with tests for connection lifecycle\n- [ ] Test WebSocket state transitions (connecting, open, closing, closed)\n- [ ] Test reconnection with exponential backoff\n- [ ] Test request timeout handling\n- [ ] Test concurrent request management\n- [ ] Test error propagation from transport layer\n\n## TDD Approach\nWrite failing tests first that define expected behavior for:\n1. Connection establishment and teardown\n2. Reconnection after disconnect\n3. Request correlation and response matching\n4. Memory cleanup on connection close","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:07.232523-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.226135-06:00","closed_at":"2026-01-21T20:14:08.226135-06:00","close_reason":"Duplicate of already-implemented issues","labels":["rpc","tdd","testing"]}
{"id":"evodb-db7n","title":"PRODUCT: Offline-first/local-first sync patterns","description":"## Problem\nREADME shows edge architecture but doesn't address offline-first patterns for mobile/PWA apps.\n\n## Solution\nAdd local-first synchronization:\n```typescript\nimport { createLocalDB, sync } from '@evodb/local';\n\n// Create local database (IndexedDB or SQLite)\nconst local = createLocalDB('my-app');\n\n// Sync with remote EvoDB\nawait sync(local, {\n  remote: 'https://api.example.com/evodb',\n  strategy: 'crdt',  // or 'last-write-wins'\n  conflicts: 'server-wins'\n});\n\n// Work offline\nawait local.insert('todos', { title: 'Buy milk' });\n\n// Changes sync when online\nlocal.on('sync', (result) =\u003e {\n  console.log('Synced:', result.pushed, result.pulled);\n});\n```\n\n## Impact\nCritical for mobile and PWA applications","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:51.330509-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.061995-06:00","closed_at":"2026-01-21T20:14:08.061995-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-dflm","title":"TDD: Consolidate 5+ storage interfaces into StorageProvider","description":"## Overview\nConsolidate the 5+ different storage interfaces (BlockStorage, StorageBackend, StorageProvider, etc.) into a single unified StorageProvider interface.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests for unified StorageProvider interface\n  - Core CRUD operations (get, put, delete, list)\n  - Batch operations\n  - Metadata operations\n  - Range queries\n- [ ] Write tests for each storage implementation\n  - MemoryStorage satisfies StorageProvider\n  - R2Storage satisfies StorageProvider\n  - FileStorage satisfies StorageProvider\n- [ ] Write tests for storage adapter patterns\n  - Wrapping legacy interfaces\n  - Migration path tests\n\n### GREEN Phase - Make Tests Pass\n- [ ] Define unified StorageProvider interface in core\n  ```typescript\n  interface StorageProvider {\n    get(key: string): Promise\u003cUint8Array | null\u003e\n    put(key: string, value: Uint8Array): Promise\u003cvoid\u003e\n    delete(key: string): Promise\u003cvoid\u003e\n    list(prefix?: string): Promise\u003cstring[]\u003e\n    // ... additional methods\n  }\n  ```\n- [ ] Migrate MemoryStorage to implement StorageProvider\n- [ ] Migrate R2Storage to implement StorageProvider\n- [ ] Migrate FileStorage to implement StorageProvider\n- [ ] Create adapters for any legacy code\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Remove legacy interface definitions\n  - BlockStorage\n  - StorageBackend\n  - Other duplicates\n- [ ] Update all imports to use StorageProvider\n- [ ] Add storage capability detection\n- [ ] Document migration guide for external implementations\n\n## Acceptance Criteria\n- Single StorageProvider interface definition\n- All storage implementations use unified interface\n- Legacy interfaces removed or deprecated\n- Clear migration path documented\n- No breaking changes to public API (or documented migration)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:45.625236-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:51:42.010677-06:00","closed_at":"2026-01-21T19:51:42.010677-06:00","close_reason":"Closed"}
{"id":"evodb-dlp","title":"Simplify schema.ts to essential functions","description":"## Problem\nschema.ts (270+ lines) has complex serialization/migration logic that's rarely used.\n\n## TDD Approach\n1. Write tests for essential schema functions (inferSchema, compatibility check)\n2. Remove serialization/deserialization (manifest handles it)\n3. Remove migration functions\n4. Verify tests pass with simplified code\n\n## Expected Impact\n- Reduce from 270 to ~50 lines\n- ~8KB bundle reduction\n\n## Keep\n- inferSchema()\n- isSchemaCompatible()\n\n## Remove\n- Schema serialization/deserialization\n- promoteColumn(), promoteValue() migrations\n- Complex type promotion logic","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:26.685215-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:36:48.869292-06:00","closed_at":"2026-01-21T12:36:48.869292-06:00","close_reason":"Closed"}
{"id":"evodb-dmu","title":"DOCS: Migration guide from Dexie/SQL.js/LokiJS","description":"## Problem\nDevelopers coming from other client-side databases need guidance on migrating to EvoDB.\n\n## Solution\nCreate migration guides showing:\n1. **From Dexie** - IndexedDB patterns to EvoDB\n2. **From SQL.js** - SQL queries to EvoDB query builder\n3. **From LokiJS** - In-memory DB patterns to EvoDB\n4. **From Firebase/Supabase** - Cloud DB to edge architecture\n\n## Content for Each Guide\n- Feature comparison table\n- Code migration examples\n- Data migration strategies\n- Performance differences\n\n## Impact\nReduces friction for developers with existing apps","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:55.418464-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.918079-06:00","closed_at":"2026-01-21T20:08:17.918079-06:00","close_reason":"Already implemented"}
{"id":"evodb-doh","title":"TDD: Add benchmark package tests (10% coverage)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:36.261694-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.386056-06:00","closed_at":"2026-01-20T16:49:22.386056-06:00","close_reason":"Closed"}
{"id":"evodb-dp3","title":"TDD: Add fuzz testing for binary data","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:00.898936-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.242074-06:00","closed_at":"2026-01-20T16:49:22.242074-06:00","close_reason":"Closed"}
{"id":"evodb-dq6j","title":"WRITE: Create VECTOR_SEARCH.md for AI/ML integration guide","description":"## Overview\n\nCreate comprehensive documentation for vector search capabilities and AI/ML integration with EvoDb.\n\n## Content Requirements\n\n### 1. Lance Format Explanation\n- What is Lance format\n- Benefits over alternatives (Parquet, Arrow)\n- Column-oriented storage for vectors\n- Versioning and time travel support\n- Integration with EvoDb storage layer\n\n### 2. Index Building\n- **IVF-PQ (Inverted File with Product Quantization)**\n  - When to use\n  - Parameter tuning (nlist, nprobe, m, nbits)\n  - Build time vs query time tradeoffs\n  \n- **HNSW (Hierarchical Navigable Small World)**\n  - Graph-based indexing\n  - Parameter tuning (M, efConstruction, efSearch)\n  - Memory requirements\n  \n- Index selection criteria\n- Hybrid approaches\n\n### 3. Query Patterns for RAG\n- Retrieval-Augmented Generation workflows\n- Semantic search implementation\n- Hybrid search (vector + keyword)\n- Re-ranking strategies\n- Context window optimization\n- Chunking strategies\n\n### 4. Embedding Storage Best Practices\n- Dimension considerations (384, 768, 1536)\n- Normalization requirements\n- Quantization options (float32, float16, int8)\n- Batch embedding workflows\n- Embedding model recommendations\n\n### 5. Performance Optimization\n- Query batching\n- Caching embeddings\n- Index warming\n- Sharding strategies\n\n## File Location\n\n`docs/VECTOR_SEARCH.md`\n\n## Acceptance Criteria\n\n- [ ] Working code examples for each index type\n- [ ] Benchmark data for different configurations\n- [ ] Integration examples with popular embedding models\n- [ ] RAG pipeline example end-to-end","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:43.330359-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:01:55.050318-06:00","closed_at":"2026-01-21T20:01:55.050318-06:00","close_reason":"Closed"}
{"id":"evodb-dxq","title":"Simplify manifest.ts from 968 to ~300 lines","description":"## Problem\nmanifest.ts (968 lines) has excessive abstraction for JSON manipulation. Half the functions are object constructors.\n\n## TDD Approach\n1. Identify essential manifest functions\n2. Inline simple object construction\n3. Remove builder pattern abstractions\n4. Verify manifest operations still work\n\n## Expected Impact\n- ~670 lines removed\n- ~15KB bundle reduction\n\n## Keep Essential\n- createTable()\n- appendSnapshot()\n- pruneFiles()\n- getSnapshotFiles()\n- findSnapshotAsOf()\n\n## Remove/Inline\n- 20+ helper functions that just construct objects\n- Builder patterns\n- Intermediate abstractions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:37.752546-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T13:12:44.488625-06:00","closed_at":"2026-01-21T13:12:44.488625-06:00","close_reason":"Closed"}
{"id":"evodb-ebu","title":"TDD: Fix WebSocket null state race conditions","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:09.494944-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:27:33.868243-06:00","closed_at":"2026-01-20T17:27:33.868243-06:00","close_reason":"Closed"}
{"id":"evodb-egf","title":"TDD: Consolidate query engine duplication","description":"@evodb/query and @evodb/reader both implement query engines. Extract shared logic to common module.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:19.001445-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:18:45.238135-06:00","closed_at":"2026-01-20T13:18:45.238135-06:00","close_reason":"Query engine duplication already consolidated. Both @evodb/query and @evodb/reader use shared query operations from @evodb/core (evaluateFilter, sortRows, computeAggregations) and implement the unified QueryExecutor interface. All tests pass."}
{"id":"evodb-ehg","title":"TDD: Implement UPDATE/DELETE operations","description":"EvoDB facade only has INSERT. Add update() and delete() methods with proper CDC integration.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:56.314485-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:31:56.976806-06:00","closed_at":"2026-01-20T13:31:56.976806-06:00","close_reason":"Closed"}
{"id":"evodb-euef","title":"TDD: Add performance regression tests for encoding","description":"## Problem\nWhile there are benchmark tests, there are no automated performance regression tests that would catch encoding performance degradation. The `snippet-benchmark.unit.test.ts` exists but doesn't enforce performance baselines.\n\n## Coverage Gap\n- No baseline enforcement for encode/decode latency\n- No regression detection for dictionary encoding\n- No memory allocation tracking during encoding\n- No tests for encoding at different data sizes\n\n## Acceptance Criteria\n- [ ] Create `encode-perf.regression.test.ts`\n- [ ] Establish baseline metrics for encode/decode operations\n- [ ] Test encoding performance at 1K, 10K, 100K row sizes\n- [ ] Test dictionary encoding doesn't regress for low cardinality\n- [ ] Test RLE encoding performance for repetitive data\n- [ ] Fail tests if performance degrades beyond threshold (e.g., 20%)\n\n## TDD Approach\n1. Establish current performance baselines\n2. Write tests that enforce maximum allowed latency\n3. Include memory allocation checks\n4. Run in CI with consistent environment","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:27.197156-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:10:39.503086-06:00","closed_at":"2026-01-21T20:10:39.503086-06:00","close_reason":"Closed","labels":["core","performance","tdd","testing"]}
{"id":"evodb-fz2","title":"TDD: Add debugging guide doc","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:50.627854-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.848844-06:00","closed_at":"2026-01-20T16:54:57.848844-06:00","close_reason":"Closed"}
{"id":"evodb-g03","title":"Decouple Observability from Core Package","description":"## Current State\nThe @evodb/core package contains observability type definitions:\n- logging-types.ts: Logger, LogLevel, LogEntry, etc.\n- tracing-types.ts: Span, SpanContext, TracingConfig, etc.\n- metrics-types.ts: Counter, Gauge, Histogram, MetricsRegistry, etc.\n\nWhile @evodb/observability has the implementations.\n\nThis pattern means:\n1. Core knows about observability concepts (tight coupling)\n2. Changes to observability require core package changes\n3. Core package has larger type surface\n\n## Proposed Improvement\n1. Move all observability types to @evodb/observability\n2. @evodb/core should be purely about data types and operations\n3. Packages that need logging/tracing/metrics import from @evodb/observability\n4. Make observability an optional peer dependency for other packages\n\n## Alternative Approach\nKeep types in core but minimize to just interfaces (no constants, no type guards).\nThis allows core code to accept loggers without importing observability.\n\n## Migration Path\n1. Audit which core code actually needs observability types\n2. If none: move all types to observability\n3. If some: keep minimal interfaces, move implementations\n4. Update imports across all packages\n\n## Trade-offs\n- Pro: Cleaner separation of concerns\n- Pro: Smaller core bundle\n- Con: More peer dependencies to manage\n- Con: Slightly more complex imports\n\n## Edge Computing Impact\n- Smaller bundle for apps not using observability\n- No impact if observability is used","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:07.704077-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:29:12.47831-06:00","closed_at":"2026-01-22T03:29:12.47831-06:00","close_reason":"Closed","labels":["architecture","decoupling","observability"]}
{"id":"evodb-gfj8","title":"PRODUCT: Transaction support with ACID guarantees","description":"## Problem\nREADME and API show basic insert/update/delete but no transaction support for atomic multi-operation batches.\n\n## Solution\nAdd transaction API:\n```typescript\nawait db.transaction(async (tx) =\u003e {\n  const user = await tx.insert('users', { name: 'Alice' });\n  await tx.insert('accounts', { userId: user._id, balance: 0 });\n  // All operations commit together or rollback\n});\n```\n\n## Requirements\n- ACID guarantees within a single DO\n- Cross-table atomic operations\n- Rollback on error\n- Nested transaction support (savepoints)\n\n## Impact\nCritical for production apps with data integrity requirements","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:13.42444-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.757158-06:00","closed_at":"2026-01-21T20:08:17.757158-06:00","close_reason":"Already implemented"}
{"id":"evodb-gm1u","title":"EDIT: Improve root README with installation and quick start","description":"## Overview\nImprove the root README.md to provide a better first impression and more practical getting-started information for new users.\n\n## Current State\nThe root README exists but lacks:\n- Installation instructions\n- Working quick start examples\n- Badges for project health indicators\n- Concrete benefits in the \"Why EvoDB\" section\n\n## Required Changes\n\n### 1. Add Installation Instructions\n```bash\n# Once published to npm\nnpm install @evodb/core @evodb/query @evodb/writer\n\n# Or install individual packages\nnpm install @evodb/core\n```\n\n### 2. Add Quick Start Code\nProvide a minimal working example that demonstrates:\n- Creating a simple document store\n- Writing documents\n- Querying documents\n- Basic CRUD operations\n\nThe code should be copy-pasteable and actually work.\n\n### 3. Add Badges\nAdd badges at the top of README:\n- npm version badge\n- CI status badge (GitHub Actions)\n- Code coverage badge\n- TypeScript badge\n- License badge\n\nExample:\n```markdown\n[![npm version](https://img.shields.io/npm/v/@evodb/core.svg)](https://www.npmjs.com/package/@evodb/core)\n[![CI](https://github.com/evodb/evodb/workflows/CI/badge.svg)](https://github.com/evodb/evodb/actions)\n[![codecov](https://codecov.io/gh/evodb/evodb/branch/main/graph/badge.svg)](https://codecov.io/gh/evodb/evodb)\n```\n\n### 4. Improve \"Why EvoDB\" Section\nReplace vague claims with concrete benefits:\n- Specific performance numbers (e.g., \"Query 1M documents in \u003c50ms\")\n- Cost savings (e.g., \"Zero egress fees with R2\")\n- Comparison table with alternatives (SQLite, DynamoDB, etc.)\n- Real-world use cases\n\n## Acceptance Criteria\n- [ ] Installation instructions are accurate and work\n- [ ] Quick start example runs without errors\n- [ ] All badges display correctly and link to appropriate resources\n- [ ] \"Why EvoDB\" section has measurable claims\n- [ ] README follows best practices for open source projects","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:53.744073-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:42:24.528361-06:00","closed_at":"2026-01-21T19:42:24.528361-06:00","close_reason":"Closed"}
{"id":"evodb-go5v","title":"TDD: Add comprehensive input validation for security","description":"## Overview\n\nAdd comprehensive input validation to prevent security vulnerabilities including SQL injection, XSS attacks, and path traversal.\n\n## TDD Approach\n\n### RED Phase - Write Failing Tests\n\n```typescript\n// tests/core/validation/input-validation.test.ts\n\ndescribe('Input Validation Security', () =\u003e {\n  describe('SQL Injection Prevention', () =\u003e {\n    it('should reject SQL injection patterns in column names', () =\u003e {\n      const maliciousInputs = [\n        \"id; DROP TABLE users;--\",\n        \"id' OR '1'='1\",\n        \"id UNION SELECT * FROM passwords\",\n        \"id/**/OR/**/1=1\",\n      ];\n      \n      for (const input of maliciousInputs) {\n        expect(() =\u003e validateColumnName(input)).toThrow(ValidationError);\n        expect(() =\u003e validateColumnName(input)).toThrow(/SQL injection pattern detected/);\n      }\n    });\n\n    it('should reject SQL injection in query parameters', () =\u003e {\n      const maliciousParams = {\n        where: \"1=1; DROP TABLE users;--\",\n        orderBy: \"id; DELETE FROM data\",\n      };\n      \n      expect(() =\u003e validateQueryParams(maliciousParams)).toThrow(ValidationError);\n    });\n\n    it('should allow legitimate SQL-like strings in data values', () =\u003e {\n      // User might legitimately store \"SELECT\" as a value\n      expect(() =\u003e validateDataValue(\"SELECT\")).not.toThrow();\n    });\n  });\n\n  describe('XSS Prevention', () =\u003e {\n    it('should reject XSS patterns in column names', () =\u003e {\n      const xssInputs = [\n        '\u003cscript\u003ealert(\"xss\")\u003c/script\u003e',\n        'column\u003cimg src=x onerror=alert(1)\u003e',\n        'javascript:alert(1)',\n        'onmouseover=alert(1)',\n      ];\n      \n      for (const input of xssInputs) {\n        expect(() =\u003e validateColumnName(input)).toThrow(ValidationError);\n        expect(() =\u003e validateColumnName(input)).toThrow(/XSS pattern detected/);\n      }\n    });\n\n    it('should sanitize HTML in user-provided data for display', () =\u003e {\n      const input = '\u003cscript\u003eevil()\u003c/script\u003eHello';\n      expect(sanitizeForDisplay(input)).toBe('\u0026lt;script\u0026gt;evil()\u0026lt;/script\u0026gt;Hello');\n    });\n  });\n\n  describe('Path Traversal Prevention', () =\u003e {\n    it('should reject path traversal patterns', () =\u003e {\n      const traversalInputs = [\n        '../../../etc/passwd',\n        '..\\\\..\\\\windows\\\\system32',\n        '/etc/passwd',\n        'file:///etc/passwd',\n        '%2e%2e%2f%2e%2e%2f',  // URL encoded ../\n      ];\n      \n      for (const input of traversalInputs) {\n        expect(() =\u003e validatePath(input)).toThrow(ValidationError);\n        expect(() =\u003e validatePath(input)).toThrow(/path traversal/i);\n      }\n    });\n\n    it('should allow legitimate relative paths within sandbox', () =\u003e {\n      expect(() =\u003e validatePath('data/users/file.json', { sandbox: '/app/data' })).not.toThrow();\n    });\n\n    it('should reject paths escaping sandbox', () =\u003e {\n      expect(() =\u003e validatePath('../outside.json', { sandbox: '/app/data' })).toThrow();\n    });\n  });\n\n  describe('Input Length Limits', () =\u003e {\n    it('should reject excessively long inputs', () =\u003e {\n      const longInput = 'a'.repeat(10001);\n      expect(() =\u003e validateColumnName(longInput)).toThrow(/exceeds maximum length/);\n    });\n\n    it('should reject deeply nested objects', () =\u003e {\n      let nested: any = { value: 'deep' };\n      for (let i = 0; i \u003c 100; i++) {\n        nested = { child: nested };\n      }\n      expect(() =\u003e validateDataValue(nested)).toThrow(/maximum nesting depth/);\n    });\n  });\n\n  describe('Type Coercion Safety', () =\u003e {\n    it('should reject prototype pollution attempts', () =\u003e {\n      const malicious = JSON.parse('{\"__proto__\": {\"polluted\": true}}');\n      expect(() =\u003e validateDataValue(malicious)).toThrow(/prototype pollution/);\n    });\n\n    it('should reject constructor pollution', () =\u003e {\n      const malicious = { constructor: { prototype: { polluted: true } } };\n      expect(() =\u003e validateDataValue(malicious)).toThrow(/constructor pollution/);\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Validation\n\n```typescript\n// src/core/validation/input-validation.ts\n\nexport class ValidationError extends Error {\n  constructor(message: string, public code: string) {\n    super(message);\n    this.name = 'ValidationError';\n  }\n}\n\nconst SQL_INJECTION_PATTERNS = [\n  /(\\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION|ALTER|CREATE|TRUNCATE)\\b.*\\b(FROM|INTO|TABLE|WHERE)\\b)/i,\n  /(-{2}|\\/\\*|\\*\\/)/,  // SQL comments\n  /(\\bOR\\b|\\bAND\\b)\\s*[\\d'\"]?\\s*=\\s*[\\d'\"]/i,  // OR 1=1 style\n  /;\\s*(DROP|DELETE|UPDATE|INSERT)/i,  // Chained statements\n];\n\nconst XSS_PATTERNS = [\n  /\u003cscript\\b[^\u003c]*(?:(?!\u003c\\/script\u003e)\u003c[^\u003c]*)*\u003c\\/script\u003e/gi,\n  /javascript:/gi,\n  /on\\w+\\s*=/gi,  // Event handlers\n  /\u003c[^\u003e]+\\s+on\\w+\\s*=/gi,\n];\n\nconst PATH_TRAVERSAL_PATTERNS = [\n  /\\.\\.[\\/\\\\]/,\n  /%2e%2e[%2f%5c]/i,\n  /^[\\/\\\\]/,\n  /^file:/i,\n];\n\nexport function validateColumnName(input: string): void {\n  if (typeof input !== 'string') {\n    throw new ValidationError('Column name must be a string', 'INVALID_TYPE');\n  }\n  \n  if (input.length \u003e 255) {\n    throw new ValidationError('Column name exceeds maximum length of 255', 'MAX_LENGTH');\n  }\n  \n  for (const pattern of SQL_INJECTION_PATTERNS) {\n    if (pattern.test(input)) {\n      throw new ValidationError('SQL injection pattern detected', 'SQL_INJECTION');\n    }\n  }\n  \n  for (const pattern of XSS_PATTERNS) {\n    if (pattern.test(input)) {\n      throw new ValidationError('XSS pattern detected', 'XSS_DETECTED');\n    }\n  }\n}\n\nexport function validatePath(input: string, options?: { sandbox?: string }): void {\n  for (const pattern of PATH_TRAVERSAL_PATTERNS) {\n    if (pattern.test(input)) {\n      throw new ValidationError('Path traversal pattern detected', 'PATH_TRAVERSAL');\n    }\n  }\n  \n  if (options?.sandbox) {\n    const resolved = path.resolve(options.sandbox, input);\n    if (!resolved.startsWith(path.resolve(options.sandbox))) {\n      throw new ValidationError('Path escapes sandbox', 'PATH_TRAVERSAL');\n    }\n  }\n}\n\nexport function validateDataValue(value: unknown, depth = 0): void {\n  if (depth \u003e 50) {\n    throw new ValidationError('Value exceeds maximum nesting depth', 'MAX_DEPTH');\n  }\n  \n  if (value \u0026\u0026 typeof value === 'object') {\n    if ('__proto__' in value) {\n      throw new ValidationError('Prototype pollution attempt detected', 'PROTOTYPE_POLLUTION');\n    }\n    if ('constructor' in value \u0026\u0026 typeof (value as any).constructor === 'object') {\n      throw new ValidationError('Constructor pollution attempt detected', 'CONSTRUCTOR_POLLUTION');\n    }\n    \n    for (const key of Object.keys(value)) {\n      validateDataValue((value as any)[key], depth + 1);\n    }\n  }\n}\n\nexport function sanitizeForDisplay(input: string): string {\n  return input\n    .replace(/\u0026/g, '\u0026amp;')\n    .replace(/\u003c/g, '\u0026lt;')\n    .replace(/\u003e/g, '\u0026gt;')\n    .replace(/\"/g, '\u0026quot;')\n    .replace(/'/g, '\u0026#x27;');\n}\n```\n\n### REFACTOR Phase - Centralize and Optimize\n\n1. **Create unified validation module**: Move all validation to `src/core/validation/`\n2. **Add validation middleware**: Create middleware that auto-validates all inputs\n3. **Add allowlist approach**: Define valid patterns rather than blocklisting bad ones\n4. **Add rate limiting**: Prevent validation bypass through volume attacks\n5. **Add audit logging**: Log all validation failures for security monitoring\n\n```typescript\n// src/core/validation/index.ts - Centralized validation\n\nexport const Validator = {\n  columnName: validateColumnName,\n  path: validatePath,\n  dataValue: validateDataValue,\n  queryParams: validateQueryParams,\n  sanitize: {\n    forDisplay: sanitizeForDisplay,\n    forStorage: sanitizeForStorage,\n  },\n};\n\n// Middleware for automatic validation\nexport function withValidation\u003cT\u003e(\n  fn: (...args: unknown[]) =\u003e T,\n  schema: ValidationSchema\n): (...args: unknown[]) =\u003e T {\n  return (...args) =\u003e {\n    schema.validate(args);\n    return fn(...args);\n  };\n}\n```\n\n## Acceptance Criteria\n\n- [ ] All SQL injection test patterns are caught\n- [ ] All XSS patterns are detected and blocked\n- [ ] Path traversal attempts are prevented\n- [ ] Prototype pollution is blocked\n- [ ] Validation is centralized in one module\n- [ ] Security audit logging is in place\n- [ ] Performance: validation adds \u003c1ms overhead","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:27.354848-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:40:31.721199-06:00","closed_at":"2026-01-21T19:40:31.721199-06:00","close_reason":"Closed"}
{"id":"evodb-h13i","title":"Testing: Increase coverage thresholds after gap resolution","description":"## Context\nCurrent coverage thresholds are set at 70% for statements/functions/lines and 65% for branches. After resolving identified testing gaps, consider increasing these thresholds.\n\n## Current Thresholds (vitest.shared.ts)\n- statements: 70%\n- branches: 65%\n- functions: 70%\n- lines: 70%\n\n## Proposed New Thresholds (after gap resolution)\n- statements: 80%\n- branches: 75%\n- functions: 80%\n- lines: 80%\n\n## Dependencies\nThis issue should be addressed after:\n- evodb-d8s (RPC client tests)\n- evodb-6ek (RPC server tests)\n- evodb-880 (writer atomic flush tests)\n- evodb-5hqd (E2E CDC tests)\n\n## Acceptance Criteria\n- [ ] Verify coverage reaches proposed thresholds\n- [ ] Update vitest.shared.ts with new thresholds\n- [ ] Ensure CI enforces new thresholds\n- [ ] Document any packages that need exceptions","status":"closed","priority":3,"issue_type":"chore","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:16.217144-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:04:13.402973-06:00","closed_at":"2026-01-22T03:04:13.402973-06:00","close_reason":"Increased coverage thresholds from 70/65/70/70 to 75/70/75/75 across all vitest configs. Target 80/75/80/80 to be reached incrementally. All dependent issues (evodb-d8s, evodb-6ek, evodb-880, evodb-5hqd) were already closed.","labels":["coverage","tech-debt","testing"]}
{"id":"evodb-hd9","title":"TDD: Add architecture overview doc","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:38.457227-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.955054-06:00","closed_at":"2026-01-20T16:54:57.955054-06:00","close_reason":"Closed","external_ref":"gh-83"}
{"id":"evodb-hf9","title":"Consolidate Dual Query Engine Architecture","description":"## Current State\nThe codebase has a dual query engine design:\n- **SimpleQueryEngine** (formerly @evodb/reader): Lightweight for basic queries\n- **QueryEngine**: Full-featured with zone maps, bloom filters, streaming\n\nThis creates:\n1. Code duplication in filter evaluation, sorting, aggregation\n2. Confusion about which engine to use\n3. Additional maintenance burden\n4. The @evodb/reader package is essentially merged into @evodb/query but still exists\n\n## Proposed Improvement\n1. Create a unified QueryEngine with feature flags/config to enable advanced features\n2. Remove the SimpleQueryEngine class and consolidate into a single implementation\n3. Use a 'mode' config option: `{ mode: 'simple' | 'full' }` or feature flags\n4. Deprecate and remove @evodb/reader package entirely\n\n## Migration Path\n1. Add deprecation warnings to SimpleQueryEngine\n2. Implement feature flags in QueryEngine for gradual migration\n3. Update all imports to use QueryEngine with appropriate config\n4. Remove SimpleQueryEngine and reader package in v0.2.0\n\n## Benefits\n- Single source of truth for query execution\n- Reduced bundle size for simple use cases (tree-shaking)\n- Clearer API surface\n- Less maintenance\n\n## Edge Computing Impact\n- Simple mode still respects snippet constraints (5ms CPU)\n- No impact on cold starts (same bundle strategy)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:57.722888-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:30:44.403602-06:00","closed_at":"2026-01-21T20:30:44.403602-06:00","close_reason":"Closed","labels":["architecture","query","refactoring"]}
{"id":"evodb-hxtk","title":"EDIT: Expand @evodb/query README with advanced examples","description":"## Overview\nExpand the @evodb/query package README with advanced usage examples covering zone maps, bloom filters, budgets, and streaming.\n\n## Current State\nThe query package README needs more advanced examples beyond basic queries.\n\n## Required Changes\n\n### 1. Zone Map Usage Examples\n```typescript\nimport { createQuery, ZoneMapIndex } from '@evodb/query'\n\n// Zone maps enable partition pruning\nconst query = createQuery({\n  filter: { timestamp: { $gte: startDate, $lt: endDate } },\n  // Zone maps automatically prune partitions outside date range\n})\n\n// Check zone map statistics\nconst stats = await db.getZoneMapStats('timestamp')\n// { min: '2024-01-01', max: '2024-12-31', partitionsPruned: 45 }\n```\n\nDocument:\n- How zone maps work (min/max per partition)\n- Which column types support zone maps\n- Query patterns that benefit from zone maps\n- Zone map maintenance during compaction\n\n### 2. Bloom Filter Examples\n```typescript\nimport { BloomFilter } from '@evodb/query'\n\n// Bloom filters for membership testing\nconst query = createQuery({\n  filter: { userId: { $in: userIds } },\n  hints: { useBloomFilter: true },\n})\n\n// Configure bloom filter parameters\nconst config = {\n  bloomFilter: {\n    expectedItems: 10000,\n    falsePositiveRate: 0.01,\n  },\n}\n```\n\nDocument:\n- When bloom filters are used\n- False positive rate tradeoffs\n- Memory vs accuracy configuration\n- Columns that benefit from bloom filters\n\n### 3. Memory/Subrequest Budget Configuration\n```typescript\nimport { createQueryExecutor } from '@evodb/query'\n\nconst executor = createQueryExecutor({\n  // Memory budget (important for Workers)\n  memoryBudget: 128 * 1024 * 1024, // 128MB\n  \n  // Subrequest budget (Cloudflare limit: 1000/request)\n  subrequestBudget: 50,\n  \n  // Strategies when budget exceeded\n  onBudgetExceeded: 'stream', // or 'error', 'partial'\n})\n\n// Check budget usage\nconst result = await executor.execute(query)\nconsole.log(result.budgetUsage)\n// { memoryUsed: 45MB, subrequests: 12 }\n```\n\n### 4. Streaming Query Examples\n```typescript\nimport { streamQuery } from '@evodb/query'\n\n// Stream results for large datasets\nconst stream = streamQuery(db, {\n  filter: { type: 'event' },\n  batchSize: 1000,\n})\n\nfor await (const batch of stream) {\n  await processBatch(batch)\n  // Memory-efficient: only one batch in memory\n}\n\n// With backpressure\nconst stream = streamQuery(db, query, {\n  highWaterMark: 10, // batches\n  onBackpressure: () =\u003e console.log('Slowing down...'),\n})\n```\n\nDocument:\n- When to use streaming vs batch queries\n- Backpressure handling\n- Cursor-based pagination\n- Combining with Workers streaming responses\n\n## Acceptance Criteria\n- [ ] Zone map examples show real performance benefits\n- [ ] Bloom filter configuration is clearly explained\n- [ ] Budget examples include Cloudflare-specific limits\n- [ ] Streaming examples handle edge cases\n- [ ] All examples are tested and work","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:57.509619-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:59:42.931893-06:00","closed_at":"2026-01-21T19:59:42.931893-06:00","close_reason":"Closed"}
{"id":"evodb-hyt","title":"P1 Architecture: Scalability \u0026 Design","description":"Epic for P1 architectural issues: writer sharding, strategy pattern completion, type unification, and memory leak fixes.","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:27:42.264128-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:05:24.223998-06:00","closed_at":"2026-01-20T11:05:24.223998-06:00","close_reason":"Closed"}
{"id":"evodb-hyt.1","title":"Implement writer DO sharding for horizontal scaling","description":"All CDC streams funnel through a single Parent DO, creating a bottleneck at scale.\n\nImplement hash-based writer sharding:\n- Hash(tenant/table) -\u003e Writer DO Shard N -\u003e R2\n- Partition assignment logic\n- Shard discovery for readers\n\nThis is critical for scaling beyond single-DO throughput.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:57.444008-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:43:33.670077-06:00","closed_at":"2026-01-20T10:43:33.670077-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.1","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:28:57.444546-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.2","title":"Complete strategy pattern migration in writer","description":"LakehouseWriter maintains BOTH legacy components AND strategy implementations:\n- bufferStrategy + buffer (duplicate)\n- blockWriter + r2Writer (duplicate)\n- compactionStrategy + compactor (duplicate)\n\nBoth must be kept in sync (line 296-297). Remove legacy components, complete migration to strategy pattern.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:04.648686-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:39:27.679335-06:00","closed_at":"2026-01-20T10:39:27.679335-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.2","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:04.649266-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.3","title":"Unify type definitions across packages","description":"Schema and WalEntry are defined differently across packages:\n\nCore Schema: { id, version, parentVersion, columns }\nLakehouse Schema: { schemaId, version, columns, createdAt }\n\nRPC WalEntry: { sequence: number, timestamp: number, operation }\nCore WalEntry: { lsn: bigint, timestamp: bigint, op }\n\nCreate single source of truth for these types.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:09.739484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:50:19.923324-06:00","closed_at":"2026-01-20T10:50:19.923324-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.3","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:09.740111-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.4","title":"Fix memory leak in edge-cache access pattern tracking","description":"In edge-cache/src/cache-aware-planner.ts, access pattern tracking is unbounded and can grow indefinitely.\n\nAdd LRU eviction or time-based cleanup to prevent memory exhaustion under sustained load.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:13.275023-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:45:07.940607-06:00","closed_at":"2026-01-20T10:45:07.940607-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.4","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:13.275548-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.5","title":"Fix unbounded allocation from untrusted headers in snippets-lance","description":"In snippets-lance, lookup table allocation uses sizes from file headers without validation.\n\nA malicious or corrupted file could cause OOM by specifying huge allocation sizes.\n\nAdd bounds checking before allocation.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:17.281398-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:57:48.388813-06:00","closed_at":"2026-01-20T10:57:48.388813-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.5","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:17.282032-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.6","title":"Add missing error propagation in edge-cache background prefetch","description":"Background prefetch errors in edge-cache are silently swallowed.\n\nAdd proper error logging/propagation so prefetch failures are visible for debugging.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:20.96063-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:46:31.522859-06:00","closed_at":"2026-01-20T10:46:31.522859-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.6","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:20.962733-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyt.7","title":"Fix non-null assertion abuse in snippets-lance","description":"Multiple non-null assertions (!) in production code that could cause runtime errors:\n- snippets-vector-search.ts line 339: this.partitionMeta![partitionId]\n- cached-lance-reader.ts: various metadata accesses\n\nReplace with proper null checks or early returns.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:29:24.554455-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:04:37.875517-06:00","closed_at":"2026-01-20T11:04:37.875517-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-hyt.7","depends_on_id":"evodb-hyt","type":"parent-child","created_at":"2026-01-20T10:29:24.555439-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-hyz2","title":"TDD: Add Zod validation for JSON.parse results","description":"## Overview\nAdd runtime validation using Zod schemas for all JSON.parse results to catch malformed data at parse time rather than at runtime.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests for invalid manifest JSON parsing\n  - Missing required fields\n  - Wrong field types\n  - Extra unexpected fields\n- [ ] Write tests for malformed block parsing\n  - Invalid block structure\n  - Corrupted block data\n  - Schema version mismatches\n- [ ] Write tests for edge cases\n  - Empty objects\n  - Null values where objects expected\n  - Arrays where objects expected\n\n### GREEN Phase - Make Tests Pass\n- [ ] Create Zod schema for manifest structure\n- [ ] Create Zod schemas for block types\n- [ ] Create Zod schemas for metadata types\n- [ ] Wrap all JSON.parse calls with schema validation\n- [ ] Add meaningful error messages for validation failures\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Generate TypeScript types from Zod schemas (single source of truth)\n- [ ] Remove duplicate type definitions\n- [ ] Add schema composition for shared fields\n- [ ] Consider adding strict mode for development vs lenient for production\n\n## Acceptance Criteria\n- All JSON.parse calls have corresponding Zod validation\n- Type definitions are generated from schemas\n- Clear error messages identify exactly what validation failed\n- No runtime type errors from malformed JSON","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:40.67504-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:29:59.999142-06:00","closed_at":"2026-01-21T20:29:59.999142-06:00","close_reason":"Closed"}
{"id":"evodb-i2l9","title":"WRITE: Create migration guides from competitor databases","description":"## Action: WRITE (Create from scratch)\n\nCreate comprehensive migration guides for developers moving from popular competitor databases to EvoDB.\n\n## Document Locations\n```\ndocs/migration/\n├── MIGRATING-FROM-DEXIE.md\n├── MIGRATING-FROM-SQL-JS.md\n└── MIGRATING-FROM-D1.md\n```\n\n---\n\n## Guide 1: MIGRATING-FROM-DEXIE.md\n**Target audience**: Dexie.js users (IndexedDB wrapper)\n\n### Content Structure\n1. **Why Migrate from Dexie?**\n   - Server-side sync capabilities\n   - Vector search support\n   - Cloudflare Workers deployment\n\n2. **Concept Mapping**\n   | Dexie | EvoDB |\n   |-------|-------|\n   | `db.table()` | `db.collection()` |\n   | `table.add()` | `collection.insert()` |\n   | `table.put()` | `collection.upsert()` |\n   | `table.where()` | `collection.query()` |\n\n3. **Code Comparison**\n   ```typescript\n   // Dexie\n   const db = new Dexie('mydb');\n   db.version(1).stores({ users: '++id, name, email' });\n   await db.users.add({ name: 'John', email: 'john@example.com' });\n   \n   // EvoDB\n   const db = await createDatabase({\n     name: 'mydb',\n     schema: { users: { id: 'string', name: 'string', email: 'string' }}\n   });\n   await db.users.insert({ id: uuid(), name: 'John', email: 'john@example.com' });\n   ```\n\n4. **Migration Steps**\n   - Export Dexie data\n   - Transform schema\n   - Import to EvoDB\n   - Update application code\n\n5. **Feature Mapping**\n   - Indexes → EvoDB indexes\n   - Hooks → EvoDB middleware\n   - Live queries → EvoDB subscriptions\n\n---\n\n## Guide 2: MIGRATING-FROM-SQL-JS.md\n**Target audience**: sql.js users (SQLite in browser)\n\n### Content Structure\n1. **Why Migrate from sql.js?**\n   - No manual database file handling\n   - Built-in sync and persistence\n   - TypeScript-first design\n\n2. **Concept Mapping**\n   | sql.js | EvoDB |\n   |--------|-------|\n   | `db.run(sql)` | `collection.insert()` |\n   | `db.exec(sql)` | `collection.query()` |\n   | Raw SQL | Type-safe queries |\n\n3. **Code Comparison**\n   ```typescript\n   // sql.js\n   const SQL = await initSqlJs();\n   const db = new SQL.Database();\n   db.run(\"CREATE TABLE users (id TEXT, name TEXT, email TEXT)\");\n   db.run(\"INSERT INTO users VALUES (?, ?, ?)\", [id, 'John', 'john@example.com']);\n   const results = db.exec(\"SELECT * FROM users WHERE name = ?\", ['John']);\n   \n   // EvoDB\n   const db = await createDatabase({\n     name: 'mydb',\n     schema: { users: { id: 'string', name: 'string', email: 'string' }}\n   });\n   await db.users.insert({ id, name: 'John', email: 'john@example.com' });\n   const results = await db.users.query({ where: { name: 'John' }});\n   ```\n\n4. **Migration Script**\n   - Provide script to export sql.js data to JSON\n   - Schema inference from CREATE TABLE\n   - Bulk import to EvoDB\n\n5. **SQL Query Translation**\n   - SELECT → query()\n   - WHERE → where clause\n   - JOIN → reference handling\n   - GROUP BY → aggregations\n\n---\n\n## Guide 3: MIGRATING-FROM-D1.md\n**Target audience**: Cloudflare D1 users\n\n### Content Structure\n1. **Why Migrate from D1?**\n   - Local-first architecture\n   - Offline support\n   - Real-time subscriptions\n   - Vector search capabilities\n\n2. **Concept Mapping**\n   | D1 | EvoDB |\n   |----|-------|\n   | `db.prepare().run()` | `collection.insert()` |\n   | `db.prepare().all()` | `collection.query()` |\n   | SQL bindings | Type-safe API |\n   | REST API | Durable Objects |\n\n3. **Code Comparison**\n   ```typescript\n   // D1\n   const stmt = env.DB.prepare(\"SELECT * FROM users WHERE id = ?\").bind(userId);\n   const user = await stmt.first();\n   \n   // EvoDB\n   const user = await db.users.get(userId);\n   ```\n\n4. **Wrangler Configuration Changes**\n   - D1 binding removal\n   - Durable Objects setup\n   - R2 bucket configuration\n\n5. **Data Migration**\n   - Export D1 data using wrangler\n   - Transform to EvoDB format\n   - Import using bulk operations\n\n6. **Feature Comparison**\n   - D1 backups vs EvoDB persistence\n   - D1 replicas vs EvoDB sync\n   - D1 analytics vs EvoDB monitoring\n\n---\n\n## Common Sections for All Guides\n\n### Each Guide Must Include\n1. **Quick Start** - Get running in 5 minutes\n2. **Full Migration Checklist** - Step-by-step process\n3. **Code Comparison Table** - Side-by-side syntax\n4. **Feature Matrix** - What exists in both, what's new\n5. **Common Gotchas** - Known differences and how to handle\n6. **Rollback Plan** - How to revert if needed\n7. **Performance Considerations** - What to expect\n\n## Acceptance Criteria\n- [ ] All 3 migration guides created\n- [ ] Code examples tested and working\n- [ ] Feature mapping tables are accurate\n- [ ] Migration scripts provided where applicable\n- [ ] Common edge cases documented\n- [ ] Reviewed by someone familiar with source database","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:51:22.049723-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:03:45.368485-06:00","closed_at":"2026-01-21T20:03:45.368485-06:00","close_reason":"Closed"}
{"id":"evodb-i8h","title":"TDD: Add runtime validation for JSON parsing","description":"reader/src/index.ts:132 - JSON.parse without validation. Add schema validation with type guards.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:42.607913-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:25:27.658705-06:00","closed_at":"2026-01-20T13:25:27.658705-06:00","close_reason":"Closed"}
{"id":"evodb-icq0","title":"WRITE: Create SNIPPETS.md for Cloudflare Snippets optimization guide","description":"## Overview\n\nCreate comprehensive documentation for optimizing EvoDb usage within Cloudflare Snippets constraints.\n\n## Content Requirements\n\n### 1. Snippets Constraints\n- **5ms CPU time limit** - strict enforcement\n- **32MB RAM limit** - memory ceiling\n- No persistent connections\n- Cold start implications\n\n### 2. snippets-chain Usage Patterns\n- Lightweight chaining patterns\n- Avoiding heavy operations\n- Caching strategies within constraints\n- Request/response optimization\n\n### 3. snippets-lance Vector Search\n- Lance format in memory-constrained environments\n- Pre-computed index loading\n- Query optimization for 5ms budget\n- Batch vs single query tradeoffs\n\n### 4. Memory Optimization Techniques\n- Object pooling patterns\n- Avoiding allocations in hot paths\n- Buffer reuse strategies\n- Garbage collection awareness\n- Memory profiling tools\n\n### 5. Performance Patterns\n- Pre-warming strategies\n- Lazy initialization\n- Incremental processing\n- Early termination patterns\n\n## File Location\n\n`docs/SNIPPETS.md`\n\n## Acceptance Criteria\n\n- [ ] All code examples tested within Snippets constraints\n- [ ] Memory usage documented for each pattern\n- [ ] CPU time budgets provided for operations\n- [ ] Real-world use cases included","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:42.21317-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:01:53.978789-06:00","closed_at":"2026-01-21T20:01:53.978789-06:00","close_reason":"Closed"}
{"id":"evodb-ifx5","title":"TDD: Unit tests for RPC server.ts","description":"## Overview\nAdd comprehensive unit tests for the RPC server module to ensure reliable server-side message handling.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\nWrite unit tests covering all server functionality:\n\n```typescript\ndescribe('RPC Server', () =\u003e {\n  describe('message routing', () =\u003e {\n    it('should route requests to registered handlers', async () =\u003e {\n      // Handler dispatch verification\n    });\n    \n    it('should return 404 for unknown methods', async () =\u003e {\n      // Unknown method handling\n    });\n    \n    it('should validate request schema', async () =\u003e {\n      // Input validation\n    });\n    \n    it('should support middleware chain', async () =\u003e {\n      // Pre/post processing hooks\n    });\n  });\n  \n  describe('hibernation', () =\u003e {\n    it('should serialize state before hibernation', async () =\u003e {\n      // State persistence\n    });\n    \n    it('should restore state after wake', async () =\u003e {\n      // State recovery\n    });\n    \n    it('should handle in-flight requests during hibernation', async () =\u003e {\n      // Graceful request handling\n    });\n    \n    it('should maintain client connections across hibernation', async () =\u003e {\n      // WebSocket preservation\n    });\n  });\n  \n  describe('broadcast', () =\u003e {\n    it('should send message to all connected clients', async () =\u003e {\n      // Fan-out verification\n    });\n    \n    it('should support filtered broadcast', async () =\u003e {\n      // Selective client targeting\n    });\n    \n    it('should handle client disconnect during broadcast', async () =\u003e {\n      // Partial failure handling\n    });\n    \n    it('should support broadcast acknowledgment', async () =\u003e {\n      // Delivery confirmation\n    });\n  });\n});\n```\n\n### GREEN Phase - Add Tests with Mocked DO\n\n1. Create mock Durable Object environment:\n   ```typescript\n   class MockDurableObjectState {\n     storage: MockStorage;\n     id: DurableObjectId;\n     \n     waitUntil(promise: Promise\u003cany\u003e): void;\n     blockConcurrencyWhile\u003cT\u003e(fn: () =\u003e Promise\u003cT\u003e): Promise\u003cT\u003e;\n   }\n   \n   class MockStorage {\n     get\u003cT\u003e(key: string): Promise\u003cT | undefined\u003e;\n     put\u003cT\u003e(key: string, value: T): Promise\u003cvoid\u003e;\n     delete(key: string): Promise\u003cboolean\u003e;\n     list(): Promise\u003cMap\u003cstring, any\u003e\u003e;\n   }\n   ```\n\n2. Create mock WebSocket session manager:\n   ```typescript\n   class MockWebSocketSession {\n     id: string;\n     send(message: any): void;\n     close(code?: number): void;\n     simulateMessage(data: any): void;\n   }\n   ```\n\n3. Make all unit tests pass\n\n### REFACTOR Phase - Load and Concurrency Tests\n\n1. Add load tests:\n   ```typescript\n   describe('load handling', () =\u003e {\n     it('should handle 100 concurrent requests', async () =\u003e {});\n     it('should maintain response time under load', async () =\u003e {});\n     it('should queue requests when at capacity', async () =\u003e {});\n   });\n   ```\n\n2. Add concurrent client tests:\n   ```typescript\n   describe('concurrent clients', () =\u003e {\n     it('should handle 50 simultaneous connections', async () =\u003e {});\n     it('should isolate client state correctly', async () =\u003e {});\n     it('should handle rapid connect/disconnect cycles', async () =\u003e {});\n     it('should broadcast to 1000 clients efficiently', async () =\u003e {});\n   });\n   ```\n\n3. Performance optimizations:\n   - Profile handler dispatch\n   - Optimize serialization\n   - Improve broadcast efficiency\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] Mock DO environment complete\n- [ ] All GREEN phase tests passing\n- [ ] Load tests handle 100 concurrent requests\n- [ ] Concurrent client tests passing\n- [ ] 100% branch coverage for server.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:20.739777-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:03:28.473823-06:00","closed_at":"2026-01-21T20:03:28.473823-06:00","close_reason":"Closed"}
{"id":"evodb-imj","title":"RLE/Delta decode count bounds validation","description":"TDD: RLE and Delta decoders don't validate count parameter bounds. Add bounds checking to prevent buffer overflows when decoding malformed data.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:17:57.733816-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:24:13.592748-06:00","closed_at":"2026-01-20T18:24:13.592751-06:00"}
{"id":"evodb-irq","title":"TDD: Add memory limit guards to query engine","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:05.970797-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:27:33.77839-06:00","closed_at":"2026-01-20T17:27:33.77839-06:00","close_reason":"Closed"}
{"id":"evodb-ivh","title":"Use fast path for ASCII string comparison","description":"## Problem\nString comparison uses localeCompare() which is ~10x slower than \u003c operator for ASCII strings.\n\n## TDD Approach\n1. Write benchmark comparing localeCompare vs \u003c operator\n2. Add fast path for ASCII-only strings\n3. Fall back to localeCompare for unicode\n4. Verify correctness\n\n## Expected Impact\n- Faster string sorts for ASCII data (most common case)\n\n## Current Code (query-ops.ts:292-293)\n```typescript\nif (typeof a === 'string' \u0026\u0026 typeof b === 'string') {\n  return a.localeCompare(b);\n}\n```\n\n## Fix\n```typescript\nfunction compareStrings(a: string, b: string): number {\n  // Fast path for ASCII\n  if (isAscii(a) \u0026\u0026 isAscii(b)) {\n    return a \u003c b ? -1 : a \u003e b ? 1 : 0;\n  }\n  return a.localeCompare(b);\n}\n```","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:33.415745-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T13:01:25.80354-06:00","closed_at":"2026-01-21T13:01:25.80354-06:00","close_reason":"Closed"}
{"id":"evodb-j6x","title":"TDD: Normalize Query type names across packages","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:08.827174-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.184889-06:00","closed_at":"2026-01-20T16:49:22.184889-06:00","close_reason":"Closed","external_ref":"gh-84"}
{"id":"evodb-j8u","title":"TDD: Add snippets-chain tests","description":"snippets-chain package has 0 unit tests. Add comprehensive tests for scatter-gather and map-reduce patterns.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:55.579472-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:19:27.267265-06:00","closed_at":"2026-01-20T13:19:27.267265-06:00","close_reason":"Package already has comprehensive tests: 255 tests across 6 test files (executor.test.ts, chain-builder.test.ts, scatter-gather.test.ts, map-reduce.test.ts, pipeline.test.ts, types.test.ts)"}
{"id":"evodb-j92o","title":"EDIT: Update ARCHITECTURE.md with recent optimizations","description":"## Overview\nUpdate ARCHITECTURE.md to reflect recent optimizations and architectural changes made during the performance improvement phase.\n\n## Tasks\n\n### Add Observability Package Extraction Section\n- Document the extraction of observability features into a separate `@evodb/observability` package\n- Explain the rationale for this separation (bundle size, optional dependency)\n- Show how to integrate observability when needed\n\n### Update Bundle Size Estimates\n- Update bundle size numbers to reflect current optimized state\n- Include comparison of:\n  - Core package size\n  - With vs without observability\n  - Tree-shaken builds\n  - Focused entry points impact\n\n### Add Performance Optimization Section\n- Document lazy initialization patterns\n- Explain sparse bitmap optimization for IDs\n- Cover query planning improvements\n- Memory management strategies\n\n### Document New Focused Entry Points\n- List all available entry points:\n  - `@evodb/core` - main entry\n  - `@evodb/core/lite` - minimal bundle\n  - `@evodb/core/server` - server-optimized\n  - etc.\n- Explain when to use each entry point\n- Show import examples\n\n## Acceptance Criteria\n- [ ] All new architectural patterns are documented\n- [ ] Bundle sizes are accurate and up-to-date\n- [ ] Code examples compile and work\n- [ ] Links to relevant source files included","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:28.397777-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:00:01.016787-06:00","closed_at":"2026-01-21T20:00:01.016787-06:00","close_reason":"Closed"}
{"id":"evodb-j9r","title":"TDD: Add cache invalidation on block writes","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:04.364282-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.661006-06:00","closed_at":"2026-01-20T17:49:16.661006-06:00","close_reason":"Closed","external_ref":"gh-118"}
{"id":"evodb-jave","title":"WRITE: Create CHANGELOG.md following Keep a Changelog format","description":"## Overview\n\nCreate a CHANGELOG.md file following the [Keep a Changelog](https://keepachangelog.com/en/1.1.0/) format to document all changes, optimizations, and releases.\n\n## Content Requirements\n\n### 1. Format Structure\n- Follow Keep a Changelog 1.1.0 specification\n- Semantic Versioning compliance\n- Categories: Added, Changed, Deprecated, Removed, Fixed, Security\n\n### 2. Document Recent Optimizations\n- Sparse bitmap performance improvements\n- Zero-time edge case handling\n- TDD review round 3 changes (10 issues resolved)\n- Query optimization work\n- Memory efficiency improvements\n\n### 3. Breaking Changes Section\n- API changes requiring migration\n- Configuration changes\n- Deprecated feature removals\n- Type signature changes\n\n### 4. Migration Notes\n- Step-by-step upgrade guides\n- Compatibility matrices\n- Rollback procedures\n- Data migration scripts if needed\n\n### 5. Version History\n- Document from earliest available version\n- Include unreleased changes section\n- Link to GitHub releases/tags\n- Date format: YYYY-MM-DD\n\n## Example Structure\n\n```markdown\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [Unreleased]\n\n### Added\n- ...\n\n### Changed\n- ...\n\n## [1.0.0] - 2024-XX-XX\n\n### Added\n- Initial release\n```\n\n## File Location\n\n`CHANGELOG.md` (root directory)\n\n## Acceptance Criteria\n\n- [ ] Follows Keep a Changelog format exactly\n- [ ] All recent commits categorized appropriately\n- [ ] Breaking changes clearly marked\n- [ ] Migration paths documented for major changes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:48.150696-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:31:57.999713-06:00","closed_at":"2026-01-21T19:31:57.999713-06:00","close_reason":"Closed"}
{"id":"evodb-jc1","title":"TDD: Use monotonic time in circuit breaker","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:27.370756-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:36:42.851414-06:00","closed_at":"2026-01-20T17:36:42.851414-06:00","close_reason":"Closed"}
{"id":"evodb-jcph","title":"EDIT: Add usage examples to @evodb/core README","description":"## Overview\nEnhance the @evodb/core package README with comprehensive usage examples and documentation.\n\n## Current State\nThe core package README needs more practical examples showing how to use the fundamental APIs.\n\n## Required Changes\n\n### 1. Add Encoding/Decoding Examples\n```typescript\nimport { encode, decode } from '@evodb/core/encoding'\n\n// Encode a document\nconst doc = { id: '123', name: 'Alice', score: 95.5 }\nconst encoded = encode(doc)\n\n// Decode back\nconst decoded = decode(encoded)\n```\n\nShow examples for:\n- Primitive types (strings, numbers, booleans)\n- Complex objects\n- Arrays and nested structures\n- Binary data handling\n\n### 2. Add Shredding Examples\n```typescript\nimport { shred, reassemble } from '@evodb/core/shredding'\n\n// Shred a document into columnar format\nconst columns = shred(document, schema)\n\n// Reassemble from columns\nconst original = reassemble(columns, schema)\n```\n\nDemonstrate:\n- Schema definition\n- Column extraction\n- Selective column reading\n- Handling null/missing values\n\n### 3. Add Type Definitions Reference\nDocument the key types:\n- `Document` - base document type\n- `Schema` - schema definition format\n- `Column` - columnar data representation\n- `EncodedValue` - encoded value types\n\nInclude TypeScript examples showing proper typing.\n\n### 4. Document Focused Entry Points\nExplain the modular entry points:\n```typescript\n// Full package\nimport { ... } from '@evodb/core'\n\n// Just encoding\nimport { encode, decode } from '@evodb/core/encoding'\n\n// Just shredding\nimport { shred } from '@evodb/core/shredding'\n\n// Just types\nimport type { Document, Schema } from '@evodb/core/types'\n```\n\n## Acceptance Criteria\n- [ ] All code examples compile and run correctly\n- [ ] Examples cover common use cases\n- [ ] Type definitions are documented with examples\n- [ ] Entry points are clearly explained\n- [ ] Examples follow TypeScript best practices","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:54.617471-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:01:03.331136-06:00","closed_at":"2026-01-21T20:01:03.331136-06:00","close_reason":"Closed"}
{"id":"evodb-jo7","title":"Fix merge column concatenation - pre-allocate buffers","description":"## Problem\nmerge.ts:140-142 uses spread operators that create intermediate arrays during column merging.\n\n## TDD Approach\n1. Write benchmark test for merge performance\n2. Pre-allocate target arrays based on total size\n3. Use direct copy instead of spread\n4. Verify memory reduction\n\n## Expected Impact\n- 3x to 1.2x memory reduction during compaction\n- Faster merge operations\n\n## Current Code (merge.ts:140-142)\n```typescript\nvalues.push(...col.values);\nnulls.push(...col.nulls);\n```\n\n## Fix\n```typescript\nconst totalRows = columns.reduce((sum, c) =\u003e sum + c.values.length, 0);\nconst values = new Array(totalRows);\nlet offset = 0;\nfor (const col of columns) {\n  for (let i = 0; i \u003c col.values.length; i++) {\n    values[offset + i] = col.values[i];\n  }\n  offset += col.values.length;\n}\n```","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:29.346655-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:09:17.110612-06:00","closed_at":"2026-01-21T12:09:17.110612-06:00","close_reason":"Closed"}
{"id":"evodb-kfym","title":"TDD: Add tests for benchmark dataset loaders","description":"## Problem\nThe benchmark package has 47 source files but only 9 test files. Dataset loaders (ONET, Wiktionary, ClickBench, IMDB) are largely untested, which could lead to silent data quality issues in benchmarks.\n\n## Coverage Gap\n- `datasets/onet/loader.ts`: No tests\n- `datasets/wiktionary/loader.ts`: No tests  \n- `datasets/clickbench/loader.ts`: No tests\n- `datasets/imdb/`: Only basic tests\n- Schema validation during load\n- Error handling for malformed data\n\n## Acceptance Criteria\n- [ ] Create loader tests for each dataset\n- [ ] Test schema validation on load\n- [ ] Test error handling for missing/malformed files\n- [ ] Test data transformation correctness\n- [ ] Test memory efficiency for large datasets\n\n## TDD Approach\nWrite failing tests first that verify:\n1. Loaders produce correctly typed records\n2. Schema constraints are enforced\n3. Errors are reported for invalid data\n4. Resource cleanup occurs on error","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:29:58.721556-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T02:45:33.849142-06:00","closed_at":"2026-01-22T02:45:33.849142-06:00","close_reason":"Comprehensive TDD tests added for all 4 dataset loaders: ONET, ClickBench, Wiktionary, and IMDB. 110 tests covering data loading, error handling, validation, and performance.","labels":["benchmark","tdd","testing"]}
{"id":"evodb-kn18","title":"TDD: Implement real-time sync/subscriptions API","description":"## Overview\n\nImplement a real-time subscriptions API that enables clients to receive live updates when data changes, supporting efficient synchronization across distributed clients.\n\n## TDD Red-Green-Refactor Cycle\n\n### RED Phase: Write Failing Tests First\n\n```typescript\n// tests/subscriptions/subscription-api.test.ts\nimport { describe, it, expect, beforeEach, afterEach, vi } from 'vitest'\nimport { EvoDB } from '../src/evodb'\n\ndescribe('Real-time Subscriptions API', () =\u003e {\n  let db: EvoDB\n\n  beforeEach(async () =\u003e {\n    db = new EvoDB({ name: 'test-subscriptions' })\n    await db.clear()\n  })\n\n  afterEach(async () =\u003e {\n    await db.close()\n  })\n\n  describe('Subscribe/Unsubscribe', () =\u003e {\n    it('should subscribe to a key and receive initial value', async () =\u003e {\n      await db.put('user:1', { name: 'Alice' })\n      \n      const callback = vi.fn()\n      const subscription = await db.subscribe('user:1', callback)\n      \n      expect(subscription).toBeDefined()\n      expect(subscription.id).toBeDefined()\n      expect(callback).toHaveBeenCalledWith({\n        type: 'initial',\n        key: 'user:1',\n        value: { name: 'Alice' },\n        version: expect.any(Number)\n      })\n    })\n\n    it('should subscribe to a query pattern', async () =\u003e {\n      await db.put('user:1', { name: 'Alice' })\n      await db.put('user:2', { name: 'Bob' })\n      \n      const callback = vi.fn()\n      await db.subscribe({ prefix: 'user:' }, callback)\n      \n      expect(callback).toHaveBeenCalledTimes(2)\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'initial',\n        key: 'user:1'\n      }))\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'initial',\n        key: 'user:2'\n      }))\n    })\n\n    it('should unsubscribe and stop receiving updates', async () =\u003e {\n      const callback = vi.fn()\n      const subscription = await db.subscribe('user:1', callback)\n      \n      callback.mockClear()\n      await subscription.unsubscribe()\n      \n      await db.put('user:1', { name: 'Updated' })\n      \n      // Should not receive update after unsubscribe\n      await new Promise(r =\u003e setTimeout(r, 50))\n      expect(callback).not.toHaveBeenCalled()\n    })\n\n    it('should support multiple subscribers to same key', async () =\u003e {\n      const callback1 = vi.fn()\n      const callback2 = vi.fn()\n      \n      await db.subscribe('user:1', callback1)\n      await db.subscribe('user:1', callback2)\n      \n      await db.put('user:1', { name: 'Alice' })\n      \n      expect(callback1).toHaveBeenCalled()\n      expect(callback2).toHaveBeenCalled()\n    })\n  })\n\n  describe('Change Events', () =\u003e {\n    it('should emit create event when new key is added', async () =\u003e {\n      const callback = vi.fn()\n      await db.subscribe('user:1', callback)\n      callback.mockClear()\n      \n      await db.put('user:1', { name: 'Alice' })\n      \n      expect(callback).toHaveBeenCalledWith({\n        type: 'create',\n        key: 'user:1',\n        value: { name: 'Alice' },\n        previousValue: undefined,\n        version: expect.any(Number)\n      })\n    })\n\n    it('should emit update event when existing key is modified', async () =\u003e {\n      await db.put('user:1', { name: 'Alice' })\n      \n      const callback = vi.fn()\n      await db.subscribe('user:1', callback)\n      callback.mockClear()\n      \n      await db.put('user:1', { name: 'Alice Updated' })\n      \n      expect(callback).toHaveBeenCalledWith({\n        type: 'update',\n        key: 'user:1',\n        value: { name: 'Alice Updated' },\n        previousValue: { name: 'Alice' },\n        version: expect.any(Number)\n      })\n    })\n\n    it('should emit delete event when key is removed', async () =\u003e {\n      await db.put('user:1', { name: 'Alice' })\n      \n      const callback = vi.fn()\n      await db.subscribe('user:1', callback)\n      callback.mockClear()\n      \n      await db.delete('user:1')\n      \n      expect(callback).toHaveBeenCalledWith({\n        type: 'delete',\n        key: 'user:1',\n        value: undefined,\n        previousValue: { name: 'Alice' },\n        version: expect.any(Number)\n      })\n    })\n\n    it('should include delta/patch for partial updates', async () =\u003e {\n      await db.put('user:1', { name: 'Alice', age: 30, city: 'NYC' })\n      \n      const callback = vi.fn()\n      await db.subscribe('user:1', { includeDelta: true }, callback)\n      callback.mockClear()\n      \n      await db.put('user:1', { name: 'Alice', age: 31, city: 'NYC' })\n      \n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'update',\n        delta: [{ op: 'replace', path: '/age', value: 31 }]\n      }))\n    })\n  })\n\n  describe('Reconnection Handling', () =\u003e {\n    it('should automatically reconnect after disconnect', async () =\u003e {\n      const callback = vi.fn()\n      const subscription = await db.subscribe('user:1', callback)\n      \n      // Simulate disconnect\n      await subscription._simulateDisconnect()\n      \n      expect(subscription.status).toBe('reconnecting')\n      \n      // Wait for reconnection\n      await vi.waitFor(() =\u003e {\n        expect(subscription.status).toBe('connected')\n      }, { timeout: 5000 })\n    })\n\n    it('should emit reconnected event with missed changes', async () =\u003e {\n      await db.put('user:1', { name: 'Alice', version: 1 })\n      \n      const callback = vi.fn()\n      const subscription = await db.subscribe('user:1', callback)\n      const lastVersion = callback.mock.calls[0][0].version\n      callback.mockClear()\n      \n      // Simulate disconnect and updates during disconnect\n      await subscription._simulateDisconnect()\n      await db.put('user:1', { name: 'Bob', version: 2 })\n      await db.put('user:1', { name: 'Charlie', version: 3 })\n      \n      // Reconnect\n      await subscription._simulateReconnect(lastVersion)\n      \n      // Should receive missed changes\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'sync',\n        missedChanges: expect.arrayContaining([\n          expect.objectContaining({ value: { name: 'Bob', version: 2 } }),\n          expect.objectContaining({ value: { name: 'Charlie', version: 3 } })\n        ])\n      }))\n    })\n\n    it('should emit error event on reconnection failure', async () =\u003e {\n      const onError = vi.fn()\n      const subscription = await db.subscribe('user:1', vi.fn(), { onError })\n      \n      // Simulate permanent disconnect\n      subscription._setMaxRetries(0)\n      await subscription._simulateDisconnect()\n      \n      await vi.waitFor(() =\u003e {\n        expect(onError).toHaveBeenCalledWith(expect.objectContaining({\n          type: 'connection_failed',\n          retries: 0\n        }))\n      })\n    })\n\n    it('should use exponential backoff for retries', async () =\u003e {\n      const subscription = await db.subscribe('user:1', vi.fn())\n      \n      const retryDelays: number[] = []\n      subscription.on('retry', (event) =\u003e retryDelays.push(event.delay))\n      \n      await subscription._simulateDisconnect()\n      await subscription._simulateFailedReconnect()\n      await subscription._simulateFailedReconnect()\n      await subscription._simulateFailedReconnect()\n      \n      // Verify exponential backoff\n      expect(retryDelays[1]).toBeGreaterThan(retryDelays[0])\n      expect(retryDelays[2]).toBeGreaterThan(retryDelays[1])\n    })\n  })\n\n  describe('Query Subscriptions', () =\u003e {\n    it('should subscribe to query results', async () =\u003e {\n      await db.put('task:1', { status: 'pending', title: 'Task 1' })\n      await db.put('task:2', { status: 'done', title: 'Task 2' })\n      await db.put('task:3', { status: 'pending', title: 'Task 3' })\n      \n      const callback = vi.fn()\n      await db.subscribe(\n        { where: { status: 'pending' } },\n        callback\n      )\n      \n      // Should only receive pending tasks\n      expect(callback).toHaveBeenCalledTimes(2)\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({ key: 'task:1' }))\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({ key: 'task:3' }))\n    })\n\n    it('should notify when item enters query result set', async () =\u003e {\n      await db.put('task:1', { status: 'pending' })\n      \n      const callback = vi.fn()\n      await db.subscribe({ where: { status: 'done' } }, callback)\n      callback.mockClear()\n      \n      await db.put('task:1', { status: 'done' })\n      \n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'enter',\n        key: 'task:1'\n      }))\n    })\n\n    it('should notify when item exits query result set', async () =\u003e {\n      await db.put('task:1', { status: 'done' })\n      \n      const callback = vi.fn()\n      await db.subscribe({ where: { status: 'done' } }, callback)\n      callback.mockClear()\n      \n      await db.put('task:1', { status: 'pending' })\n      \n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        type: 'exit',\n        key: 'task:1'\n      }))\n    })\n  })\n\n  describe('Subscription Options', () =\u003e {\n    it('should support debouncing updates', async () =\u003e {\n      const callback = vi.fn()\n      await db.subscribe('user:1', { debounce: 100 }, callback)\n      callback.mockClear()\n      \n      // Rapid updates\n      await db.put('user:1', { v: 1 })\n      await db.put('user:1', { v: 2 })\n      await db.put('user:1', { v: 3 })\n      \n      // Only last update should be delivered after debounce\n      await new Promise(r =\u003e setTimeout(r, 150))\n      expect(callback).toHaveBeenCalledTimes(1)\n      expect(callback).toHaveBeenCalledWith(expect.objectContaining({\n        value: { v: 3 }\n      }))\n    })\n\n    it('should support filtering updates', async () =\u003e {\n      const callback = vi.fn()\n      await db.subscribe('user:1', {\n        filter: (prev, next) =\u003e prev?.age !== next?.age\n      }, callback)\n      callback.mockClear()\n      \n      await db.put('user:1', { name: 'Alice', age: 30 })\n      await db.put('user:1', { name: 'Alice Updated', age: 30 }) // age unchanged\n      await db.put('user:1', { name: 'Alice', age: 31 })\n      \n      // Should only receive updates where age changed\n      expect(callback).toHaveBeenCalledTimes(2)\n    })\n  })\n})\n```\n\n### GREEN Phase: Minimal Implementation to Pass Tests\n\n```typescript\n// src/subscriptions/subscription-manager.ts\nexport type ChangeEventType = 'initial' | 'create' | 'update' | 'delete' | 'enter' | 'exit' | 'sync'\n\nexport interface ChangeEvent\u003cT = any\u003e {\n  type: ChangeEventType\n  key: string\n  value: T | undefined\n  previousValue?: T\n  version: number\n  delta?: JsonPatch[]\n  missedChanges?: ChangeEvent\u003cT\u003e[]\n}\n\nexport interface SubscriptionOptions {\n  debounce?: number\n  includeDelta?: boolean\n  filter?: (prev: any, next: any) =\u003e boolean\n  onError?: (error: SubscriptionError) =\u003e void\n}\n\nexport interface Subscription {\n  id: string\n  status: 'connecting' | 'connected' | 'reconnecting' | 'disconnected'\n  unsubscribe(): Promise\u003cvoid\u003e\n  on(event: string, handler: Function): void\n}\n\nexport class SubscriptionManager {\n  private subscriptions: Map\u003cstring, Set\u003cSubscriptionEntry\u003e\u003e = new Map()\n  private querySubscriptions: Set\u003cQuerySubscription\u003e = new Set()\n  private changeBuffer: Map\u003cstring, ChangeEvent[]\u003e = new Map()\n  \n  async subscribe\u003cT\u003e(\n    target: string | QueryPattern,\n    optionsOrCallback: SubscriptionOptions | ChangeCallback\u003cT\u003e,\n    maybeCallback?: ChangeCallback\u003cT\u003e\n  ): Promise\u003cSubscription\u003e {\n    const [options, callback] = this.parseArgs(optionsOrCallback, maybeCallback)\n    \n    const subscription = new SubscriptionEntry(\n      crypto.randomUUID(),\n      target,\n      callback,\n      options\n    )\n    \n    if (typeof target === 'string') {\n      await this.subscribeToKey(target, subscription)\n    } else {\n      await this.subscribeToQuery(target, subscription)\n    }\n    \n    return subscription\n  }\n\n  private async subscribeToKey(key: string, entry: SubscriptionEntry): Promise\u003cvoid\u003e {\n    if (!this.subscriptions.has(key)) {\n      this.subscriptions.set(key, new Set())\n    }\n    this.subscriptions.get(key)!.add(entry)\n    \n    // Emit initial value\n    const { value, version } = await this.db._getWithVersion(key)\n    if (value !== undefined) {\n      entry.emit({\n        type: 'initial',\n        key,\n        value,\n        version\n      })\n    }\n    \n    entry.setStatus('connected')\n  }\n\n  async notifyChange(key: string, event: ChangeEvent): Promise\u003cvoid\u003e {\n    const subscribers = this.subscriptions.get(key)\n    if (!subscribers) return\n    \n    for (const subscriber of subscribers) {\n      if (subscriber.options.filter) {\n        if (!subscriber.options.filter(event.previousValue, event.value)) {\n          continue\n        }\n      }\n      \n      if (subscriber.options.debounce) {\n        subscriber.debounce(event)\n      } else {\n        subscriber.emit(event)\n      }\n    }\n    \n    // Check query subscriptions\n    await this.checkQuerySubscriptions(key, event)\n  }\n\n  private async checkQuerySubscriptions(key: string, event: ChangeEvent): Promise\u003cvoid\u003e {\n    for (const querySub of this.querySubscriptions) {\n      const wasMatch = this.matchesQuery(event.previousValue, querySub.query)\n      const isMatch = this.matchesQuery(event.value, querySub.query)\n      \n      if (!wasMatch \u0026\u0026 isMatch) {\n        querySub.emit({ ...event, type: 'enter' })\n      } else if (wasMatch \u0026\u0026 !isMatch) {\n        querySub.emit({ ...event, type: 'exit' })\n      } else if (wasMatch \u0026\u0026 isMatch) {\n        querySub.emit(event)\n      }\n    }\n  }\n}\n\nclass SubscriptionEntry implements Subscription {\n  id: string\n  status: 'connecting' | 'connected' | 'reconnecting' | 'disconnected' = 'connecting'\n  private eventHandlers: Map\u003cstring, Function[]\u003e = new Map()\n  private debounceTimer?: NodeJS.Timeout\n  private lastEvent?: ChangeEvent\n  private retryCount = 0\n  private maxRetries = 10\n  \n  constructor(\n    id: string,\n    private target: string | QueryPattern,\n    private callback: ChangeCallback,\n    public options: SubscriptionOptions\n  ) {\n    this.id = id\n  }\n\n  emit(event: ChangeEvent): void {\n    this.callback(event)\n  }\n\n  debounce(event: ChangeEvent): void {\n    this.lastEvent = event\n    if (this.debounceTimer) clearTimeout(this.debounceTimer)\n    \n    this.debounceTimer = setTimeout(() =\u003e {\n      if (this.lastEvent) {\n        this.emit(this.lastEvent)\n        this.lastEvent = undefined\n      }\n    }, this.options.debounce)\n  }\n\n  async unsubscribe(): Promise\u003cvoid\u003e {\n    this.status = 'disconnected'\n    // Remove from subscription manager\n  }\n\n  on(event: string, handler: Function): void {\n    if (!this.eventHandlers.has(event)) {\n      this.eventHandlers.set(event, [])\n    }\n    this.eventHandlers.get(event)!.push(handler)\n  }\n\n  setStatus(status: typeof this.status): void {\n    this.status = status\n  }\n\n  async _simulateDisconnect(): Promise\u003cvoid\u003e {\n    this.status = 'reconnecting'\n    this.attemptReconnect()\n  }\n\n  private async attemptReconnect(): Promise\u003cvoid\u003e {\n    const delay = Math.min(1000 * Math.pow(2, this.retryCount), 30000)\n    this.emitEvent('retry', { delay, attempt: this.retryCount })\n    \n    try {\n      await this.reconnect()\n      this.status = 'connected'\n      this.retryCount = 0\n    } catch (e) {\n      this.retryCount++\n      if (this.retryCount \u003e= this.maxRetries) {\n        this.status = 'disconnected'\n        this.options.onError?.({\n          type: 'connection_failed',\n          retries: this.retryCount\n        })\n      } else {\n        setTimeout(() =\u003e this.attemptReconnect(), delay)\n      }\n    }\n  }\n\n  private emitEvent(event: string, data: any): void {\n    const handlers = this.eventHandlers.get(event) || []\n    handlers.forEach(h =\u003e h(data))\n  }\n}\n```\n\n### REFACTOR Phase: Clean Up and Optimize\n\n1. **Add Batching for High-Volume Updates**\n   ```typescript\n   class BatchedSubscriptionManager {\n     private batchInterval = 16 // ~60fps\n     private pendingBatches: Map\u003cstring, ChangeEvent[]\u003e = new Map()\n     \n     queueChange(key: string, event: ChangeEvent): void {\n       if (!this.pendingBatches.has(key)) {\n         this.pendingBatches.set(key, [])\n         this.scheduleBatchFlush()\n       }\n       this.pendingBatches.get(key)!.push(event)\n     }\n     \n     private scheduleBatchFlush(): void {\n       requestAnimationFrame(() =\u003e this.flushBatches())\n     }\n     \n     private flushBatches(): void {\n       for (const [key, events] of this.pendingBatches) {\n         const consolidated = this.consolidateEvents(events)\n         this.notifySubscribers(key, consolidated)\n       }\n       this.pendingBatches.clear()\n     }\n   }\n   ```\n\n2. **Implement Backpressure Handling**\n   ```typescript\n   interface BackpressureOptions {\n     highWaterMark: number\n     lowWaterMark: number\n     strategy: 'drop' | 'buffer' | 'pause'\n   }\n   \n   class BackpressureHandler {\n     private buffer: ChangeEvent[] = []\n     private paused = false\n     \n     handleEvent(event: ChangeEvent, subscriber: SubscriptionEntry): void {\n       if (this.buffer.length \u003e= this.options.highWaterMark) {\n         switch (this.options.strategy) {\n           case 'drop':\n             return // Drop oldest or newest\n           case 'buffer':\n             this.buffer.push(event) // Keep buffering\n             break\n           case 'pause':\n             this.paused = true\n             break\n         }\n       }\n       \n       if (!this.paused) {\n         subscriber.emit(event)\n       }\n     }\n   }\n   ```\n\n3. **Optimize Memory Usage**\n   - Use WeakRef for subscriber callbacks\n   - Implement subscription cleanup on disconnect\n   - Add memory limits for change buffers\n\n4. **Add Compression for Wire Protocol**\n   - Delta compression for frequent updates\n   - Binary encoding for change events\n   - Gzip for large payloads\n\n## Acceptance Criteria\n\n- [ ] All RED phase tests pass\n- [ ] Subscribe/unsubscribe work for keys and queries\n- [ ] Change events (create/update/delete) emit correctly\n- [ ] Reconnection with missed change recovery works\n- [ ] Query subscriptions detect enter/exit correctly\n- [ ] Debouncing and filtering options work\n- [ ] Batching implemented for high-volume updates\n- [ ] Backpressure handling prevents memory issues\n- [ ] Performance: \u003c 1ms latency for local updates\n\n## Dependencies\n\n- Change tracking/versioning system\n- Query engine for query subscriptions\n- Event emitter infrastructure","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:58.293956-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:56:53.190361-06:00","closed_at":"2026-01-21T19:56:53.190361-06:00","close_reason":"Closed"}
{"id":"evodb-kwk","title":"TDD: Fix swallowed cache exceptions","description":"reader/src/cache.ts:77-91 - Silent failures hide legitimate errors. Add error logging, distinguish error types.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:08:52.033012-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:11:35.053923-06:00","closed_at":"2026-01-20T13:11:35.053923-06:00","close_reason":"Closed"}
{"id":"evodb-lay","title":"TypeScript: Address @ts-expect-error comments for unimplemented features","description":"## Summary\n\nFound 16 @ts-expect-error comments in core/src/__tests__/columnar-json.unit.test.ts indicating planned but unimplemented features. These should be tracked and addressed.\n\n## Locations\n\n### core/src/__tests__/columnar-json.unit.test.ts\n\n1. **Line 703** - `extractPath not yet implemented`\n2. **Line 718** - `extractPaths not yet implemented`\n3. **Line 739,742** - `parentVersion not yet on Schema type`\n4. **Line 761** - `Type.Timestamp not yet defined` (NOTE: This appears to be implemented now)\n5. **Line 775** - `Type.Date not yet defined` (NOTE: This appears to be implemented now)\n6. **Line 787,799** - `shred does not yet support options.columns`\n7. **Line 817,819** - `nullBitmapCompressed/nullBitmapCompression not yet on Column`\n8. **Line 826** - `coerceToType not yet exported`\n9. **Line 842,857** - `appendRows not yet implemented`\n10. **Line 877** - `buildPathIndex not yet implemented`\n\n### reader/src/__tests__/cache.unit.test.ts\n\n- **Line 78,83** - Mocking global caches object\n\n## Analysis\n\nSeveral @ts-expect-error comments reference features that may already be implemented:\n- `Type.Timestamp` and `Type.Date` are now in the Type enum (types.ts:186-187)\n- `parentVersion` is now on Schema type (types.ts:275)\n\n## Recommendations\n\n1. **Audit existing @ts-expect-error** - Remove if features are now implemented\n2. **Create feature issues** - For genuinely unimplemented features:\n   - `extractPath` / `extractPaths` - column extraction API\n   - `options.columns` for shred - selective shredding\n   - `nullBitmapCompressed` - compression support\n   - `coerceToType` - type coercion utility\n   - `appendRows` - incremental row addition\n   - `buildPathIndex` - path indexing for fast lookup\n3. **Use // TODO instead** - For features planned but not blocking tests\n\n## References\n\n- TypeScript @ts-expect-error vs @ts-ignore\n- Test-driven development patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:41.337039-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:20:53.793248-06:00","closed_at":"2026-01-21T20:20:53.793248-06:00","close_reason":"Closed"}
{"id":"evodb-ldi","title":"Build and use path index for multi-column operations","description":"## Problem\nextractPath() in shred.ts:492 uses linear search through columns. Called N times for N-column projection.\n\n## TDD Approach\n1. Write benchmark for multi-column projection\n2. Always build path index for operations with \u003e1 column\n3. Use index in extractPath() and related functions\n4. Verify performance improvement\n\n## Expected Impact\n- O(1) instead of O(n) column lookup\n- Significant speedup for wide tables\n\n## Current Code (shred.ts:492-505)\n```typescript\nconst col = columns.find(c =\u003e c.path === path);  // O(n)\n```\n\n## Fix\n```typescript\nconst pathIndex = buildPathIndex(columns);  // Already exists!\nconst col = pathIndex.get(path);  // O(1)\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:29.440805-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:29:41.160277-06:00","closed_at":"2026-01-21T12:29:41.160277-06:00","close_reason":"Closed"}
{"id":"evodb-lgp2","title":"TypeScript: Review Column.values and ColumnStats unknown types","description":"## Summary\n\nSeveral core types use \\`unknown\\` or \\`unknown[]\\` for value storage. While this is intentional for flexibility, it creates type safety gaps when consuming the data.\n\n## Locations\n\n### core/src/types.ts\n\n```typescript\nexport interface Column {\n  path: string;\n  type: Type;\n  nullable: boolean;\n  values: unknown[];   // Raw values before encoding\n  nulls: NullBitmap;\n}\n\nexport interface ColumnStats {\n  min: unknown;\n  max: unknown;\n  nullCount: number;\n  distinctEst: number;\n}\n```\n\n### query/src/types.ts\n\n```typescript\nexport interface ZoneMapColumn {\n  min: unknown;\n  max: unknown;\n  nullCount: number;\n  allNull: boolean;\n  distinctCount?: number;\n}\n```\n\n## Analysis\n\nThe \\`unknown[]\\` type for values is correct because:\n1. Columns can hold any JSON-compatible type\n2. The Type enum indicates the actual runtime type\n3. Encoding/decoding validates type consistency\n\nHowever, consuming code often needs to:\n1. Access min/max for zone map comparisons\n2. Process values for aggregation\n3. Serialize stats to JSON\n\n## Current Issues\n\n1. **No type-safe min/max access**:\n   ```typescript\n   const min = stats.min; // unknown\n   if (typeof min === 'number') { ... } // manual narrowing\n   ```\n\n2. **Generic ZoneMapColumn across types**:\n   Zone maps for string columns have string min/max, for numbers have number min/max, etc.\n\n## Recommendations\n\n### Option 1: Generic ColumnStats (Recommended)\n\n```typescript\nexport interface ColumnStats\u003cT = unknown\u003e {\n  min: T | null;\n  max: T | null;\n  nullCount: number;\n  distinctEst: number;\n}\n\n// Usage\ntype StringColumnStats = ColumnStats\u003cstring\u003e;\ntype NumberColumnStats = ColumnStats\u003cnumber\u003e;\n```\n\n### Option 2: Type-indexed stats\n\n```typescript\nexport type TypedColumnStats = {\n  [Type.String]: { min: string | null; max: string | null; ... };\n  [Type.Int32]: { min: number | null; max: number | null; ... };\n  // etc\n};\n```\n\n### Option 3: Runtime type guards (Current approach, improve)\n\nAdd typed accessors:\n\n```typescript\nexport function getNumericMin(stats: ColumnStats): number | null {\n  if (typeof stats.min === 'number') return stats.min;\n  return null;\n}\n```\n\n## Trade-offs\n\n- Generic stats: More type-safe but more complex\n- Type-indexed: Maximum type safety but verbose\n- Accessors: Backward compatible but runtime overhead\n\n## References\n\n- Type enum definition (types.ts:176-188)\n- encode.ts comment about type guarantees","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:29.173234-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:23:54.121276-06:00","closed_at":"2026-01-21T20:23:54.121276-06:00","close_reason":"Closed"}
{"id":"evodb-lh2l","title":"TDD: Implement true streaming for large result sets","description":"## Overview\n\nImplement true streaming for large query result sets using async iterators, providing memory-efficient processing with backpressure support, progress callbacks, and cancellation capabilities.\n\n## TDD Approach\n\n### RED Phase - Write Failing Tests\n\n```typescript\n// tests/core/streaming/result-stream.test.ts\n\ndescribe('Streaming Large Result Sets', () =\u003e {\n  describe('Async Iterator Pattern', () =\u003e {\n    it('should return async iterable for large queries', async () =\u003e {\n      const stream = await db.query('SELECT * FROM large_table').stream();\n      \n      expect(Symbol.asyncIterator in stream).toBe(true);\n    });\n\n    it('should yield rows one at a time', async () =\u003e {\n      const mockRows = Array.from({ length: 100 }, (_, i) =\u003e ({ id: i }));\n      const stream = createResultStream(mockRows);\n      \n      const received: any[] = [];\n      for await (const row of stream) {\n        received.push(row);\n      }\n      \n      expect(received).toEqual(mockRows);\n    });\n\n    it('should support for-await-of syntax', async () =\u003e {\n      const stream = await db.query('SELECT * FROM users LIMIT 10').stream();\n      \n      let count = 0;\n      for await (const row of stream) {\n        expect(row).toHaveProperty('id');\n        count++;\n      }\n      \n      expect(count).toBe(10);\n    });\n\n    it('should work with spread operator via Array.fromAsync', async () =\u003e {\n      const stream = createResultStream([{ a: 1 }, { a: 2 }, { a: 3 }]);\n      \n      const rows = await Array.fromAsync(stream);\n      \n      expect(rows).toHaveLength(3);\n    });\n  });\n\n  describe('Memory Bounds', () =\u003e {\n    it('should maintain bounded memory for large streams', async () =\u003e {\n      const rowCount = 100000;\n      const stream = createMockLargeStream(rowCount, { rowSizeBytes: 1000 });\n      \n      const memoryBefore = process.memoryUsage().heapUsed;\n      \n      let count = 0;\n      for await (const row of stream) {\n        count++;\n        \n        // Check memory at intervals\n        if (count % 10000 === 0) {\n          const currentMemory = process.memoryUsage().heapUsed;\n          const memoryGrowth = currentMemory - memoryBefore;\n          \n          // Memory should not grow proportionally to rows processed\n          // Allow for some overhead, but not 100MB for 100k rows\n          expect(memoryGrowth).toBeLessThan(50 * 1024 * 1024); // 50MB max\n        }\n      }\n      \n      expect(count).toBe(rowCount);\n    });\n\n    it('should respect buffer size configuration', async () =\u003e {\n      const stream = createResultStream(generateRows(1000), {\n        bufferSize: 10, // Only buffer 10 rows at a time\n      });\n      \n      const bufferSizes: number[] = [];\n      stream.on('bufferStatus', (size) =\u003e bufferSizes.push(size));\n      \n      for await (const _ of stream) {\n        // consume\n      }\n      \n      // Buffer should never exceed configured size\n      expect(Math.max(...bufferSizes)).toBeLessThanOrEqual(10);\n    });\n  });\n\n  describe('Backpressure Handling', () =\u003e {\n    it('should pause fetching when consumer is slow', async () =\u003e {\n      const fetchCalls: number[] = [];\n      const stream = createResultStream(generateRows(100), {\n        bufferSize: 5,\n        onFetch: (count) =\u003e fetchCalls.push(count),\n      });\n      \n      let processed = 0;\n      for await (const row of stream) {\n        // Slow consumer\n        await sleep(10);\n        processed++;\n        \n        if (processed === 20) {\n          // By now, we should see pauses in fetching\n          break;\n        }\n      }\n      \n      // Fetching should have been throttled\n      expect(fetchCalls.length).toBeLessThan(20);\n    });\n\n    it('should resume fetching when buffer drains', async () =\u003e {\n      const events: string[] = [];\n      const stream = createResultStream(generateRows(50), {\n        bufferSize: 5,\n        highWaterMark: 5,\n        lowWaterMark: 2,\n      });\n      \n      stream.on('pause', () =\u003e events.push('pause'));\n      stream.on('resume', () =\u003e events.push('resume'));\n      \n      for await (const _ of stream) {\n        await sleep(5);\n      }\n      \n      // Should see pause/resume cycles\n      expect(events.filter(e =\u003e e === 'pause').length).toBeGreaterThan(0);\n      expect(events.filter(e =\u003e e === 'resume').length).toBeGreaterThan(0);\n    });\n\n    it('should handle backpressure from transform streams', async () =\u003e {\n      const sourceStream = createResultStream(generateRows(100));\n      \n      let transformCalls = 0;\n      const slowTransform = async function*(source: AsyncIterable\u003cany\u003e) {\n        for await (const row of source) {\n          await sleep(50); // Slow transform\n          transformCalls++;\n          yield { ...row, transformed: true };\n        }\n      };\n      \n      const transformed = slowTransform(sourceStream);\n      \n      // Process only first 5\n      let count = 0;\n      for await (const row of transformed) {\n        count++;\n        if (count \u003e= 5) break;\n      }\n      \n      // Transform should have been called roughly 5 times (not 100)\n      expect(transformCalls).toBeLessThanOrEqual(10);\n    });\n  });\n\n  describe('Chunked Delivery', () =\u003e {\n    it('should support chunked batch delivery', async () =\u003e {\n      const stream = createResultStream(generateRows(100), {\n        chunkSize: 10,\n      });\n      \n      const chunks: any[][] = [];\n      for await (const chunk of stream.chunks()) {\n        chunks.push(chunk);\n      }\n      \n      expect(chunks).toHaveLength(10);\n      expect(chunks[0]).toHaveLength(10);\n      expect(chunks.flat()).toHaveLength(100);\n    });\n\n    it('should handle partial final chunk', async () =\u003e {\n      const stream = createResultStream(generateRows(25), {\n        chunkSize: 10,\n      });\n      \n      const chunks: any[][] = [];\n      for await (const chunk of stream.chunks()) {\n        chunks.push(chunk);\n      }\n      \n      expect(chunks).toHaveLength(3);\n      expect(chunks[2]).toHaveLength(5); // Partial chunk\n    });\n\n    it('should support configurable chunk timeout', async () =\u003e {\n      const stream = createResultStream(generateSlowRows(5), {\n        chunkSize: 10,\n        chunkTimeoutMs: 100, // Emit chunk after 100ms even if not full\n      });\n      \n      const startTime = Date.now();\n      const chunks: any[][] = [];\n      \n      for await (const chunk of stream.chunks()) {\n        chunks.push(chunk);\n      }\n      \n      // Should have received partial chunks due to timeout\n      expect(chunks.length).toBeGreaterThan(0);\n    });\n  });\n\n  describe('Progress Callbacks', () =\u003e {\n    it('should report progress during streaming', async () =\u003e {\n      const progressUpdates: { processed: number; total?: number }[] = [];\n      \n      const stream = createResultStream(generateRows(100), {\n        totalCount: 100,\n        onProgress: (progress) =\u003e progressUpdates.push(progress),\n      });\n      \n      for await (const _ of stream) {\n        // consume\n      }\n      \n      expect(progressUpdates.length).toBeGreaterThan(0);\n      expect(progressUpdates[progressUpdates.length - 1]).toEqual({\n        processed: 100,\n        total: 100,\n        percent: 100,\n      });\n    });\n\n    it('should report progress without total count', async () =\u003e {\n      const progressUpdates: any[] = [];\n      \n      const stream = createResultStream(generateRows(50), {\n        onProgress: (progress) =\u003e progressUpdates.push(progress),\n        progressInterval: 10, // Report every 10 rows\n      });\n      \n      for await (const _ of stream) {\n        // consume\n      }\n      \n      expect(progressUpdates.some(p =\u003e p.processed === 10)).toBe(true);\n      expect(progressUpdates.some(p =\u003e p.processed === 50)).toBe(true);\n      expect(progressUpdates[0].percent).toBeUndefined(); // No total\n    });\n\n    it('should include timing information in progress', async () =\u003e {\n      const progressUpdates: any[] = [];\n      \n      const stream = createResultStream(generateSlowRows(20), {\n        onProgress: (progress) =\u003e progressUpdates.push(progress),\n        progressInterval: 5,\n      });\n      \n      for await (const _ of stream) {\n        // consume\n      }\n      \n      const lastProgress = progressUpdates[progressUpdates.length - 1];\n      expect(lastProgress.elapsedMs).toBeGreaterThan(0);\n      expect(lastProgress.rowsPerSecond).toBeDefined();\n    });\n  });\n\n  describe('Cancellation', () =\u003e {\n    it('should support cancellation via AbortSignal', async () =\u003e {\n      const controller = new AbortController();\n      const stream = createResultStream(generateRows(1000), {\n        signal: controller.signal,\n      });\n      \n      let count = 0;\n      try {\n        for await (const _ of stream) {\n          count++;\n          if (count === 50) {\n            controller.abort();\n          }\n        }\n      } catch (error) {\n        expect(error).toBeInstanceOf(AbortError);\n      }\n      \n      expect(count).toBeLessThan(100);\n    });\n\n    it('should clean up resources on cancellation', async () =\u003e {\n      let cleanupCalled = false;\n      const controller = new AbortController();\n      \n      const stream = createResultStream(generateRows(100), {\n        signal: controller.signal,\n        onCleanup: () =\u003e { cleanupCalled = true; },\n      });\n      \n      let count = 0;\n      try {\n        for await (const _ of stream) {\n          count++;\n          if (count === 10) controller.abort();\n        }\n      } catch {\n        // Expected\n      }\n      \n      expect(cleanupCalled).toBe(true);\n    });\n\n    it('should support manual cancellation method', async () =\u003e {\n      const stream = createResultStream(generateRows(100));\n      \n      let count = 0;\n      for await (const _ of stream) {\n        count++;\n        if (count === 25) {\n          stream.cancel('Manual cancellation');\n        }\n      }\n      \n      expect(count).toBeLessThanOrEqual(30); // Some buffered rows may process\n      expect(stream.cancelled).toBe(true);\n      expect(stream.cancelReason).toBe('Manual cancellation');\n    });\n\n    it('should handle timeout cancellation', async () =\u003e {\n      const stream = createResultStream(generateSlowRows(100), {\n        timeoutMs: 500,\n      });\n      \n      const startTime = Date.now();\n      \n      await expect(async () =\u003e {\n        for await (const _ of stream) {\n          // consume slowly\n        }\n      }).rejects.toThrow(/timeout/i);\n      \n      expect(Date.now() - startTime).toBeLessThan(1000);\n    });\n  });\n\n  describe('Error Handling', () =\u003e {\n    it('should propagate errors from source', async () =\u003e {\n      const stream = createResultStream(async function*() {\n        yield { id: 1 };\n        yield { id: 2 };\n        throw new Error('Source error');\n      });\n      \n      const received: any[] = [];\n      \n      await expect(async () =\u003e {\n        for await (const row of stream) {\n          received.push(row);\n        }\n      }).rejects.toThrow('Source error');\n      \n      expect(received).toHaveLength(2);\n    });\n\n    it('should clean up on error', async () =\u003e {\n      let cleanupCalled = false;\n      \n      const stream = createResultStream(async function*() {\n        yield { id: 1 };\n        throw new Error('fail');\n      }, {\n        onCleanup: () =\u003e { cleanupCalled = true; },\n      });\n      \n      try {\n        for await (const _ of stream) {}\n      } catch {\n        // Expected\n      }\n      \n      expect(cleanupCalled).toBe(true);\n    });\n\n    it('should support error recovery with retry', async () =\u003e {\n      let attempts = 0;\n      \n      const stream = createResultStream(async function*() {\n        attempts++;\n        yield { id: 1 };\n        if (attempts \u003c 3) {\n          throw new Error('transient error');\n        }\n        yield { id: 2 };\n      }, {\n        retryOnError: true,\n        maxRetries: 3,\n      });\n      \n      const received: any[] = [];\n      for await (const row of stream) {\n        received.push(row);\n      }\n      \n      expect(received).toHaveLength(2);\n      expect(attempts).toBe(3);\n    });\n  });\n\n  describe('Transformations', () =\u003e {\n    it('should support map transformation', async () =\u003e {\n      const stream = createResultStream([{ x: 1 }, { x: 2 }, { x: 3 }]);\n      \n      const doubled = stream.map(row =\u003e ({ x: row.x * 2 }));\n      \n      const results = await Array.fromAsync(doubled);\n      expect(results).toEqual([{ x: 2 }, { x: 4 }, { x: 6 }]);\n    });\n\n    it('should support filter transformation', async () =\u003e {\n      const stream = createResultStream([{ x: 1 }, { x: 2 }, { x: 3 }, { x: 4 }]);\n      \n      const evens = stream.filter(row =\u003e row.x % 2 === 0);\n      \n      const results = await Array.fromAsync(evens);\n      expect(results).toEqual([{ x: 2 }, { x: 4 }]);\n    });\n\n    it('should support take/skip operations', async () =\u003e {\n      const stream = createResultStream(generateRows(100));\n      \n      const subset = stream.skip(10).take(5);\n      \n      const results = await Array.fromAsync(subset);\n      expect(results).toHaveLength(5);\n      expect(results[0].id).toBe(10);\n    });\n\n    it('should chain transformations lazily', async () =\u003e {\n      let mapCalls = 0;\n      let filterCalls = 0;\n      \n      const stream = createResultStream(generateRows(100))\n        .map(row =\u003e { mapCalls++; return row; })\n        .filter(row =\u003e { filterCalls++; return row.id \u003c 5; });\n      \n      // Nothing executed yet\n      expect(mapCalls).toBe(0);\n      expect(filterCalls).toBe(0);\n      \n      const results = await Array.fromAsync(stream);\n      \n      // Should have short-circuited after finding 5 matching\n      expect(results).toHaveLength(5);\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Streaming\n\n```typescript\n// src/core/streaming/result-stream.ts\n\nexport interface StreamConfig {\n  bufferSize?: number;\n  highWaterMark?: number;\n  lowWaterMark?: number;\n  chunkSize?: number;\n  chunkTimeoutMs?: number;\n  totalCount?: number;\n  onProgress?: (progress: ProgressInfo) =\u003e void;\n  progressInterval?: number;\n  signal?: AbortSignal;\n  timeoutMs?: number;\n  onCleanup?: () =\u003e void;\n  retryOnError?: boolean;\n  maxRetries?: number;\n}\n\nexport interface ProgressInfo {\n  processed: number;\n  total?: number;\n  percent?: number;\n  elapsedMs: number;\n  rowsPerSecond: number;\n}\n\nexport class ResultStream\u003cT\u003e implements AsyncIterable\u003cT\u003e {\n  private buffer: T[] = [];\n  private source: AsyncIterator\u003cT\u003e;\n  private done = false;\n  private paused = false;\n  private _cancelled = false;\n  private _cancelReason?: string;\n  private processedCount = 0;\n  private startTime = Date.now();\n  private events = new EventEmitter();\n\n  constructor(\n    source: AsyncIterable\u003cT\u003e | AsyncIterator\u003cT\u003e | (() =\u003e AsyncIterable\u003cT\u003e),\n    private config: StreamConfig = {}\n  ) {\n    if (typeof source === 'function') {\n      source = source();\n    }\n    this.source = Symbol.asyncIterator in source \n      ? (source as AsyncIterable\u003cT\u003e)[Symbol.asyncIterator]()\n      : source as AsyncIterator\u003cT\u003e;\n      \n    this.setupAbortHandling();\n    this.setupTimeout();\n  }\n\n  get cancelled(): boolean {\n    return this._cancelled;\n  }\n\n  get cancelReason(): string | undefined {\n    return this._cancelReason;\n  }\n\n  private setupAbortHandling(): void {\n    if (this.config.signal) {\n      this.config.signal.addEventListener('abort', () =\u003e {\n        this.cancel('Aborted via signal');\n      });\n    }\n  }\n\n  private setupTimeout(): void {\n    if (this.config.timeoutMs) {\n      setTimeout(() =\u003e {\n        if (!this.done) {\n          this.cancel('Stream timeout');\n        }\n      }, this.config.timeoutMs);\n    }\n  }\n\n  cancel(reason: string): void {\n    this._cancelled = true;\n    this._cancelReason = reason;\n    this.events.emit('cancel', reason);\n  }\n\n  on(event: string, handler: (...args: any[]) =\u003e void): void {\n    this.events.on(event, handler);\n  }\n\n  async *[Symbol.asyncIterator](): AsyncIterator\u003cT\u003e {\n    try {\n      while (!this.done \u0026\u0026 !this._cancelled) {\n        const result = await this.source.next();\n        \n        if (result.done) {\n          this.done = true;\n          break;\n        }\n\n        this.processedCount++;\n        this.reportProgress();\n        \n        yield result.value;\n      }\n\n      if (this._cancelled) {\n        throw new AbortError(this._cancelReason || 'Stream cancelled');\n      }\n    } finally {\n      this.cleanup();\n    }\n  }\n\n  async *chunks(): AsyncIterable\u003cT[]\u003e {\n    const chunkSize = this.config.chunkSize || 100;\n    let currentChunk: T[] = [];\n    let chunkStartTime = Date.now();\n\n    for await (const item of this) {\n      currentChunk.push(item);\n      \n      const shouldEmit = \n        currentChunk.length \u003e= chunkSize ||\n        (this.config.chunkTimeoutMs \u0026\u0026 \n         Date.now() - chunkStartTime \u003e= this.config.chunkTimeoutMs);\n      \n      if (shouldEmit) {\n        yield currentChunk;\n        currentChunk = [];\n        chunkStartTime = Date.now();\n      }\n    }\n\n    if (currentChunk.length \u003e 0) {\n      yield currentChunk;\n    }\n  }\n\n  private reportProgress(): void {\n    if (!this.config.onProgress) return;\n    \n    const interval = this.config.progressInterval || 100;\n    if (this.processedCount % interval !== 0) return;\n\n    const elapsed = Date.now() - this.startTime;\n    const progress: ProgressInfo = {\n      processed: this.processedCount,\n      elapsedMs: elapsed,\n      rowsPerSecond: this.processedCount / (elapsed / 1000),\n    };\n\n    if (this.config.totalCount) {\n      progress.total = this.config.totalCount;\n      progress.percent = Math.round(\n        (this.processedCount / this.config.totalCount) * 100\n      );\n    }\n\n    this.config.onProgress(progress);\n  }\n\n  private cleanup(): void {\n    this.config.onCleanup?.();\n    this.events.emit('cleanup');\n  }\n\n  // Transformation methods\n  map\u003cU\u003e(fn: (item: T) =\u003e U): ResultStream\u003cU\u003e {\n    const self = this;\n    return new ResultStream\u003cU\u003e(async function*() {\n      for await (const item of self) {\n        yield fn(item);\n      }\n    }, this.config);\n  }\n\n  filter(predicate: (item: T) =\u003e boolean): ResultStream\u003cT\u003e {\n    const self = this;\n    return new ResultStream\u003cT\u003e(async function*() {\n      for await (const item of self) {\n        if (predicate(item)) {\n          yield item;\n        }\n      }\n    }, this.config);\n  }\n\n  take(count: number): ResultStream\u003cT\u003e {\n    const self = this;\n    return new ResultStream\u003cT\u003e(async function*() {\n      let taken = 0;\n      for await (const item of self) {\n        if (taken \u003e= count) break;\n        yield item;\n        taken++;\n      }\n    }, this.config);\n  }\n\n  skip(count: number): ResultStream\u003cT\u003e {\n    const self = this;\n    return new ResultStream\u003cT\u003e(async function*() {\n      let skipped = 0;\n      for await (const item of self) {\n        if (skipped \u003c count) {\n          skipped++;\n          continue;\n        }\n        yield item;\n      }\n    }, this.config);\n  }\n\n  async collect(): Promise\u003cT[]\u003e {\n    const results: T[] = [];\n    for await (const item of this) {\n      results.push(item);\n    }\n    return results;\n  }\n}\n\nexport class AbortError extends Error {\n  constructor(message: string) {\n    super(message);\n    this.name = 'AbortError';\n  }\n}\n\nexport function createResultStream\u003cT\u003e(\n  source: T[] | AsyncIterable\u003cT\u003e | (() =\u003e AsyncIterable\u003cT\u003e),\n  config?: StreamConfig\n): ResultStream\u003cT\u003e {\n  if (Array.isArray(source)) {\n    return new ResultStream(async function*() {\n      for (const item of source) {\n        yield item;\n      }\n    }, config);\n  }\n  return new ResultStream(source, config);\n}\n```\n\n### REFACTOR Phase - Optimize and Enhance\n\n1. **Add true backpressure**: Implement proper pause/resume with buffer management\n2. **Add streaming to/from network**: Support HTTP streaming and WebSocket delivery\n3. **Add memory monitoring**: Automatic buffer adjustment based on memory pressure\n4. **Add metrics**: Track throughput, latency percentiles, buffer utilization\n\n```typescript\n// Enhanced with backpressure and metrics\n\nexport class BackpressureStream\u003cT\u003e extends ResultStream\u003cT\u003e {\n  private highWaterMark: number;\n  private lowWaterMark: number;\n  private isPaused = false;\n  private pendingReads: Array\u003c() =\u003e void\u003e = [];\n\n  constructor(source: AsyncIterable\u003cT\u003e, config: StreamConfig = {}) {\n    super(source, config);\n    this.highWaterMark = config.highWaterMark || 16;\n    this.lowWaterMark = config.lowWaterMark || 4;\n  }\n\n  private checkBackpressure(): void {\n    if (this.buffer.length \u003e= this.highWaterMark \u0026\u0026 !this.isPaused) {\n      this.isPaused = true;\n      this.events.emit('pause');\n    } else if (this.buffer.length \u003c= this.lowWaterMark \u0026\u0026 this.isPaused) {\n      this.isPaused = false;\n      this.events.emit('resume');\n      this.pendingReads.forEach(resolve =\u003e resolve());\n      this.pendingReads = [];\n    }\n  }\n\n  async waitForBuffer(): Promise\u003cvoid\u003e {\n    if (!this.isPaused) return;\n    \n    return new Promise(resolve =\u003e {\n      this.pendingReads.push(resolve);\n    });\n  }\n}\n\n// Streaming metrics\nexport interface StreamMetrics {\n  totalRows: number;\n  bytesProcessed: number;\n  startTime: number;\n  endTime?: number;\n  averageRowsPerSecond: number;\n  peakBufferSize: number;\n  pauseCount: number;\n  errorCount: number;\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Async iterator pattern works with for-await-of\n- [ ] Memory stays bounded for large result sets\n- [ ] Buffer size is configurable and respected\n- [ ] Backpressure pauses fetching when consumer is slow\n- [ ] Backpressure resumes when buffer drains\n- [ ] Chunked delivery groups rows correctly\n- [ ] Progress callbacks report accurate information\n- [ ] AbortSignal cancellation works\n- [ ] Manual cancellation works\n- [ ] Timeout cancellation works\n- [ ] Resources are cleaned up on cancellation/error\n- [ ] Error propagation works correctly\n- [ ] map/filter/take/skip transformations work\n- [ ] Transformations are lazy (don't process until iterated)\n- [ ] Partial final chunk is delivered correctly","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:39:09.179805-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:59:05.583706-06:00","closed_at":"2026-01-21T20:59:05.583706-06:00","close_reason":"Closed"}
{"id":"evodb-lje5","title":"WRITE: Create step-by-step interactive tutorials","description":"## Action: WRITE (Create from scratch)\n\nCreate step-by-step interactive tutorials that guide users through building complete applications with EvoDB.\n\n## Tutorial Locations\n```\ndocs/tutorials/\n├── 01-first-query/           # 5-minute quick start\n├── 02-todo-app/              # Classic TODO application\n├── 03-realtime-chat/         # Real-time messaging\n└── 04-vector-search-rag/     # AI/RAG implementation\n```\n\n---\n\n## Tutorial 1: First Query in 5 Minutes\n**Goal**: Get users from zero to running their first query as fast as possible\n\n### Structure\n```\ndocs/tutorials/01-first-query/\n├── README.md           # Tutorial content\n├── start/              # Starting point (empty project)\n└── complete/           # Finished code for reference\n```\n\n### Steps\n1. **Install EvoDB** (30 seconds)\n   ```bash\n   npm init -y\n   npm install @evodb/core\n   ```\n\n2. **Create Database** (1 minute)\n   ```typescript\n   import { createDatabase } from '@evodb/core';\n   \n   const db = await createDatabase({\n     name: 'quickstart',\n     schema: {\n       messages: {\n         id: 'string',\n         text: 'string',\n         createdAt: 'number'\n       }\n     }\n   });\n   ```\n\n3. **Insert Data** (1 minute)\n4. **Query Data** (1 minute)\n5. **Update \u0026 Delete** (1.5 minutes)\n\n### Learning Outcomes\n- EvoDB installation\n- Schema definition\n- Basic CRUD operations\n\n---\n\n## Tutorial 2: Building a TODO App\n**Goal**: Build a complete, functional TODO application\n\n### Structure\n```\ndocs/tutorials/02-todo-app/\n├── README.md\n├── steps/\n│   ├── 01-setup.md\n│   ├── 02-schema.md\n│   ├── 03-crud.md\n│   ├── 04-filtering.md\n│   └── 05-persistence.md\n├── start/\n└── complete/\n```\n\n### Steps\n1. **Project Setup**\n   - Initialize project\n   - Configure TypeScript\n   - Install dependencies\n\n2. **Define Schema**\n   ```typescript\n   const schema = {\n     todos: {\n       id: 'string',\n       title: 'string',\n       completed: 'boolean',\n       priority: 'string', // 'low' | 'medium' | 'high'\n       dueDate: 'number?',\n       tags: 'string[]',\n       createdAt: 'number',\n       updatedAt: 'number'\n     }\n   };\n   ```\n\n3. **Implement CRUD**\n   - Create todos\n   - List all todos\n   - Update todo status\n   - Delete todos\n\n4. **Add Filtering**\n   - Filter by completion status\n   - Filter by priority\n   - Search by title\n   - Sort by date\n\n5. **Add Persistence**\n   - Configure storage adapter\n   - Handle offline scenarios\n   - Implement sync\n\n### Learning Outcomes\n- Schema design patterns\n- Complex queries and filtering\n- Data persistence strategies\n\n---\n\n## Tutorial 3: Real-time Chat\n**Goal**: Build a real-time chat application with presence\n\n### Structure\n```\ndocs/tutorials/03-realtime-chat/\n├── README.md\n├── steps/\n│   ├── 01-setup.md\n│   ├── 02-messages.md\n│   ├── 03-subscriptions.md\n│   ├── 04-presence.md\n│   └── 05-rooms.md\n├── start/\n└── complete/\n```\n\n### Steps\n1. **Project Setup**\n   - Server and client setup\n   - WebSocket configuration\n\n2. **Message Schema**\n   ```typescript\n   const schema = {\n     messages: {\n       id: 'string',\n       roomId: 'string',\n       userId: 'string',\n       content: 'string',\n       timestamp: 'number'\n     },\n     rooms: {\n       id: 'string',\n       name: 'string',\n       members: 'string[]'\n     }\n   };\n   ```\n\n3. **Real-time Subscriptions**\n   - Subscribe to new messages\n   - Live message updates\n   - Optimistic UI updates\n\n4. **Presence System**\n   - User online/offline status\n   - Typing indicators\n   - Last seen timestamps\n\n5. **Chat Rooms**\n   - Create/join rooms\n   - Room-scoped messages\n   - Member management\n\n### Learning Outcomes\n- Real-time subscriptions\n- Presence patterns\n- Multi-room architecture\n\n---\n\n## Tutorial 4: Vector Search for RAG\n**Goal**: Build a RAG (Retrieval Augmented Generation) system\n\n### Structure\n```\ndocs/tutorials/04-vector-search-rag/\n├── README.md\n├── steps/\n│   ├── 01-setup.md\n│   ├── 02-embeddings.md\n│   ├── 03-storage.md\n│   ├── 04-search.md\n│   └── 05-rag-pipeline.md\n├── start/\n└── complete/\n```\n\n### Steps\n1. **Project Setup**\n   - Install EvoDB with vector support\n   - Configure OpenAI (or alternative) for embeddings\n\n2. **Understanding Embeddings**\n   - What are embeddings?\n   - Generating embeddings from text\n   - Embedding dimensions and models\n\n3. **Vector Schema \u0026 Storage**\n   ```typescript\n   const schema = {\n     documents: {\n       id: 'string',\n       title: 'string',\n       content: 'string',\n       embedding: 'vector[1536]', // OpenAI ada-002 dimension\n       metadata: 'object'\n     }\n   };\n   ```\n\n4. **Similarity Search**\n   - Cosine similarity queries\n   - k-nearest neighbors\n   - Hybrid search (vector + filters)\n\n5. **RAG Pipeline**\n   ```typescript\n   async function ragQuery(question: string) {\n     // 1. Generate embedding for question\n     const questionEmbedding = await generateEmbedding(question);\n     \n     // 2. Find relevant documents\n     const relevantDocs = await db.documents.vectorSearch({\n       vector: questionEmbedding,\n       topK: 5\n     });\n     \n     // 3. Generate response with context\n     const response = await generateWithContext(question, relevantDocs);\n     \n     return response;\n   }\n   ```\n\n### Learning Outcomes\n- Vector embeddings fundamentals\n- Semantic search implementation\n- RAG architecture patterns\n- Hybrid search strategies\n\n---\n\n## Tutorial Requirements\n\n### Each Tutorial Must Have\n1. **Clear Prerequisites** - What users need before starting\n2. **Estimated Time** - Realistic completion time\n3. **Step-by-Step Instructions** - No skipped steps\n4. **Code Checkpoints** - Complete code at each step\n5. **Explanation Boxes** - Why, not just how\n6. **Common Errors** - What can go wrong and how to fix\n7. **Next Steps** - Where to go after completion\n\n### Interactive Elements\n- Copy-paste ready code blocks\n- \"Try it\" challenges at each step\n- Quiz questions to verify understanding\n- Links to related documentation\n\n## Acceptance Criteria\n- [ ] All 4 tutorials created with complete content\n- [ ] Each tutorial has start/ and complete/ directories\n- [ ] All code examples tested and working\n- [ ] Step-by-step instructions are clear and complete\n- [ ] Estimated times are accurate\n- [ ] Common errors documented with solutions\n- [ ] Learning outcomes clearly defined\n- [ ] Tutorials progress in difficulty appropriately","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:51:23.134427-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:07:21.747847-06:00","closed_at":"2026-01-21T20:07:21.747847-06:00","close_reason":"Closed"}
{"id":"evodb-llh2","title":"EvoDB memory limit exceeded with 100MB datasets","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-22T09:37:05.065132-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T09:37:05.065132-06:00"}
{"id":"evodb-lln","title":"TDD: Fix 4 skipped critical tests","description":"Re-enable: time-travel queries, snapshot restore, auto compaction, concurrent writes with optimistic locking.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:30.324005-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.40034-06:00","closed_at":"2026-01-20T16:23:39.40034-06:00","close_reason":"Closed","external_ref":"gh-51"}
{"id":"evodb-lpv","title":"P0 Critical: Foundation \u0026 Safety","description":"Epic for P0 critical issues blocking MVP: missing tests, facade class, mock data removal, and type safety fixes. These must be resolved before any release.","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:27:30.503365-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:05:24.199243-06:00","closed_at":"2026-01-20T11:05:24.199243-06:00","close_reason":"Closed"}
{"id":"evodb-lpv.1","title":"TDD: Add tests for snippets-chain package","description":"snippets-chain has 9 source files with 0 tests. Critical gap - complex orchestration logic (ChainBuilder, ChainExecutor) completely untested.\n\nFiles needing tests:\n- chain-builder.ts (16KB) - fluent API with validation\n- executor.ts (19KB) - parallel execution, timeouts, resource tracking\n- patterns/ - map-reduce, pipeline, scatter-gather\n\nWrite tests using TDD red-green-refactor approach.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:14.835501-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:46:10.787287-06:00","closed_at":"2026-01-20T10:46:10.787287-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-lpv.1","depends_on_id":"evodb-lpv","type":"parent-child","created_at":"2026-01-20T10:28:14.836131-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-lpv.2","title":"Create EvoDB facade class matching README API","description":"The README shows an API that doesn't exist:\n- new EvoDB({ mode, storage })\n- db.insert('users', {...})\n- db.query('users').where(...).select(...)\n- db.schema.lock()\n- db.schema.relate()\n- db.schema.enforce()\n\nCreate the actual EvoDB class in @evodb/core that wraps existing primitives and provides this high-level API.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:18.567637-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:06:37.645461-06:00","closed_at":"2026-01-20T11:06:37.645461-06:00","close_reason":"Closed","external_ref":"gh-21","dependencies":[{"issue_id":"evodb-lpv.2","depends_on_id":"evodb-lpv","type":"parent-child","created_at":"2026-01-20T10:28:18.568201-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-lpv.3","title":"Remove mock data from query engine","description":"Query engine (query/src/engine.ts lines 1131-1355) contains heavy mock data initialization with initializeMockData() generating fake users, orders, events.\n\nThe parseColumnarData method acknowledges 'In a real implementation, this would parse the columnar format' and returns mock data.\n\nRemove all mock data and connect to actual R2 columnar blocks.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:22.008557-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:57:23.555924-06:00","closed_at":"2026-01-20T10:57:23.555924-06:00","close_reason":"Removed mock data from QueryEngine by implementing TableDataSource abstraction pattern. MockDataSource now contains all test data, while R2DataSource provides the integration point for production R2 bucket access.","dependencies":[{"issue_id":"evodb-lpv.3","depends_on_id":"evodb-lpv","type":"parent-child","created_at":"2026-01-20T10:28:22.009159-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-lpv.4","title":"Fix BigInt to Number precision loss in snippets-lance","description":"In snippets-lance/src/cached-lance-reader.ts, BigInt values are converted to Number for file offsets, causing precision loss for large files (\u003e2^53 bytes).\n\nThis can cause data corruption or incorrect reads for large vector indices.\n\nFix by keeping BigInt throughout the read path or using safe conversion with bounds checking.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:25.006065-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:51:21.434359-06:00","closed_at":"2026-01-20T10:51:21.434359-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-lpv.4","depends_on_id":"evodb-lpv","type":"parent-child","created_at":"2026-01-20T10:28:25.006739-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-lpv.5","title":"Fix potential SQL injection in query engine","description":"In query/src/engine.ts, if column names come from user input, there's potential for SQL-like injection attacks.\n\nReview all places where column names are used in query construction and add proper validation/escaping.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:28:28.891416-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:43:42.971491-06:00","closed_at":"2026-01-20T10:43:42.971491-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-lpv.5","depends_on_id":"evodb-lpv","type":"parent-child","created_at":"2026-01-20T10:28:28.892002-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-lqqa","title":"TDD: Expand property-based testing coverage","description":"## Problem\nThe property-based tests in `property-based.unit.test.ts` are excellent but limited to encoding roundtrips and basic query operations. More components would benefit from property-based testing.\n\n## Coverage Gap\n- No property tests for schema inference edge cases\n- No property tests for partition pruning logic\n- No property tests for merge operations\n- No property tests for WAL entry serialization\n\n## Acceptance Criteria\n- [ ] Add property tests for schema inference with arbitrary JSON shapes\n- [ ] Add property tests for partition pruning: filter matches iff data matches\n- [ ] Add property tests for column merge: order independence\n- [ ] Add property tests for WAL roundtrip with arbitrary payloads\n- [ ] Add property tests for zone map: prune decision matches actual data\n\n## TDD Approach\nUsing fast-check:\n1. Define arbitraries for each domain type\n2. Write properties that must always hold\n3. Use shrinking to find minimal failing cases\n4. Target edge cases with custom generators","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:07.494509-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:11:39.087236-06:00","closed_at":"2026-01-21T20:11:39.087236-06:00","close_reason":"Closed","labels":["core","property-based","tdd","testing"]}
{"id":"evodb-m47u","title":"TypeScript: Good discriminated union patterns - Document and expand","description":"## Summary\n\nThe codebase has good examples of discriminated unions that should be documented and potentially expanded.\n\n## Good Patterns Found\n\n### 1. RPC Message Types (rpc/src/types.ts)\n\nExcellent discriminated union with type guards:\n\n```typescript\nexport type RpcMessageType =\n  | 'cdc_batch'\n  | 'ack'\n  | 'nack'\n  | 'heartbeat'\n  | 'connect'\n  | 'disconnect'\n  | 'flush_request'\n  | 'status';\n\nexport interface RpcMessage {\n  type: RpcMessageType;\n  timestamp: number;\n  correlationId?: string;\n}\n\nexport interface CDCBatchMessage extends RpcMessage {\n  type: 'cdc_batch';\n  entries: WalEntry[];\n  // ...\n}\n\n// Type guards\nexport function isCDCBatchMessage(msg: RpcMessage): msg is CDCBatchMessage {\n  return msg.type === 'cdc_batch';\n}\n```\n\n### 2. Plan Operators (query/src/types.ts)\n\nClean discriminated union for query plan operators:\n\n```typescript\nexport type PlanOperator =\n  | ScanOperator\n  | FilterOperator\n  | ProjectOperator\n  | AggregateOperator\n  | SortOperator\n  | LimitOperator\n  | MergeOperator;\n\nexport interface ScanOperator extends BaseOperator {\n  type: 'scan';\n  partitions: PartitionInfo[];\n  columns: string[];\n}\n\nexport interface FilterOperator extends BaseOperator {\n  type: 'filter';\n  input: PlanOperator;\n  predicates: Predicate[];\n}\n```\n\n### 3. Partition Transforms (lakehouse/src/types.ts)\n\nGood use for partition transforms:\n\n```typescript\nexport type PartitionTransform =\n  | { type: 'identity' }\n  | { type: 'year' }\n  | { type: 'month' }\n  | { type: 'day' }\n  | { type: 'hour' }\n  | { type: 'bucket'; numBuckets: number }\n  | { type: 'truncate'; width: number };\n```\n\n### 4. Filter Types (lakehouse/src/types.ts)\n\nClean partition and column filters:\n\n```typescript\nexport type PartitionFilter =\n  | { eq: string | number }\n  | { in: (string | number)[] }\n  | { gte: number; lte?: number }\n  | { lte: number; gte?: number }\n  | { between: [number, number] };\n```\n\n## Areas to Expand\n\n### 1. Type Enum (core/src/types.ts)\n\nCurrently a const enum, could have discriminated data types:\n\n```typescript\n// Current\nexport const enum Type {\n  Null = 0,\n  Bool = 1,\n  Int32 = 2,\n  // ...\n}\n\n// Could add typed value wrappers\nexport type TypedValue =\n  | { type: Type.Null; value: null }\n  | { type: Type.Bool; value: boolean }\n  | { type: Type.Int32; value: number }\n  | { type: Type.Int64; value: bigint }\n  | { type: Type.Float64; value: number }\n  | { type: Type.String; value: string }\n  // ...\n```\n\n### 2. Storage Errors\n\nCurrently uses error codes, could use discriminated union:\n\n```typescript\nexport type StorageFailure =\n  | { type: 'not_found'; key: string }\n  | { type: 'permission_denied'; key: string; operation: string }\n  | { type: 'quota_exceeded'; limit: number; used: number }\n  | { type: 'network'; message: string; retryable: boolean };\n```\n\n### 3. Query Results\n\nCould distinguish between success states:\n\n```typescript\nexport type QueryExecutionResult\u003cT\u003e =\n  | { status: 'success'; rows: T[]; stats: QueryStats }\n  | { status: 'timeout'; partialRows: T[]; stats: QueryStats }\n  | { status: 'cancelled'; reason: string }\n  | { status: 'error'; error: QueryError };\n```\n\n## Recommendations\n\n1. **Document patterns in ARCHITECTURE.md**\n2. **Add type guards for all discriminated unions**\n3. **Use exhaustive switch with assertNever**\n4. **Consider expanding Type enum to TypedValue**","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:21.640231-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:45.357425-06:00","closed_at":"2026-01-21T20:18:45.357425-06:00","close_reason":"Closed"}
{"id":"evodb-mcw","title":"TDD: Fix BigInt initialization logic in buffer.ts","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:14.328174-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:02:14.174688-06:00","closed_at":"2026-01-20T14:02:14.174688-06:00","close_reason":"Closed"}
{"id":"evodb-mdl","title":"Extract tracing.ts to optional @evodb/observability plugin","description":"## Problem\ntracing.ts (807 lines, ~15KB) implements full OpenTelemetry support but most embedded DB users don't need it.\n\n## TDD Approach\n1. Write tests for tracing in @evodb/observability\n2. Move tracing.ts to observability package\n3. Create noop tracing interface in core\n4. Verify core bundle reduction\n\n## Expected Impact\n- ~15KB reduction from core bundle\n\n## Implementation\n- @evodb/observability/tracing entry point\n- Core keeps minimal TracingContext interface\n- Plugin provides full OTEL implementation","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:03:21.857486-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:29:01.64951-06:00","closed_at":"2026-01-21T12:29:01.64951-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-mdl","depends_on_id":"evodb-cpx","type":"blocks","created_at":"2026-01-21T12:05:47.197457-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-mer","title":"Implement Consistent Error Hierarchy Across Packages","description":"## Current State\nEach package has its own error classes with inconsistent patterns:\n\n**@evodb/core**:\n- EvoDBError, QueryError, ValidationError, TimeoutError, StorageError\n- Uses error codes (string enum) - well designed\n\n**@evodb/query**:\n- MemoryLimitExceededError, SubrequestBudgetExceededError\n- Not extending EvoDBError\n\n**@evodb/rpc**:\n- LakehouseRpcError, ConnectionError, BufferOverflowError, FlushError, ProtocolError\n- Own error hierarchy, not extending EvoDBError\n\n**@evodb/writer**:\n- BlockIndexLimitError, WriterError, BufferOverflowError\n- BufferOverflowError duplicated from rpc\n\n**@evodb/lakehouse**:\n- ManifestError, SchemaError, JsonParseError, VersionMismatchError\n- Own hierarchy\n\n## Proposed Improvement\n1. All package errors should extend EvoDBError\n2. Use consistent error code patterns across packages:\n   - QUERY_*, STORAGE_*, RPC_*, WRITER_*, MANIFEST_*, SCHEMA_*\n3. Remove duplicate error classes (BufferOverflowError)\n4. Export all errors from @evodb/core/errors for single import point\n\n## Migration Path\n1. Update error classes to extend EvoDBError\n2. Add error codes to all errors\n3. Export errors from packages AND re-export from core\n4. Document error hierarchy in ARCHITECTURE.md\n\n## Benefits\n- Catch all EvoDB errors with single catch block\n- Programmatic error handling via codes\n- Better error messages and debugging","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:55.568969-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:31:48.250943-06:00","closed_at":"2026-01-21T20:31:48.250943-06:00","close_reason":"Closed","labels":["architecture","consistency","errors"]}
{"id":"evodb-mgwo","title":"TDD: Implement declarative query caching","description":"## Overview\n\nImplement a declarative query caching layer that automatically caches query results, handles cache invalidation, and supports advanced caching strategies like stale-while-revalidate.\n\n## TDD Red-Green-Refactor Cycle\n\n### RED Phase: Write Failing Tests First\n\n```typescript\n// tests/cache/query-cache.test.ts\nimport { describe, it, expect, beforeEach, afterEach, vi } from 'vitest'\nimport { EvoDB } from '../src/evodb'\n\ndescribe('Query Caching Layer', () =\u003e {\n  let db: EvoDB\n\n  beforeEach(async () =\u003e {\n    db = new EvoDB({ \n      name: 'test-cache',\n      cache: { enabled: true }\n    })\n    await db.clear()\n    \n    // Seed test data\n    await db.put('user:1', { name: 'Alice', age: 30 })\n    await db.put('user:2', { name: 'Bob', age: 25 })\n    await db.put('user:3', { name: 'Charlie', age: 35 })\n  })\n\n  afterEach(async () =\u003e {\n    await db.close()\n  })\n\n  describe('Cache Hit/Miss', () =\u003e {\n    it('should cache query results on first execution', async () =\u003e {\n      const query = { where: { age: { $gt: 25 } } }\n      \n      const result1 = await db.query(query)\n      expect(result1.fromCache).toBe(false)\n      \n      const result2 = await db.query(query)\n      expect(result2.fromCache).toBe(true)\n      expect(result2.data).toEqual(result1.data)\n    })\n\n    it('should return cache miss for new queries', async () =\u003e {\n      const query1 = { where: { name: 'Alice' } }\n      const query2 = { where: { name: 'Bob' } }\n      \n      await db.query(query1) // Cache query1\n      const result = await db.query(query2)\n      \n      expect(result.fromCache).toBe(false)\n    })\n\n    it('should use cache key based on query normalization', async () =\u003e {\n      // Same query, different key order\n      const query1 = { where: { age: 30, name: 'Alice' } }\n      const query2 = { where: { name: 'Alice', age: 30 } }\n      \n      await db.query(query1)\n      const result = await db.query(query2)\n      \n      expect(result.fromCache).toBe(true)\n    })\n\n    it('should track cache statistics', async () =\u003e {\n      const query = { where: { age: 30 } }\n      \n      await db.query(query) // miss\n      await db.query(query) // hit\n      await db.query(query) // hit\n      \n      const stats = db.cache.getStats()\n      expect(stats.hits).toBe(2)\n      expect(stats.misses).toBe(1)\n      expect(stats.hitRate).toBeCloseTo(0.667, 2)\n    })\n\n    it('should support cache bypass', async () =\u003e {\n      const query = { where: { age: 30 } }\n      \n      await db.query(query) // Cache it\n      \n      const dbSpy = vi.spyOn(db, '_executeQuery')\n      const result = await db.query(query, { cache: false })\n      \n      expect(result.fromCache).toBe(false)\n      expect(dbSpy).toHaveBeenCalled()\n    })\n  })\n\n  describe('Cache Invalidation', () =\u003e {\n    it('should invalidate cache when data changes', async () =\u003e {\n      const query = { where: { age: { $gt: 25 } } }\n      \n      await db.query(query) // Cache it\n      \n      await db.put('user:1', { name: 'Alice', age: 20 }) // Age change\n      \n      const result = await db.query(query)\n      expect(result.fromCache).toBe(false)\n      expect(result.data.length).toBe(2) // Charlie and Bob only now\n    })\n\n    it('should only invalidate affected queries', async () =\u003e {\n      const query1 = { where: { name: 'Alice' } }\n      const query2 = { where: { name: 'Bob' } }\n      \n      await db.query(query1)\n      await db.query(query2)\n      \n      await db.put('user:1', { name: 'Alice Updated', age: 30 })\n      \n      // query1 should be invalidated, query2 should still be cached\n      const result1 = await db.query(query1)\n      const result2 = await db.query(query2)\n      \n      expect(result1.fromCache).toBe(false)\n      expect(result2.fromCache).toBe(true)\n    })\n\n    it('should support manual cache invalidation', async () =\u003e {\n      const query = { where: { age: 30 } }\n      \n      await db.query(query)\n      db.cache.invalidate(query)\n      \n      const result = await db.query(query)\n      expect(result.fromCache).toBe(false)\n    })\n\n    it('should invalidate all caches on clear()', async () =\u003e {\n      await db.query({ where: { age: 30 } })\n      await db.query({ where: { name: 'Bob' } })\n      \n      db.cache.clear()\n      \n      const result1 = await db.query({ where: { age: 30 } })\n      const result2 = await db.query({ where: { name: 'Bob' } })\n      \n      expect(result1.fromCache).toBe(false)\n      expect(result2.fromCache).toBe(false)\n    })\n\n    it('should support tag-based invalidation', async () =\u003e {\n      await db.query({ where: { age: 30 } }, { tags: ['users', 'age-query'] })\n      await db.query({ where: { name: 'Bob' } }, { tags: ['users', 'name-query'] })\n      \n      db.cache.invalidateByTag('age-query')\n      \n      const result1 = await db.query({ where: { age: 30 } })\n      const result2 = await db.query({ where: { name: 'Bob' } })\n      \n      expect(result1.fromCache).toBe(false)\n      expect(result2.fromCache).toBe(true) // Different tag\n    })\n  })\n\n  describe('TTL (Time-To-Live)', () =\u003e {\n    it('should expire cache entries after TTL', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      await db.query(query, { ttl: 1000 }) // 1 second TTL\n      \n      vi.advanceTimersByTime(500)\n      const result1 = await db.query(query)\n      expect(result1.fromCache).toBe(true)\n      \n      vi.advanceTimersByTime(600) // Total 1100ms\n      const result2 = await db.query(query)\n      expect(result2.fromCache).toBe(false)\n      \n      vi.useRealTimers()\n    })\n\n    it('should support per-query TTL override', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      \n      await db.query(query, { ttl: 5000 })\n      vi.advanceTimersByTime(3000)\n      \n      // Override with shorter TTL\n      await db.query(query, { ttl: 1000, refreshTTL: true })\n      vi.advanceTimersByTime(1500)\n      \n      const result = await db.query(query)\n      expect(result.fromCache).toBe(false)\n      \n      vi.useRealTimers()\n    })\n\n    it('should support global default TTL', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const dbWithTTL = new EvoDB({\n        name: 'test-ttl',\n        cache: { enabled: true, defaultTTL: 2000 }\n      })\n      \n      await dbWithTTL.query({ where: { age: 30 } })\n      \n      vi.advanceTimersByTime(2500)\n      const result = await dbWithTTL.query({ where: { age: 30 } })\n      expect(result.fromCache).toBe(false)\n      \n      vi.useRealTimers()\n      await dbWithTTL.close()\n    })\n\n    it('should return remaining TTL with cached result', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      await db.query(query, { ttl: 5000 })\n      \n      vi.advanceTimersByTime(2000)\n      const result = await db.query(query)\n      \n      expect(result.remainingTTL).toBeGreaterThan(2900)\n      expect(result.remainingTTL).toBeLessThanOrEqual(3000)\n      \n      vi.useRealTimers()\n    })\n  })\n\n  describe('LRU Eviction', () =\u003e {\n    it('should evict least recently used entries when cache is full', async () =\u003e {\n      const dbWithLimit = new EvoDB({\n        name: 'test-lru',\n        cache: { enabled: true, maxEntries: 3 }\n      })\n      \n      await dbWithLimit.query({ where: { name: 'Alice' } }) // Entry 1\n      await dbWithLimit.query({ where: { name: 'Bob' } })   // Entry 2\n      await dbWithLimit.query({ where: { name: 'Charlie' } }) // Entry 3\n      \n      // Access entry 1 to make it recently used\n      await dbWithLimit.query({ where: { name: 'Alice' } })\n      \n      // Add new entry, should evict entry 2 (LRU)\n      await dbWithLimit.query({ where: { age: 30 } }) // Entry 4\n      \n      const result1 = await dbWithLimit.query({ where: { name: 'Alice' } })\n      const result2 = await dbWithLimit.query({ where: { name: 'Bob' } })\n      \n      expect(result1.fromCache).toBe(true)  // Still cached\n      expect(result2.fromCache).toBe(false) // Was evicted\n      \n      await dbWithLimit.close()\n    })\n\n    it('should respect maxSize limit for cache memory', async () =\u003e {\n      const dbWithSize = new EvoDB({\n        name: 'test-size',\n        cache: { enabled: true, maxSize: '1MB' }\n      })\n      \n      // Add large entries\n      for (let i = 0; i \u003c 100; i++) {\n        await dbWithSize.put(`large:${i}`, { data: 'x'.repeat(20000) })\n        await dbWithSize.query({ where: { key: `large:${i}` } })\n      }\n      \n      const stats = dbWithSize.cache.getStats()\n      expect(stats.currentSize).toBeLessThanOrEqual(1024 * 1024)\n      \n      await dbWithSize.close()\n    })\n  })\n\n  describe('Stale-While-Revalidate', () =\u003e {\n    it('should return stale data while revalidating', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      await db.query(query, { \n        ttl: 1000,\n        staleWhileRevalidate: 5000 \n      })\n      \n      vi.advanceTimersByTime(1500) // Past TTL but within SWR window\n      \n      await db.put('user:4', { name: 'Diana', age: 30 }) // Data changes\n      \n      const result = await db.query(query)\n      \n      expect(result.fromCache).toBe(true)\n      expect(result.stale).toBe(true)\n      expect(result.data.length).toBe(1) // Old data\n      \n      // Wait for revalidation\n      await vi.advanceTimersByTimeAsync(100)\n      \n      const freshResult = await db.query(query)\n      expect(freshResult.stale).toBe(false)\n      expect(freshResult.data.length).toBe(2) // Includes Diana\n      \n      vi.useRealTimers()\n    })\n\n    it('should not return stale data after SWR window expires', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      await db.query(query, {\n        ttl: 1000,\n        staleWhileRevalidate: 2000\n      })\n      \n      vi.advanceTimersByTime(4000) // Past both TTL and SWR\n      \n      const result = await db.query(query)\n      expect(result.fromCache).toBe(false)\n      expect(result.stale).toBeUndefined()\n      \n      vi.useRealTimers()\n    })\n\n    it('should support stale-if-error fallback', async () =\u003e {\n      const query = { where: { age: 30 } }\n      await db.query(query, { staleIfError: true })\n      \n      // Simulate database error\n      vi.spyOn(db, '_executeQuery').mockRejectedValueOnce(new Error('DB Error'))\n      \n      const result = await db.query(query)\n      \n      expect(result.fromCache).toBe(true)\n      expect(result.stale).toBe(true)\n      expect(result.error).toBeDefined()\n    })\n  })\n\n  describe('Cache Warming', () =\u003e {\n    it('should support cache warming on startup', async () =\u003e {\n      const warmupQueries = [\n        { where: { age: 30 } },\n        { where: { name: 'Bob' } }\n      ]\n      \n      await db.cache.warmup(warmupQueries)\n      \n      const result1 = await db.query(warmupQueries[0])\n      const result2 = await db.query(warmupQueries[1])\n      \n      expect(result1.fromCache).toBe(true)\n      expect(result2.fromCache).toBe(true)\n    })\n\n    it('should support background refresh', async () =\u003e {\n      vi.useFakeTimers()\n      \n      const query = { where: { age: 30 } }\n      await db.query(query, { \n        backgroundRefresh: true,\n        refreshInterval: 5000 \n      })\n      \n      const dbSpy = vi.spyOn(db, '_executeQuery')\n      \n      vi.advanceTimersByTime(5500)\n      \n      expect(dbSpy).toHaveBeenCalled()\n      \n      vi.useRealTimers()\n    })\n  })\n\n  describe('Declarative Cache Configuration', () =\u003e {\n    it('should support declarative cache rules', async () =\u003e {\n      db.cache.addRule({\n        pattern: { prefix: 'user:' },\n        ttl: 60000,\n        staleWhileRevalidate: 300000,\n        tags: ['users']\n      })\n      \n      const result = await db.query({ prefix: 'user:' })\n      \n      // Cache entry should have rule-based config\n      const cacheEntry = db.cache.getEntry(result.cacheKey)\n      expect(cacheEntry.ttl).toBe(60000)\n      expect(cacheEntry.tags).toContain('users')\n    })\n\n    it('should match most specific rule', async () =\u003e {\n      db.cache.addRule({\n        pattern: { prefix: 'user:' },\n        ttl: 60000\n      })\n      \n      db.cache.addRule({\n        pattern: { where: { role: 'admin' } },\n        ttl: 10000 // Admins cached for shorter time\n      })\n      \n      const adminQuery = { prefix: 'user:', where: { role: 'admin' } }\n      await db.query(adminQuery)\n      \n      const cacheEntry = db.cache.getEntry(db.cache.getCacheKey(adminQuery))\n      expect(cacheEntry.ttl).toBe(10000)\n    })\n  })\n})\n```\n\n### GREEN Phase: Minimal Implementation to Pass Tests\n\n```typescript\n// src/cache/cache-manager.ts\nexport interface CacheOptions {\n  enabled: boolean\n  maxEntries?: number\n  maxSize?: string | number\n  defaultTTL?: number\n}\n\nexport interface QueryCacheEntry {\n  key: string\n  data: any[]\n  createdAt: number\n  ttl: number\n  staleWhileRevalidate?: number\n  tags: string[]\n  size: number\n}\n\nexport interface CacheStats {\n  hits: number\n  misses: number\n  hitRate: number\n  entries: number\n  currentSize: number\n}\n\nexport interface CacheRule {\n  pattern: QueryPattern\n  ttl: number\n  staleWhileRevalidate?: number\n  tags?: string[]\n  priority?: number\n}\n\nexport class CacheManager {\n  private cache: Map\u003cstring, QueryCacheEntry\u003e = new Map()\n  private accessOrder: string[] = [] // For LRU\n  private rules: CacheRule[] = []\n  private stats = { hits: 0, misses: 0 }\n  \n  constructor(private options: CacheOptions) {}\n\n  getCacheKey(query: any): string {\n    // Normalize and hash query for consistent keys\n    const normalized = this.normalizeQuery(query)\n    return this.hash(JSON.stringify(normalized))\n  }\n\n  private normalizeQuery(query: any): any {\n    if (typeof query !== 'object' || query === null) return query\n    \n    // Sort object keys recursively\n    const sorted: any = {}\n    for (const key of Object.keys(query).sort()) {\n      sorted[key] = this.normalizeQuery(query[key])\n    }\n    return sorted\n  }\n\n  async get(key: string): Promise\u003cQueryCacheEntry | null\u003e {\n    const entry = this.cache.get(key)\n    if (!entry) {\n      this.stats.misses++\n      return null\n    }\n\n    const now = Date.now()\n    const age = now - entry.createdAt\n    \n    // Check if expired and past SWR window\n    if (age \u003e entry.ttl + (entry.staleWhileRevalidate || 0)) {\n      this.cache.delete(key)\n      this.stats.misses++\n      return null\n    }\n\n    // Update LRU order\n    this.updateAccessOrder(key)\n    this.stats.hits++\n\n    // Mark as stale if past TTL\n    if (age \u003e entry.ttl) {\n      return { ...entry, stale: true }\n    }\n\n    return entry\n  }\n\n  async set(\n    key: string, \n    data: any[], \n    options: { ttl?: number; tags?: string[]; staleWhileRevalidate?: number } = {}\n  ): Promise\u003cvoid\u003e {\n    const entry: QueryCacheEntry = {\n      key,\n      data,\n      createdAt: Date.now(),\n      ttl: options.ttl || this.options.defaultTTL || Infinity,\n      staleWhileRevalidate: options.staleWhileRevalidate,\n      tags: options.tags || [],\n      size: this.calculateSize(data)\n    }\n\n    // Check capacity and evict if needed\n    await this.ensureCapacity(entry.size)\n\n    this.cache.set(key, entry)\n    this.updateAccessOrder(key)\n  }\n\n  invalidate(query: any): void {\n    const key = this.getCacheKey(query)\n    this.cache.delete(key)\n  }\n\n  invalidateByTag(tag: string): void {\n    for (const [key, entry] of this.cache) {\n      if (entry.tags.includes(tag)) {\n        this.cache.delete(key)\n      }\n    }\n  }\n\n  invalidateByKeys(affectedKeys: string[]): void {\n    // Invalidate queries that might be affected by data changes\n    for (const [cacheKey, entry] of this.cache) {\n      if (this.queryAffectedByKeys(entry, affectedKeys)) {\n        this.cache.delete(cacheKey)\n      }\n    }\n  }\n\n  clear(): void {\n    this.cache.clear()\n    this.accessOrder = []\n    this.stats = { hits: 0, misses: 0 }\n  }\n\n  getStats(): CacheStats {\n    const total = this.stats.hits + this.stats.misses\n    return {\n      ...this.stats,\n      hitRate: total \u003e 0 ? this.stats.hits / total : 0,\n      entries: this.cache.size,\n      currentSize: this.calculateTotalSize()\n    }\n  }\n\n  addRule(rule: CacheRule): void {\n    this.rules.push(rule)\n    this.rules.sort((a, b) =\u003e (b.priority || 0) - (a.priority || 0))\n  }\n\n  async warmup(queries: any[]): Promise\u003cvoid\u003e {\n    await Promise.all(\n      queries.map(async (query) =\u003e {\n        const key = this.getCacheKey(query)\n        if (!this.cache.has(key)) {\n          const data = await this.db._executeQuery(query)\n          await this.set(key, data)\n        }\n      })\n    )\n  }\n\n  private async ensureCapacity(newEntrySize: number): Promise\u003cvoid\u003e {\n    // Check entry count limit\n    if (this.options.maxEntries \u0026\u0026 this.cache.size \u003e= this.options.maxEntries) {\n      this.evictLRU()\n    }\n\n    // Check size limit\n    if (this.options.maxSize) {\n      const maxBytes = this.parseSize(this.options.maxSize)\n      while (this.calculateTotalSize() + newEntrySize \u003e maxBytes) {\n        this.evictLRU()\n      }\n    }\n  }\n\n  private evictLRU(): void {\n    if (this.accessOrder.length === 0) return\n    const lruKey = this.accessOrder.shift()!\n    this.cache.delete(lruKey)\n  }\n\n  private updateAccessOrder(key: string): void {\n    const index = this.accessOrder.indexOf(key)\n    if (index \u003e -1) {\n      this.accessOrder.splice(index, 1)\n    }\n    this.accessOrder.push(key)\n  }\n\n  private calculateSize(data: any): number {\n    return JSON.stringify(data).length * 2 // Rough estimate\n  }\n\n  private calculateTotalSize(): number {\n    let total = 0\n    for (const entry of this.cache.values()) {\n      total += entry.size\n    }\n    return total\n  }\n\n  private parseSize(size: string | number): number {\n    if (typeof size === 'number') return size\n    const match = size.match(/^(\\d+)(KB|MB|GB)?$/i)\n    if (!match) return 0\n    const [, num, unit] = match\n    const multipliers: Record\u003cstring, number\u003e = {\n      KB: 1024, MB: 1024 * 1024, GB: 1024 * 1024 * 1024\n    }\n    return parseInt(num) * (multipliers[unit?.toUpperCase()] || 1)\n  }\n}\n```\n\n### REFACTOR Phase: Clean Up and Optimize\n\n1. **Optimize Cache Key Generation**\n   ```typescript\n   // Use faster hashing algorithm\n   import { xxhash64 } from 'xxhash-wasm'\n   \n   getCacheKey(query: any): string {\n     const normalized = this.normalizeQuery(query)\n     return xxhash64(JSON.stringify(normalized))\n   }\n   ```\n\n2. **Implement Smarter Invalidation**\n   ```typescript\n   // Track which keys each cached query depends on\n   class DependencyTracker {\n     private queryDependencies: Map\u003cstring, Set\u003cstring\u003e\u003e = new Map()\n     private keyToQueries: Map\u003cstring, Set\u003cstring\u003e\u003e = new Map()\n     \n     trackDependency(queryKey: string, dataKey: string): void\n     invalidateByDataKey(dataKey: string): string[] // Returns invalidated queries\n   }\n   ```\n\n3. **Add Cache Warming Strategies**\n   - Predictive warming based on access patterns\n   - Background refresh for frequently accessed queries\n   - Priority-based warming\n\n4. **Memory Optimization**\n   - Compress cached data\n   - Use ArrayBuffer for binary data\n   - Implement tiered caching (memory -\u003e disk)\n\n## Acceptance Criteria\n\n- [ ] All RED phase tests pass\n- [ ] Cache hit/miss tracking accurate\n- [ ] TTL expiration works correctly\n- [ ] LRU eviction respects maxEntries and maxSize\n- [ ] Invalidation (manual, automatic, tag-based) works\n- [ ] Stale-while-revalidate returns stale data and refreshes\n- [ ] Cache warming and background refresh work\n- [ ] Declarative cache rules apply correctly\n- [ ] Performance: cache lookup \u003c 1ms\n\n## Dependencies\n\n- Query normalization for consistent cache keys\n- Change tracking for smart invalidation\n- Memory management utilities","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:38:13.237469-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:53:05.546009-06:00","closed_at":"2026-01-21T19:53:05.546009-06:00","close_reason":"Closed"}
{"id":"evodb-micf","title":"WRITE: Document npm publishing strategy and process","description":"## Overview\n\nCreate comprehensive documentation for npm publishing strategy, process, and configuration for the EvoDb monorepo.\n\n## Content Requirements\n\n### 1. Package Versioning Approach\n- Semantic Versioning (SemVer) policy\n- When to bump major/minor/patch\n- Pre-release version naming (alpha, beta, rc)\n- Version synchronization across packages\n- Independent vs fixed versioning decision\n\n### 2. Monorepo Publishing Workflow\n- Package dependency graph\n- Build order requirements\n- Publishing sequence\n- Handling internal dependencies\n- npm workspaces configuration\n- pnpm/yarn workspace considerations\n\n### 3. Pre-release Channels\n- **Alpha channel**: Unstable, breaking changes expected\n  - Version format: `x.y.z-alpha.n`\n  - npm tag: `alpha`\n  \n- **Beta channel**: Feature complete, bug fixes only\n  - Version format: `x.y.z-beta.n`\n  - npm tag: `beta`\n  \n- **RC channel**: Release candidate\n  - Version format: `x.y.z-rc.n`\n  - npm tag: `next`\n\n- **Stable channel**: Production ready\n  - npm tag: `latest`\n\n### 4. Changesets Configuration\n- Changesets setup and configuration\n- Writing changeset entries\n- Automated version bumping\n- Changelog generation\n- GitHub Actions integration\n- PR workflow with changesets bot\n\n```yaml\n# .changeset/config.json structure\n{\n  \"$schema\": \"https://unpkg.com/@changesets/config/schema.json\",\n  \"changelog\": \"@changesets/changelog-github\",\n  \"commit\": false,\n  \"access\": \"public\",\n  \"baseBranch\": \"main\"\n}\n```\n\n### 5. CI/CD Integration\n- GitHub Actions publish workflow\n- npm token management\n- Provenance and attestation\n- Automated release notes\n- Canary releases from PRs\n\n### 6. Publishing Checklist\n- Pre-publish verification steps\n- npm pack inspection\n- Dry-run publishing\n- Post-publish verification\n- Rollback procedures\n\n## File Location\n\n`docs/NPM_PUBLISHING.md`\n\n## Acceptance Criteria\n\n- [ ] Complete step-by-step publishing guide\n- [ ] Working GitHub Actions workflow examples\n- [ ] Changesets configuration documented\n- [ ] Security best practices included\n- [ ] Troubleshooting common issues section","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:49.272019-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:31:07.021091-06:00","closed_at":"2026-01-21T19:31:07.021091-06:00","close_reason":"Closed"}
{"id":"evodb-mjk","title":"TDD: Standardize tsconfig across packages","description":"e2e package missing strict flags. Create tsconfig.base.json and have all packages extend it.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:44.437296-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:23:39.340916-06:00","closed_at":"2026-01-20T16:23:39.340916-06:00","close_reason":"Closed","external_ref":"gh-52"}
{"id":"evodb-n58v","title":"TDD: Implement backup and disaster recovery","description":"## Overview\nImplement comprehensive backup and disaster recovery capabilities including snapshot backups, incremental backups, point-in-time recovery, and encrypted backups.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. Snapshot Backup Tests\n```typescript\ndescribe('Snapshot Backup', () =\u003e {\n  beforeEach(async () =\u003e {\n    await db.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)');\n    await db.execute('INSERT INTO users (id, name) VALUES (1, ?), (2, ?)', ['Alice', 'Bob']);\n  });\n\n  it('should create a full snapshot backup', async () =\u003e {\n    const backupPath = await db.backup({\n      type: 'snapshot',\n      path: '/backups/db-snapshot.bak',\n    });\n    \n    expect(await fileExists(backupPath)).toBe(true);\n  });\n\n  it('should restore from snapshot backup', async () =\u003e {\n    const backupPath = await db.backup({\n      type: 'snapshot',\n      path: '/backups/db-snapshot.bak',\n    });\n    \n    // Modify data\n    await db.execute('DELETE FROM users');\n    await db.execute('INSERT INTO users (id, name) VALUES (3, ?)', ['Charlie']);\n    \n    // Restore\n    await db.restore({ path: backupPath });\n    \n    const users = await db.query('SELECT * FROM users ORDER BY id');\n    expect(users).toEqual([\n      { id: 1, name: 'Alice' },\n      { id: 2, name: 'Bob' },\n    ]);\n  });\n\n  it('should include all tables in snapshot', async () =\u003e {\n    await db.execute('CREATE TABLE posts (id INTEGER PRIMARY KEY, title TEXT)');\n    await db.execute('INSERT INTO posts (id, title) VALUES (1, ?)', ['Hello']);\n    \n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/full.bak' });\n    \n    const newDb = await openDatabase(':memory:');\n    await newDb.restore({ path: backupPath });\n    \n    const tables = await newDb.query(\"SELECT name FROM sqlite_master WHERE type='table'\");\n    expect(tables.map(t =\u003e t.name)).toContain('users');\n    expect(tables.map(t =\u003e t.name)).toContain('posts');\n  });\n\n  it('should create backup without blocking writes', async () =\u003e {\n    const backupPromise = db.backup({\n      type: 'snapshot',\n      path: '/backups/concurrent.bak',\n    });\n    \n    // Concurrent writes should succeed\n    await db.execute('INSERT INTO users (id, name) VALUES (3, ?)', ['Charlie']);\n    \n    await backupPromise;\n    \n    const users = await db.query('SELECT * FROM users');\n    expect(users).toHaveLength(3);\n  });\n\n  it('should report backup progress', async () =\u003e {\n    // Create large dataset\n    for (let i = 0; i \u003c 10000; i++) {\n      await db.execute('INSERT INTO users (name) VALUES (?)', [`User ${i}`]);\n    }\n    \n    const progressUpdates: number[] = [];\n    await db.backup({\n      type: 'snapshot',\n      path: '/backups/large.bak',\n      onProgress: (progress) =\u003e progressUpdates.push(progress.percentage),\n    });\n    \n    expect(progressUpdates.length).toBeGreaterThan(1);\n    expect(progressUpdates[progressUpdates.length - 1]).toBe(100);\n  });\n\n  it('should include backup metadata', async () =\u003e {\n    const backupPath = await db.backup({\n      type: 'snapshot',\n      path: '/backups/meta.bak',\n    });\n    \n    const meta = await db.getBackupMetadata(backupPath);\n    expect(meta).toMatchObject({\n      type: 'snapshot',\n      createdAt: expect.any(Number),\n      databaseSize: expect.any(Number),\n      tables: expect.arrayContaining(['users']),\n      version: expect.any(String),\n    });\n  });\n\n  it('should verify backup integrity', async () =\u003e {\n    const backupPath = await db.backup({\n      type: 'snapshot',\n      path: '/backups/verify.bak',\n    });\n    \n    const result = await db.verifyBackup(backupPath);\n    expect(result.valid).toBe(true);\n    expect(result.checksum).toBeDefined();\n  });\n\n  it('should detect corrupted backup', async () =\u003e {\n    const backupPath = await db.backup({\n      type: 'snapshot',\n      path: '/backups/corrupt.bak',\n    });\n    \n    // Corrupt the file\n    await corruptFile(backupPath);\n    \n    const result = await db.verifyBackup(backupPath);\n    expect(result.valid).toBe(false);\n    expect(result.error).toContain('checksum');\n  });\n});\n```\n\n#### 2. Incremental Backup Tests\n```typescript\ndescribe('Incremental Backup', () =\u003e {\n  beforeEach(async () =\u003e {\n    await db.execute('CREATE TABLE events (id INTEGER PRIMARY KEY, data TEXT, created_at TEXT)');\n  });\n\n  it('should create base backup for incremental chain', async () =\u003e {\n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 1']);\n    \n    const backupPath = await db.backup({\n      type: 'incremental',\n      path: '/backups/inc',\n      base: true,\n    });\n    \n    expect(backupPath).toContain('inc-base');\n  });\n\n  it('should create incremental backup with changes only', async () =\u003e {\n    // Base backup\n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 1']);\n    await db.backup({ type: 'incremental', path: '/backups/inc', base: true });\n    \n    // More changes\n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 2']);\n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 3']);\n    \n    // Incremental backup\n    const incPath = await db.backup({\n      type: 'incremental',\n      path: '/backups/inc',\n    });\n    \n    // Incremental should be smaller than full\n    const baseSize = await fileSize('/backups/inc-base.bak');\n    const incSize = await fileSize(incPath);\n    expect(incSize).toBeLessThan(baseSize);\n  });\n\n  it('should restore from incremental chain', async () =\u003e {\n    // Create base + 2 incrementals\n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 1']);\n    await db.backup({ type: 'incremental', path: '/backups/inc', base: true });\n    \n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 2']);\n    await db.backup({ type: 'incremental', path: '/backups/inc' });\n    \n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 3']);\n    await db.backup({ type: 'incremental', path: '/backups/inc' });\n    \n    // Restore\n    const newDb = await openDatabase(':memory:');\n    await newDb.restore({ path: '/backups/inc', type: 'incremental' });\n    \n    const events = await newDb.query('SELECT * FROM events');\n    expect(events).toHaveLength(3);\n  });\n\n  it('should track WAL-based changes for incremental', async () =\u003e {\n    await db.backup({ type: 'incremental', path: '/backups/wal', base: true });\n    \n    // Record WAL position\n    const walStart = await db.getWALPosition();\n    \n    await db.execute('INSERT INTO events (data) VALUES (?)', ['New Event']);\n    \n    const walEnd = await db.getWALPosition();\n    expect(walEnd).toBeGreaterThan(walStart);\n    \n    // Incremental should include WAL range\n    const backup = await db.backup({ type: 'incremental', path: '/backups/wal' });\n    const meta = await db.getBackupMetadata(backup);\n    expect(meta.walRange).toEqual({ start: walStart, end: walEnd });\n  });\n\n  it('should support point-in-time recovery', async () =\u003e {\n    await db.backup({ type: 'incremental', path: '/backups/pitr', base: true });\n    \n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 1']);\n    const time1 = Date.now();\n    await db.backup({ type: 'incremental', path: '/backups/pitr' });\n    \n    await new Promise(r =\u003e setTimeout(r, 100));\n    \n    await db.execute('INSERT INTO events (data) VALUES (?)', ['Event 2']);\n    await db.backup({ type: 'incremental', path: '/backups/pitr' });\n    \n    // Restore to time1 (before Event 2)\n    const newDb = await openDatabase(':memory:');\n    await newDb.restore({\n      path: '/backups/pitr',\n      type: 'incremental',\n      pointInTime: time1,\n    });\n    \n    const events = await newDb.query('SELECT * FROM events');\n    expect(events).toHaveLength(1);\n    expect(events[0].data).toBe('Event 1');\n  });\n\n  it('should automatically manage retention', async () =\u003e {\n    const config = {\n      type: 'incremental' as const,\n      path: '/backups/retention',\n      retention: {\n        keepDaily: 7,\n        keepWeekly: 4,\n        keepMonthly: 3,\n      },\n    };\n    \n    // Create many backups\n    for (let i = 0; i \u003c 30; i++) {\n      await db.execute('INSERT INTO events (data) VALUES (?)', [`Event ${i}`]);\n      await db.backup(config);\n    }\n    \n    // Should have pruned old backups\n    const backups = await db.listBackups('/backups/retention');\n    expect(backups.length).toBeLessThan(30);\n  });\n});\n```\n\n#### 3. Restore Tests\n```typescript\ndescribe('Restore Operations', () =\u003e {\n  it('should restore to new database', async () =\u003e {\n    await db.execute('CREATE TABLE data (id INTEGER, value TEXT)');\n    await db.execute('INSERT INTO data VALUES (1, ?)', ['Original']);\n    \n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/restore.bak' });\n    \n    const newDb = await openDatabase('/data/restored.db');\n    await newDb.restore({ path: backupPath });\n    \n    const data = await newDb.query('SELECT * FROM data');\n    expect(data).toEqual([{ id: 1, value: 'Original' }]);\n  });\n\n  it('should restore to existing database (overwrite)', async () =\u003e {\n    await db.execute('CREATE TABLE data (id INTEGER, value TEXT)');\n    await db.execute('INSERT INTO data VALUES (1, ?)', ['Original']);\n    \n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/overwrite.bak' });\n    \n    // Modify original\n    await db.execute('UPDATE data SET value = ?', ['Modified']);\n    \n    // Restore (overwrite)\n    await db.restore({ path: backupPath, overwrite: true });\n    \n    const data = await db.query('SELECT * FROM data');\n    expect(data[0].value).toBe('Original');\n  });\n\n  it('should restore specific tables only', async () =\u003e {\n    await db.execute('CREATE TABLE table1 (id INTEGER)');\n    await db.execute('CREATE TABLE table2 (id INTEGER)');\n    await db.execute('INSERT INTO table1 VALUES (1)');\n    await db.execute('INSERT INTO table2 VALUES (2)');\n    \n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/partial.bak' });\n    \n    // Modify both tables\n    await db.execute('DELETE FROM table1');\n    await db.execute('DELETE FROM table2');\n    \n    // Restore only table1\n    await db.restore({\n      path: backupPath,\n      tables: ['table1'],\n    });\n    \n    expect(await db.query('SELECT * FROM table1')).toHaveLength(1);\n    expect(await db.query('SELECT * FROM table2')).toHaveLength(0);\n  });\n\n  it('should validate restore before applying', async () =\u003e {\n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/validate.bak' });\n    \n    const validation = await db.validateRestore({\n      path: backupPath,\n      target: '/data/new.db',\n    });\n    \n    expect(validation).toMatchObject({\n      valid: true,\n      sourceTables: expect.any(Array),\n      targetExists: false,\n      estimatedDuration: expect.any(Number),\n    });\n  });\n\n  it('should support dry-run restore', async () =\u003e {\n    const backupPath = await db.backup({ type: 'snapshot', path: '/backups/dryrun.bak' });\n    \n    await db.execute('INSERT INTO users VALUES (999, ?)');\n    \n    const result = await db.restore({\n      path: backupPath,\n      dryRun: true,\n    });\n    \n    // Database should not be modified\n    const users = await db.query('SELECT * FROM users WHERE id = 999');\n    expect(users).toHaveLength(1);\n    \n    // Should report what would happen\n    expect(result.dryRun).toBe(true);\n    expect(result.changes).toContain('DELETE users WHERE id = 999');\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/core/src/backup/backup-manager.ts\nexport class BackupManager {\n  constructor(private db: EvoDb) {}\n\n  async backup(options: BackupOptions): Promise\u003cstring\u003e {\n    const { type, path, onProgress } = options;\n    \n    if (type === 'snapshot') {\n      return this.createSnapshotBackup(path, onProgress);\n    } else {\n      return this.createIncrementalBackup(path, options.base);\n    }\n  }\n\n  private async createSnapshotBackup(\n    basePath: string, \n    onProgress?: ProgressCallback\n  ): Promise\u003cstring\u003e {\n    const backupPath = `${basePath}-${Date.now()}.bak`;\n    \n    // Use SQLite backup API (non-blocking)\n    const backup = await this.db.raw.backup(backupPath);\n    \n    const totalPages = backup.pageCount;\n    let pagesRemaining = totalPages;\n    \n    while (pagesRemaining \u003e 0) {\n      // Copy 100 pages at a time\n      pagesRemaining = await backup.step(100);\n      \n      const progress = ((totalPages - pagesRemaining) / totalPages) * 100;\n      onProgress?.({ percentage: progress, pagesRemaining });\n      \n      // Allow other operations between steps\n      await new Promise(r =\u003e setImmediate(r));\n    }\n    \n    await backup.finish();\n    \n    // Add metadata and checksum\n    await this.writeMetadata(backupPath, {\n      type: 'snapshot',\n      createdAt: Date.now(),\n      databaseSize: await this.getDatabaseSize(),\n      tables: await this.getTableList(),\n      version: this.db.version,\n      checksum: await this.calculateChecksum(backupPath),\n    });\n    \n    return backupPath;\n  }\n\n  private async createIncrementalBackup(basePath: string, isBase: boolean): Promise\u003cstring\u003e {\n    if (isBase) {\n      // Create full base backup\n      const backupPath = `${basePath}-base.bak`;\n      await this.createSnapshotBackup(backupPath);\n      \n      // Record WAL position\n      await this.recordWALPosition(basePath, await this.db.getWALPosition());\n      \n      return backupPath;\n    }\n    \n    // Incremental: copy WAL changes since last backup\n    const lastPosition = await this.getLastWALPosition(basePath);\n    const currentPosition = await this.db.getWALPosition();\n    \n    const incPath = `${basePath}-inc-${Date.now()}.bak`;\n    await this.copyWALRange(incPath, lastPosition, currentPosition);\n    \n    await this.recordWALPosition(basePath, currentPosition);\n    \n    return incPath;\n  }\n\n  async restore(options: RestoreOptions): Promise\u003cRestoreResult\u003e {\n    const { path, type, tables, overwrite, dryRun, pointInTime } = options;\n    \n    if (dryRun) {\n      return this.simulateRestore(options);\n    }\n    \n    if (type === 'incremental') {\n      return this.restoreIncremental(path, pointInTime);\n    }\n    \n    // Snapshot restore\n    const metadata = await this.getBackupMetadata(path);\n    \n    // Verify backup integrity\n    const verification = await this.verifyBackup(path);\n    if (!verification.valid) {\n      throw new Error(`Backup verification failed: ${verification.error}`);\n    }\n    \n    if (tables) {\n      return this.restorePartial(path, tables);\n    }\n    \n    // Full restore\n    await this.db.close();\n    await copyFile(path, this.db.path);\n    await this.db.reopen();\n    \n    return { success: true, tablesRestored: metadata.tables };\n  }\n\n  private async restoreIncremental(basePath: string, pointInTime?: number): Promise\u003cRestoreResult\u003e {\n    // Find all backups in chain\n    const backups = await this.findIncrementalChain(basePath);\n    \n    // Restore base\n    const baseBackup = backups.find(b =\u003e b.isBase);\n    await this.db.raw.restore(baseBackup!.path);\n    \n    // Apply incrementals up to pointInTime\n    for (const inc of backups.filter(b =\u003e !b.isBase)) {\n      if (pointInTime \u0026\u0026 inc.timestamp \u003e pointInTime) break;\n      await this.applyWAL(inc.path);\n    }\n    \n    return { success: true, pointInTime };\n  }\n\n  async verifyBackup(path: string): Promise\u003cVerificationResult\u003e {\n    const metadata = await this.getBackupMetadata(path);\n    const calculatedChecksum = await this.calculateChecksum(path);\n    \n    if (calculatedChecksum !== metadata.checksum) {\n      return { valid: false, error: 'Checksum mismatch - backup may be corrupted' };\n    }\n    \n    // Try to open backup\n    try {\n      const testDb = await openDatabase(path, { readonly: true });\n      await testDb.query('SELECT 1');\n      await testDb.close();\n    } catch (e) {\n      return { valid: false, error: `Database integrity check failed: ${e}` };\n    }\n    \n    return { valid: true, checksum: calculatedChecksum };\n  }\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. Encryption Support\n```typescript\n// packages/core/src/backup/encrypted-backup.ts\nexport class EncryptedBackupManager extends BackupManager {\n  private cipher: BackupCipher;\n\n  constructor(db: EvoDb, options: EncryptionOptions) {\n    super(db);\n    this.cipher = new BackupCipher(options);\n  }\n\n  async backup(options: BackupOptions \u0026 { encrypt?: boolean }): Promise\u003cstring\u003e {\n    const rawPath = await super.backup(options);\n    \n    if (!options.encrypt) return rawPath;\n    \n    const encryptedPath = `${rawPath}.enc`;\n    await this.cipher.encryptFile(rawPath, encryptedPath);\n    await unlink(rawPath);\n    \n    return encryptedPath;\n  }\n\n  async restore(options: RestoreOptions \u0026 { decrypt?: boolean; key?: string }): Promise\u003cRestoreResult\u003e {\n    if (!options.decrypt) return super.restore(options);\n    \n    // Decrypt to temp file\n    const tempPath = `/tmp/backup-${Date.now()}.dec`;\n    await this.cipher.decryptFile(options.path, tempPath, options.key);\n    \n    try {\n      return await super.restore({ ...options, path: tempPath });\n    } finally {\n      await unlink(tempPath);\n    }\n  }\n}\n\nclass BackupCipher {\n  private algorithm = 'aes-256-gcm';\n  private keyDerivation = 'argon2id';\n\n  constructor(private options: EncryptionOptions) {}\n\n  async encryptFile(input: string, output: string): Promise\u003cvoid\u003e {\n    const key = await this.deriveKey(this.options.password);\n    const iv = crypto.randomBytes(16);\n    \n    const cipher = crypto.createCipheriv(this.algorithm, key, iv);\n    \n    const inputStream = createReadStream(input);\n    const outputStream = createWriteStream(output);\n    \n    // Write IV at start\n    outputStream.write(iv);\n    \n    await pipeline(inputStream, cipher, outputStream);\n    \n    // Write auth tag\n    const authTag = cipher.getAuthTag();\n    await appendFile(output, authTag);\n  }\n\n  async decryptFile(input: string, output: string, password?: string): Promise\u003cvoid\u003e {\n    const key = await this.deriveKey(password ?? this.options.password);\n    \n    const inputStream = createReadStream(input);\n    const iv = await this.readBytes(inputStream, 16);\n    \n    const decipher = crypto.createDecipheriv(this.algorithm, key, iv);\n    \n    // Read auth tag from end\n    const stats = await stat(input);\n    const authTag = await this.readBytesFromEnd(input, 16);\n    decipher.setAuthTag(authTag);\n    \n    const outputStream = createWriteStream(output);\n    \n    await pipeline(inputStream, decipher, outputStream);\n  }\n\n  private async deriveKey(password: string): Promise\u003cBuffer\u003e {\n    return argon2.hash(password, {\n      type: argon2.argon2id,\n      memoryCost: 65536,\n      timeCost: 3,\n      parallelism: 4,\n      hashLength: 32,\n      raw: true,\n    });\n  }\n}\n```\n\n#### 2. Compression Support\n```typescript\n// packages/core/src/backup/compressed-backup.ts\nexport class CompressedBackupManager extends BackupManager {\n  async backup(options: BackupOptions \u0026 { compression?: CompressionType }): Promise\u003cstring\u003e {\n    const rawPath = await super.backup(options);\n    \n    if (!options.compression) return rawPath;\n    \n    const compressedPath = await this.compress(rawPath, options.compression);\n    await unlink(rawPath);\n    \n    return compressedPath;\n  }\n\n  private async compress(input: string, type: CompressionType): Promise\u003cstring\u003e {\n    const output = `${input}.${type}`;\n    \n    const compressor = this.getCompressor(type);\n    const inputStream = createReadStream(input);\n    const outputStream = createWriteStream(output);\n    \n    await pipeline(inputStream, compressor, outputStream);\n    \n    return output;\n  }\n\n  private getCompressor(type: CompressionType): Transform {\n    switch (type) {\n      case 'gzip':\n        return zlib.createGzip({ level: 9 });\n      case 'brotli':\n        return zlib.createBrotliCompress({\n          params: { [zlib.constants.BROTLI_PARAM_QUALITY]: 11 },\n        });\n      case 'zstd':\n        return new ZstdCompress({ level: 19 });\n      case 'lz4':\n        return new LZ4Compress();\n      default:\n        throw new Error(`Unknown compression: ${type}`);\n    }\n  }\n\n  async restore(options: RestoreOptions): Promise\u003cRestoreResult\u003e {\n    const compression = this.detectCompression(options.path);\n    \n    if (!compression) return super.restore(options);\n    \n    // Decompress to temp file\n    const tempPath = `/tmp/backup-${Date.now()}.raw`;\n    await this.decompress(options.path, tempPath, compression);\n    \n    try {\n      return await super.restore({ ...options, path: tempPath });\n    } finally {\n      await unlink(tempPath);\n    }\n  }\n}\n\n// Streaming compression for large backups\nexport class StreamingCompressedBackup {\n  async backup(options: BackupOptions \u0026 { compression?: CompressionType }): Promise\u003cvoid\u003e {\n    const { path, compression, onProgress } = options;\n    \n    // Create compressed stream directly during backup\n    const compressor = this.getCompressor(compression ?? 'zstd');\n    const outputStream = createWriteStream(`${path}.zst`);\n    \n    await this.db.raw.backupToStream(\n      pipeline(compressor, outputStream),\n      { onProgress }\n    );\n  }\n}\n```\n\n#### 3. Cloud Storage Integration\n```typescript\n// packages/core/src/backup/cloud-backup.ts\nexport class CloudBackupManager {\n  private providers = new Map\u003cstring, CloudProvider\u003e();\n\n  registerProvider(name: string, provider: CloudProvider): void {\n    this.providers.set(name, provider);\n  }\n\n  async backup(options: BackupOptions \u0026 { cloud?: CloudOptions }): Promise\u003cstring\u003e {\n    // Create local backup first\n    const localPath = await this.localBackup(options);\n    \n    if (!options.cloud) return localPath;\n    \n    const { provider, bucket, prefix } = options.cloud;\n    const cloudProvider = this.providers.get(provider);\n    \n    if (!cloudProvider) {\n      throw new Error(`Unknown cloud provider: ${provider}`);\n    }\n    \n    // Upload to cloud\n    const cloudPath = `${prefix}/${path.basename(localPath)}`;\n    await cloudProvider.upload(localPath, bucket, cloudPath, {\n      onProgress: options.onProgress,\n    });\n    \n    // Optionally delete local\n    if (options.cloud.deleteLocal) {\n      await unlink(localPath);\n    }\n    \n    return `${provider}://${bucket}/${cloudPath}`;\n  }\n\n  async restore(options: RestoreOptions \u0026 { cloud?: CloudOptions }): Promise\u003cRestoreResult\u003e {\n    if (!options.cloud) return this.localRestore(options);\n    \n    const { provider, bucket } = options.cloud;\n    const cloudProvider = this.providers.get(provider);\n    \n    // Download from cloud\n    const localPath = `/tmp/backup-${Date.now()}.bak`;\n    await cloudProvider!.download(bucket, options.path, localPath);\n    \n    try {\n      return await this.localRestore({ ...options, path: localPath });\n    } finally {\n      await unlink(localPath);\n    }\n  }\n\n  async listBackups(cloud: CloudOptions): Promise\u003cBackupInfo[]\u003e {\n    const provider = this.providers.get(cloud.provider);\n    const files = await provider!.list(cloud.bucket, cloud.prefix);\n    \n    return files.map(f =\u003e ({\n      path: f.key,\n      size: f.size,\n      createdAt: f.lastModified,\n      provider: cloud.provider,\n    }));\n  }\n}\n\n// Provider implementations\nclass S3Provider implements CloudProvider {\n  constructor(private client: S3Client) {}\n\n  async upload(local: string, bucket: string, key: string, options?: UploadOptions): Promise\u003cvoid\u003e {\n    const upload = new Upload({\n      client: this.client,\n      params: {\n        Bucket: bucket,\n        Key: key,\n        Body: createReadStream(local),\n      },\n    });\n    \n    upload.on('httpUploadProgress', (progress) =\u003e {\n      options?.onProgress?.({\n        percentage: (progress.loaded! / progress.total!) * 100,\n      });\n    });\n    \n    await upload.done();\n  }\n}\n\nclass GCSProvider implements CloudProvider {\n  constructor(private storage: Storage) {}\n\n  async upload(local: string, bucket: string, key: string): Promise\u003cvoid\u003e {\n    await this.storage.bucket(bucket).upload(local, { destination: key });\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] Snapshot backup without blocking writes\n- [ ] Incremental backup with WAL tracking\n- [ ] Point-in-time recovery\n- [ ] Backup verification and integrity checking\n- [ ] Partial table restore\n- [ ] Encryption with AES-256-GCM\n- [ ] Compression (gzip, brotli, zstd, lz4)\n- [ ] Cloud storage integration (S3, GCS, Azure)\n- [ ] Retention policy management\n\n## Labels\nbackup, disaster-recovery, encryption, compression, cloud","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:45:59.301817-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:19:06.766779-06:00","closed_at":"2026-01-21T20:19:06.766779-06:00","close_reason":"Closed"}
{"id":"evodb-n8j","title":"TypeScript: Export assertNever helper from core for exhaustiveness checks","description":"## Summary\n\nThe `assertNever` helper function is implemented multiple times across packages. It should be exported from core for consistent exhaustiveness checking.\n\n## Locations\n\n### Implementations\n\n1. **core/src/types.ts:353-355** - Primary implementation (not exported from index)\n2. **reader/src/types.ts:351-353** - Duplicate implementation\n\n### Usage Pattern\n\n```typescript\nswitch (value.type) {\n  case 'a': return handleA();\n  case 'b': return handleB();\n  default:\n    return assertNever(value.type, \\`Unhandled type: \\${value.type}\\`);\n}\n```\n\n## Issue\n\n1. `assertNever` is defined in core/src/types.ts but not exported from core/src/index.ts\n2. reader/src/types.ts has its own copy\n3. Other packages may need this utility\n\n## Recommendations\n\n### 1. Export from @evodb/core\n\n```typescript\n// core/src/index.ts\nexport { assertNever } from './types.js';\n```\n\n### 2. Remove duplicate from reader\n\n```typescript\n// reader/src/types.ts\n// Remove local implementation, import from core\nimport { assertNever } from '@evodb/core';\nexport { assertNever };\n```\n\n### 3. Document usage\n\nAdd to developer guidelines:\n- Use assertNever for switch exhaustiveness\n- Import from @evodb/core\n\n## Additional Pattern: Type-safe switch\n\nConsider also exporting a type-safe switch helper:\n\n```typescript\nexport function exhaustiveSwitch\u003cT extends string | number | symbol, R\u003e(\n  value: T,\n  handlers: Record\u003cT, () =\u003e R\u003e,\n  fallback?: (v: never) =\u003e R\n): R {\n  if (value in handlers) {\n    return handlers[value]();\n  }\n  if (fallback) {\n    return fallback(value as never);\n  }\n  throw new Error(\\`Unhandled value: \\${String(value)}\\`);\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:50.478839-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:45.422328-06:00","closed_at":"2026-01-21T20:18:45.422328-06:00","close_reason":"Closed"}
{"id":"evodb-nkba","title":"WRITE: Generate TypeDoc API reference for all packages","description":"## Action: WRITE (Create from scratch)\n\nGenerate comprehensive API reference documentation using TypeDoc for all EvoDB packages.\n\n## Scope\n\nCreate API documentation for all public exports across packages:\n- `@evodb/core` - Core database engine\n- `@evodb/client` - Client-side API\n- `@evodb/sync` - Synchronization layer\n- `@evodb/vector` - Vector search capabilities\n- `@evodb/cloudflare` - Cloudflare Workers integration\n\n## Requirements\n\n### TypeDoc Configuration\n- Configure TypeDoc for monorepo structure\n- Set up proper navigation between packages\n- Enable search functionality\n- Configure proper linking between types\n\n### Documentation Standards\n- **Parameter descriptions**: Every parameter must have a clear description\n- **Return types**: Document all return types with explanations\n- **Exceptions**: Document all thrown exceptions and when they occur\n- **Examples**: Include code examples for all public methods\n- **Type definitions**: Document all exported types and interfaces\n\n### Content Structure\n```\ndocs/api/\n├── index.html          # Landing page with package overview\n├── core/               # @evodb/core documentation\n├── client/             # @evodb/client documentation\n├── sync/               # @evodb/sync documentation\n├── vector/             # @evodb/vector documentation\n└── cloudflare/         # @evodb/cloudflare documentation\n```\n\n### Example Format\n```typescript\n/**\n * Creates a new database instance with the specified configuration.\n * \n * @param config - Database configuration options\n * @param config.name - Unique name for the database\n * @param config.schema - Schema definition for tables\n * @returns A configured database instance ready for operations\n * @throws {DatabaseExistsError} When a database with the same name exists\n * @throws {SchemaValidationError} When the schema is invalid\n * \n * @example\n * ```typescript\n * const db = await createDatabase({\n *   name: 'myapp',\n *   schema: {\n *     users: {\n *       id: 'string',\n *       name: 'string',\n *       email: 'string'\n *     }\n *   }\n * });\n * ```\n */\n```\n\n## Acceptance Criteria\n- [ ] TypeDoc configured for all packages\n- [ ] All public exports documented\n- [ ] Every method has at least one code example\n- [ ] All parameters have descriptions\n- [ ] All return types documented\n- [ ] All exceptions documented\n- [ ] Search functionality works\n- [ ] Cross-package linking works\n- [ ] Generated docs build without warnings","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:51:18.923041-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:51:07.243448-06:00","closed_at":"2026-01-21T19:51:07.243448-06:00","close_reason":"Closed"}
{"id":"evodb-nkp8","title":"TypeScript: Consider Result\u003cT, E\u003e pattern for error handling","description":"## Summary\n\nThe codebase uses exceptions for error handling but could benefit from a Result\u003cT, E\u003e pattern in specific areas for explicit error handling and better type inference.\n\n## Current Error Handling\n\n### Exception-based (Current)\n\n```typescript\n// core/src/errors.ts\nexport class EvoDBError extends Error { ... }\nexport class QueryError extends EvoDBError { ... }\nexport class ValidationError extends EvoDBError { ... }\nexport class StorageError extends EvoDBError { ... }\n```\n\nUsage:\n```typescript\ntry {\n  const result = await query.execute();\n} catch (error) {\n  if (error instanceof QueryError) { ... }\n}\n```\n\n### Where Result Pattern Could Help\n\n1. **Validation functions** - Often return boolean or throw\n2. **Parse functions** - JSON.parse with validation\n3. **Query execution** - Expected failures vs unexpected errors\n4. **Storage operations** - NOT_FOUND is often expected\n\n## Proposed Pattern\n\n```typescript\n// core/src/result.ts\nexport type Result\u003cT, E = Error\u003e = \n  | { ok: true; value: T }\n  | { ok: false; error: E };\n\nexport const Ok = \u003cT\u003e(value: T): Result\u003cT, never\u003e =\u003e ({ ok: true, value });\nexport const Err = \u003cE\u003e(error: E): Result\u003cnever, E\u003e =\u003e ({ ok: false, error });\n\n// Type guards\nexport function isOk\u003cT, E\u003e(result: Result\u003cT, E\u003e): result is { ok: true; value: T } {\n  return result.ok;\n}\n\nexport function isErr\u003cT, E\u003e(result: Result\u003cT, E\u003e): result is { ok: false; error: E } {\n  return !result.ok;\n}\n\n// Helpers\nexport function unwrap\u003cT, E\u003e(result: Result\u003cT, E\u003e): T {\n  if (result.ok) return result.value;\n  throw result.error;\n}\n\nexport function unwrapOr\u003cT, E\u003e(result: Result\u003cT, E\u003e, defaultValue: T): T {\n  return result.ok ? result.value : defaultValue;\n}\n```\n\n## Use Cases\n\n### 1. Validation\n\n```typescript\n// Instead of\nfunction validateSchema(json: unknown): Schema {\n  // throws on invalid\n}\n\n// Could be\nfunction validateSchema(json: unknown): Result\u003cSchema, ValidationError\u003e {\n  if (!isRecord(json)) {\n    return Err(new ValidationError('Expected object'));\n  }\n  // ...\n  return Ok(schema);\n}\n```\n\n### 2. Storage GET\n\n```typescript\n// Instead of\nasync get(key: string): Promise\u003cUint8Array | null\u003e // null = not found\n\n// Could be\ntype GetError = { type: 'not_found' } | { type: 'permission_denied' } | { type: 'network' };\nasync get(key: string): Result\u003cUint8Array, GetError\u003e\n```\n\n### 3. Query execution\n\n```typescript\ntype QueryError = \n  | { type: 'timeout'; durationMs: number }\n  | { type: 'not_found'; table: string }\n  | { type: 'syntax'; message: string };\n\nasync execute(query: Query): Result\u003cQueryResult, QueryError\u003e\n```\n\n## Benefits\n\n1. **Explicit error handling** - Caller must handle errors\n2. **Type-safe errors** - Discriminated unions for error types\n3. **No try/catch needed** - Pattern matching instead\n4. **Composable** - Can chain with map/flatMap\n\n## Drawbacks\n\n1. **Breaking change** - Existing code uses exceptions\n2. **Learning curve** - Unfamiliar pattern to some\n3. **Verbosity** - More code for simple operations\n4. **Mixed usage** - Some errors still need exceptions (OOM, etc.)\n\n## Recommendation\n\nConsider Result pattern for:\n- Validation functions (where errors are expected)\n- Storage get operations (NOT_FOUND is common)\n- Parse functions (invalid input is expected)\n\nKeep exceptions for:\n- Unexpected errors (bugs)\n- System errors (OOM, network failures in critical paths)\n- Errors that should bubble up\n\n## References\n\n- neverthrow library\n- fp-ts Either type\n- Rust Result type","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:46.809766-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:20:55.041022-06:00","closed_at":"2026-01-21T20:20:55.041022-06:00","close_reason":"Closed"}
{"id":"evodb-o0v","title":"TDD: Auto-select Reader vs Query engine","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:42.242082-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:25:33.186245-06:00","closed_at":"2026-01-20T14:25:33.186245-06:00","close_reason":"Closed"}
{"id":"evodb-o7vf","title":"WRITE: Create PERFORMANCE.md with benchmarks and tuning guide","description":"## Overview\nCreate comprehensive performance documentation including benchmarks, tuning guides, and optimization recommendations.\n\n## Tasks\n\n### Benchmark Results Section\n- Document before/after optimization comparisons:\n  - Query execution times\n  - Memory usage patterns\n  - Bundle size improvements\n  - Cold start performance\n- Include methodology for running benchmarks\n- Add reproducible benchmark commands\n\n### Memory Tuning for Different Environments\n- Browser environments:\n  - IndexedDB storage limits\n  - Memory constraints\n  - Service worker considerations\n- Node.js environments:\n  - Heap size recommendations\n  - Garbage collection tuning\n  - Cluster mode guidance\n- Edge/serverless environments:\n  - Cold start optimization\n  - Memory limits per platform\n  - Connection pooling strategies\n\n### Query Optimization Tips\n- Index usage best practices\n- Query planning explanation\n- Common anti-patterns to avoid\n- Batch operation recommendations\n- Transaction sizing guidance\n\n### Bundle Size Analysis\n- Detailed breakdown by module\n- Tree-shaking effectiveness\n- Dynamic import strategies\n- Comparison with alternatives\n\n## Acceptance Criteria\n- [ ] All benchmark numbers are current and reproducible\n- [ ] Tuning recommendations tested in each environment\n- [ ] Code examples for optimization patterns\n- [ ] Clear before/after metrics included","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:29.679606-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:33:41.035984-06:00","closed_at":"2026-01-21T19:33:41.035984-06:00","close_reason":"Closed"}
{"id":"evodb-obil","title":"TDD: Add consistent retry logic with exponential backoff","description":"## Overview\n\nImplement a consistent retry pattern with exponential backoff for resilient operation handling across the codebase. This will provide a unified approach to handling transient failures.\n\n## TDD Approach\n\n### RED Phase - Write Failing Tests\n\n```typescript\n// tests/core/resilience/retry.test.ts\n\ndescribe('Retry Logic with Exponential Backoff', () =\u003e {\n  describe('Basic Retry Behavior', () =\u003e {\n    it('should succeed immediately if operation succeeds', async () =\u003e {\n      const operation = vi.fn().mockResolvedValue('success');\n      \n      const result = await retry(operation, { maxRetries: 3 });\n      \n      expect(result).toBe('success');\n      expect(operation).toHaveBeenCalledTimes(1);\n    });\n\n    it('should retry on failure up to maxRetries', async () =\u003e {\n      const operation = vi.fn()\n        .mockRejectedValueOnce(new Error('fail 1'))\n        .mockRejectedValueOnce(new Error('fail 2'))\n        .mockResolvedValue('success');\n      \n      const result = await retry(operation, { maxRetries: 3 });\n      \n      expect(result).toBe('success');\n      expect(operation).toHaveBeenCalledTimes(3);\n    });\n\n    it('should throw after maxRetries exceeded', async () =\u003e {\n      const operation = vi.fn().mockRejectedValue(new Error('always fails'));\n      \n      await expect(retry(operation, { maxRetries: 3 }))\n        .rejects.toThrow('always fails');\n      \n      expect(operation).toHaveBeenCalledTimes(4); // initial + 3 retries\n    });\n\n    it('should include retry information in final error', async () =\u003e {\n      const operation = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      try {\n        await retry(operation, { maxRetries: 2 });\n      } catch (error) {\n        expect(error).toBeInstanceOf(RetryError);\n        expect((error as RetryError).attempts).toBe(3);\n        expect((error as RetryError).lastError.message).toBe('fail');\n      }\n    });\n  });\n\n  describe('Exponential Backoff Timing', () =\u003e {\n    beforeEach(() =\u003e {\n      vi.useFakeTimers();\n    });\n\n    afterEach(() =\u003e {\n      vi.useRealTimers();\n    });\n\n    it('should wait with exponential backoff between retries', async () =\u003e {\n      const operation = vi.fn()\n        .mockRejectedValueOnce(new Error('1'))\n        .mockRejectedValueOnce(new Error('2'))\n        .mockResolvedValue('ok');\n      \n      const config = { \n        maxRetries: 3, \n        baseDelayMs: 100,\n        maxDelayMs: 10000 \n      };\n      \n      const promise = retry(operation, config);\n      \n      // First call immediate\n      expect(operation).toHaveBeenCalledTimes(1);\n      \n      // Wait for first backoff (100ms)\n      await vi.advanceTimersByTimeAsync(100);\n      expect(operation).toHaveBeenCalledTimes(2);\n      \n      // Wait for second backoff (200ms = 100 * 2^1)\n      await vi.advanceTimersByTimeAsync(200);\n      expect(operation).toHaveBeenCalledTimes(3);\n      \n      await promise;\n    });\n\n    it('should respect maxDelayMs cap', async () =\u003e {\n      const delays: number[] = [];\n      const mockSleep = vi.fn((ms: number) =\u003e {\n        delays.push(ms);\n        return Promise.resolve();\n      });\n      \n      const operation = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      await retry(operation, {\n        maxRetries: 10,\n        baseDelayMs: 1000,\n        maxDelayMs: 5000,\n        _sleep: mockSleep,\n      }).catch(() =\u003e {});\n      \n      // Delays should cap at 5000ms\n      expect(delays.every(d =\u003e d \u003c= 5000)).toBe(true);\n      expect(delays.some(d =\u003e d === 5000)).toBe(true);\n    });\n\n    it('should calculate correct exponential delays', async () =\u003e {\n      const expectedDelays = [100, 200, 400, 800, 1600];\n      const actualDelays: number[] = [];\n      \n      const mockSleep = (ms: number) =\u003e {\n        actualDelays.push(ms);\n        return Promise.resolve();\n      };\n      \n      const operation = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      await retry(operation, {\n        maxRetries: 5,\n        baseDelayMs: 100,\n        maxDelayMs: 10000,\n        jitter: false,\n        _sleep: mockSleep,\n      }).catch(() =\u003e {});\n      \n      expect(actualDelays).toEqual(expectedDelays);\n    });\n  });\n\n  describe('Retry Conditions', () =\u003e {\n    it('should only retry on retryable errors', async () =\u003e {\n      const operation = vi.fn()\n        .mockRejectedValueOnce(new Error('network timeout'))\n        .mockRejectedValue(new ValidationError('invalid input'));\n      \n      const shouldRetry = (error: Error) =\u003e \n        !error.message.includes('invalid');\n      \n      await expect(retry(operation, { maxRetries: 5, shouldRetry }))\n        .rejects.toThrow('invalid input');\n      \n      expect(operation).toHaveBeenCalledTimes(2); // Stopped on non-retryable\n    });\n\n    it('should retry specific HTTP status codes', async () =\u003e {\n      const operation = vi.fn()\n        .mockRejectedValueOnce({ status: 503 })\n        .mockRejectedValueOnce({ status: 429 })\n        .mockRejectedValue({ status: 400 });\n      \n      const shouldRetry = (error: any) =\u003e \n        [429, 500, 502, 503, 504].includes(error.status);\n      \n      await expect(retry(operation, { maxRetries: 5, shouldRetry }))\n        .rejects.toMatchObject({ status: 400 });\n      \n      expect(operation).toHaveBeenCalledTimes(3);\n    });\n  });\n\n  describe('Retry Context and Callbacks', () =\u003e {\n    it('should call onRetry callback with context', async () =\u003e {\n      const onRetry = vi.fn();\n      const operation = vi.fn()\n        .mockRejectedValueOnce(new Error('fail 1'))\n        .mockResolvedValue('ok');\n      \n      await retry(operation, { \n        maxRetries: 3, \n        onRetry,\n        baseDelayMs: 10,\n      });\n      \n      expect(onRetry).toHaveBeenCalledTimes(1);\n      expect(onRetry).toHaveBeenCalledWith({\n        attempt: 1,\n        error: expect.any(Error),\n        nextDelayMs: expect.any(Number),\n      });\n    });\n\n    it('should allow onRetry to abort retries', async () =\u003e {\n      const onRetry = vi.fn().mockReturnValue(false); // Return false to abort\n      const operation = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      await expect(retry(operation, { maxRetries: 5, onRetry }))\n        .rejects.toThrow('fail');\n      \n      expect(operation).toHaveBeenCalledTimes(2); // Initial + 1 retry before abort\n    });\n\n    it('should provide attempt number to operation', async () =\u003e {\n      const operation = vi.fn(async (context: RetryContext) =\u003e {\n        if (context.attempt \u003c 3) {\n          throw new Error(`attempt ${context.attempt}`);\n        }\n        return `success on ${context.attempt}`;\n      });\n      \n      const result = await retry(operation, { maxRetries: 5, passContext: true });\n      \n      expect(result).toBe('success on 3');\n    });\n  });\n\n  describe('Timeout Integration', () =\u003e {\n    it('should timeout individual attempts', async () =\u003e {\n      const slowOperation = vi.fn(async () =\u003e {\n        await sleep(5000);\n        return 'slow result';\n      });\n      \n      await expect(retry(slowOperation, {\n        maxRetries: 2,\n        attemptTimeoutMs: 100,\n      })).rejects.toThrow(/timeout/i);\n    });\n\n    it('should timeout entire retry sequence', async () =\u003e {\n      const operation = vi.fn().mockRejectedValue(new Error('fail'));\n      \n      await expect(retry(operation, {\n        maxRetries: 100,\n        baseDelayMs: 100,\n        totalTimeoutMs: 500,\n      })).rejects.toThrow(/total timeout/i);\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Retry Logic\n\n```typescript\n// src/core/resilience/retry.ts\n\nexport interface RetryConfig {\n  maxRetries: number;\n  baseDelayMs?: number;\n  maxDelayMs?: number;\n  jitter?: boolean;\n  shouldRetry?: (error: Error) =\u003e boolean;\n  onRetry?: (context: RetryCallbackContext) =\u003e boolean | void;\n  attemptTimeoutMs?: number;\n  totalTimeoutMs?: number;\n  passContext?: boolean;\n}\n\nexport interface RetryContext {\n  attempt: number;\n  startTime: number;\n}\n\nexport interface RetryCallbackContext {\n  attempt: number;\n  error: Error;\n  nextDelayMs: number;\n}\n\nexport class RetryError extends Error {\n  constructor(\n    message: string,\n    public attempts: number,\n    public lastError: Error,\n    public errors: Error[] = []\n  ) {\n    super(message);\n    this.name = 'RetryError';\n  }\n}\n\nconst DEFAULT_CONFIG: Required\u003cOmit\u003cRetryConfig, 'onRetry' | 'shouldRetry' | 'attemptTimeoutMs' | 'totalTimeoutMs' | 'passContext'\u003e\u003e = {\n  maxRetries: 3,\n  baseDelayMs: 100,\n  maxDelayMs: 30000,\n  jitter: true,\n};\n\nexport async function retry\u003cT\u003e(\n  operation: ((context?: RetryContext) =\u003e Promise\u003cT\u003e) | (() =\u003e Promise\u003cT\u003e),\n  config: RetryConfig\n): Promise\u003cT\u003e {\n  const {\n    maxRetries,\n    baseDelayMs = DEFAULT_CONFIG.baseDelayMs,\n    maxDelayMs = DEFAULT_CONFIG.maxDelayMs,\n    jitter = DEFAULT_CONFIG.jitter,\n    shouldRetry = () =\u003e true,\n    onRetry,\n    attemptTimeoutMs,\n    totalTimeoutMs,\n    passContext = false,\n  } = config;\n\n  const errors: Error[] = [];\n  const startTime = Date.now();\n  let attempt = 0;\n\n  while (attempt \u003c= maxRetries) {\n    // Check total timeout\n    if (totalTimeoutMs \u0026\u0026 Date.now() - startTime \u003e totalTimeoutMs) {\n      throw new RetryError(\n        `Total timeout of ${totalTimeoutMs}ms exceeded`,\n        attempt,\n        errors[errors.length - 1],\n        errors\n      );\n    }\n\n    try {\n      const context: RetryContext = { attempt, startTime };\n      \n      // Wrap with attempt timeout if configured\n      if (attemptTimeoutMs) {\n        return await withTimeout(\n          passContext ? operation(context) : (operation as () =\u003e Promise\u003cT\u003e)(),\n          attemptTimeoutMs\n        );\n      }\n      \n      return passContext \n        ? await operation(context) \n        : await (operation as () =\u003e Promise\u003cT\u003e)();\n        \n    } catch (error) {\n      const err = error instanceof Error ? error : new Error(String(error));\n      errors.push(err);\n\n      // Check if we should retry\n      if (attempt \u003e= maxRetries || !shouldRetry(err)) {\n        throw new RetryError(\n          `Failed after ${attempt + 1} attempts: ${err.message}`,\n          attempt + 1,\n          err,\n          errors\n        );\n      }\n\n      // Calculate delay with exponential backoff\n      let delayMs = Math.min(\n        baseDelayMs * Math.pow(2, attempt),\n        maxDelayMs\n      );\n\n      // Add jitter (±25%)\n      if (jitter) {\n        const jitterFactor = 0.75 + Math.random() * 0.5;\n        delayMs = Math.floor(delayMs * jitterFactor);\n      }\n\n      // Call onRetry callback\n      if (onRetry) {\n        const shouldContinue = onRetry({\n          attempt,\n          error: err,\n          nextDelayMs: delayMs,\n        });\n        \n        if (shouldContinue === false) {\n          throw new RetryError(\n            `Retry aborted by onRetry callback: ${err.message}`,\n            attempt + 1,\n            err,\n            errors\n          );\n        }\n      }\n\n      // Wait before next attempt\n      await sleep(delayMs);\n      attempt++;\n    }\n  }\n\n  // Should never reach here, but TypeScript needs it\n  throw new RetryError('Unexpected retry state', attempt, errors[errors.length - 1], errors);\n}\n\nasync function withTimeout\u003cT\u003e(promise: Promise\u003cT\u003e, ms: number): Promise\u003cT\u003e {\n  let timeoutId: NodeJS.Timeout;\n  \n  const timeoutPromise = new Promise\u003cnever\u003e((_, reject) =\u003e {\n    timeoutId = setTimeout(() =\u003e {\n      reject(new Error(`Operation timeout after ${ms}ms`));\n    }, ms);\n  });\n\n  try {\n    return await Promise.race([promise, timeoutPromise]);\n  } finally {\n    clearTimeout(timeoutId!);\n  }\n}\n\nfunction sleep(ms: number): Promise\u003cvoid\u003e {\n  return new Promise(resolve =\u003e setTimeout(resolve, ms));\n}\n```\n\n### REFACTOR Phase - Enhance and Integrate\n\n1. **Add jitter strategies**: Full jitter, equal jitter, decorrelated jitter\n2. **Circuit breaker integration**: Connect with circuit breaker pattern\n3. **Metrics collection**: Track retry counts, success rates, latencies\n4. **Predefined retry policies**: Common configurations for HTTP, database, etc.\n\n```typescript\n// Enhanced version with jitter strategies and policies\n\nexport type JitterStrategy = 'none' | 'full' | 'equal' | 'decorrelated';\n\nexport const RetryPolicies = {\n  // For HTTP requests to external services\n  http: {\n    maxRetries: 3,\n    baseDelayMs: 200,\n    maxDelayMs: 5000,\n    jitter: 'full' as JitterStrategy,\n    shouldRetry: (error: any) =\u003e {\n      const retryableCodes = [408, 429, 500, 502, 503, 504];\n      return error.status ? retryableCodes.includes(error.status) : true;\n    },\n  },\n  \n  // For database operations\n  database: {\n    maxRetries: 5,\n    baseDelayMs: 100,\n    maxDelayMs: 2000,\n    jitter: 'equal' as JitterStrategy,\n    shouldRetry: (error: any) =\u003e {\n      const retryableErrors = ['ECONNRESET', 'ETIMEDOUT', 'ECONNREFUSED'];\n      return error.code ? retryableErrors.includes(error.code) : false;\n    },\n  },\n  \n  // For RPC calls\n  rpc: {\n    maxRetries: 3,\n    baseDelayMs: 50,\n    maxDelayMs: 1000,\n    jitter: 'decorrelated' as JitterStrategy,\n  },\n};\n\n// Integration with circuit breaker\nexport function retryWithCircuitBreaker\u003cT\u003e(\n  operation: () =\u003e Promise\u003cT\u003e,\n  retryConfig: RetryConfig,\n  circuitBreaker: CircuitBreaker\n): Promise\u003cT\u003e {\n  return circuitBreaker.execute(() =\u003e retry(operation, retryConfig));\n}\n```\n\n## Acceptance Criteria\n\n- [ ] Basic retry with configurable max attempts works\n- [ ] Exponential backoff calculates correct delays\n- [ ] MaxDelayMs caps the backoff correctly\n- [ ] Jitter adds randomization to delays\n- [ ] shouldRetry predicate controls retry behavior\n- [ ] onRetry callback receives correct context\n- [ ] Attempt timeout works for individual operations\n- [ ] Total timeout works for entire sequence\n- [ ] RetryError contains full attempt history\n- [ ] Predefined policies cover common use cases\n- [ ] Circuit breaker integration is available","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:54.068674-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:45:15.399373-06:00","closed_at":"2026-01-21T19:45:15.399373-06:00","close_reason":"Closed"}
{"id":"evodb-ocx","title":"TDD: Add getting-started documentation","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:33.234699-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.99421-06:00","closed_at":"2026-01-20T16:54:57.99421-06:00","close_reason":"Closed"}
{"id":"evodb-ofu","title":"TDD: Fix SQL injection in table name interpolation","description":"core/src/storage.ts:13 - Table name interpolated without validation. Add whitelist validation or parameterization.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:06.69743-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:24:19.068733-06:00","closed_at":"2026-01-20T13:24:19.068733-06:00","close_reason":"Closed"}
{"id":"evodb-oi2y","title":"TDD: Add performance regression tests for encoding","description":"## Overview\nAdd performance regression tests to prevent encoding performance degradation and establish baseline metrics.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests with Baseline Assertions\n\nWrite performance tests with strict baseline requirements:\n\n```typescript\ndescribe('Encoding Performance', () =\u003e {\n  describe('baseline assertions', () =\u003e {\n    it('should encode 10K rows in \u003c 100ms', async () =\u003e {\n      const rows = generateRows(10_000);\n      const start = performance.now();\n      encode(rows);\n      const duration = performance.now() - start;\n      expect(duration).toBeLessThan(100);\n    });\n    \n    it('should decode 10K rows in \u003c 50ms', async () =\u003e {\n      const encoded = generateEncodedData(10_000);\n      const start = performance.now();\n      decode(encoded);\n      const duration = performance.now() - start;\n      expect(duration).toBeLessThan(50);\n    });\n    \n    it('should encode 1M rows in \u003c 1 second', async () =\u003e {\n      const rows = generateRows(1_000_000);\n      const start = performance.now();\n      encode(rows);\n      const duration = performance.now() - start;\n      expect(duration).toBeLessThan(1000);\n    });\n  });\n  \n  describe('memory efficiency', () =\u003e {\n    it('should not exceed 2x memory overhead during encode', async () =\u003e {\n      // Memory profiling during encode\n    });\n    \n    it('should release memory after encode completes', async () =\u003e {\n      // GC verification\n    });\n  });\n  \n  describe('compression ratios', () =\u003e {\n    it('should achieve \u003e 5x compression for typical data', async () =\u003e {\n      // Compression efficiency\n    });\n    \n    it('should maintain compression under adversarial input', async () =\u003e {\n      // Random/incompressible data handling\n    });\n  });\n});\n```\n\n### GREEN Phase - Add Benchmark Harness with CI Integration\n\n1. Create benchmark harness:\n   ```typescript\n   class BenchmarkRunner {\n     async run(suite: BenchmarkSuite): Promise\u003cBenchmarkResults\u003e;\n     async compare(baseline: BenchmarkResults, current: BenchmarkResults): Comparison;\n     async report(results: BenchmarkResults): void;\n   }\n   \n   interface BenchmarkSuite {\n     name: string;\n     warmup: number;\n     iterations: number;\n     benchmarks: Benchmark[];\n   }\n   ```\n\n2. CI integration:\n   - Add benchmark step to CI pipeline\n   - Store results as artifacts\n   - Fail build on regression \u003e 10%\n   - Generate performance report\n\n3. Make all baseline tests pass\n\n### REFACTOR Phase - Historical Tracking and Alerts\n\n1. Add historical tracking:\n   ```typescript\n   interface PerformanceHistory {\n     timestamp: Date;\n     commit: string;\n     results: BenchmarkResults;\n   }\n   \n   class PerformanceTracker {\n     async store(results: BenchmarkResults): Promise\u003cvoid\u003e;\n     async getHistory(days: number): Promise\u003cPerformanceHistory[]\u003e;\n     async detectRegression(current: BenchmarkResults): Promise\u003cRegression[]\u003e;\n   }\n   ```\n\n2. Add alerting:\n   - Slack/Discord notification on regression\n   - Weekly performance summary report\n   - Trend visualization dashboard\n   \n3. Add advanced benchmarks:\n   - Different data types (strings, numbers, nested)\n   - Various row sizes (small, medium, large)\n   - Concurrent encoding scenarios\n   - Streaming encode/decode\n\n## Acceptance Criteria\n- [ ] All RED phase baseline tests written\n- [ ] Benchmark harness implemented\n- [ ] CI integration complete\n- [ ] Historical tracking operational\n- [ ] Alerting configured\n- [ ] Encode 10K rows \u003c 100ms consistently\n- [ ] No regressions \u003e 10% from baseline","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:22.170055-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:10:39.476027-06:00","closed_at":"2026-01-21T20:10:39.476027-06:00","close_reason":"Closed"}
{"id":"evodb-p96","title":"PRODUCT: Create example projects and starter templates","description":"## Problem\nThere are no example projects or starter templates in the repository. New users cannot quickly see EvoDB in action.\n\n## Solution\nCreate example projects:\n1. **basic-crud** - Simple CRUD app showing insert/query/update/delete\n2. **real-time-analytics** - Dashboard using lakehouse for analytics\n3. **edge-first-app** - Per-user SQLite DOes syncing to lakehouse\n4. **vector-search** - Semantic search using lance-reader\n5. **cloudflare-worker-starter** - Complete wrangler.toml setup\n\n## Acceptance Criteria\n- [ ] Each example has its own directory under /examples\n- [ ] Each example has a README with setup instructions\n- [ ] Examples are runnable locally with wrangler\n- [ ] Include both TypeScript and JavaScript variants where appropriate\n\n## Impact\nCritical for adoption - examples are how developers learn","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:13.33818-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.306039-06:00","closed_at":"2026-01-21T20:14:08.306039-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-pg6q","title":"TDD: Add codegen CLI error handling tests","description":"## Problem\nThe codegen package has tests for generator and validation but the CLI module error handling paths are undertested. CLI tools need robust error handling for user-facing errors.\n\n## Coverage Gap\n- CLI argument parsing edge cases\n- File not found error handling\n- Permission denied scenarios\n- Invalid schema input handling\n- Output directory creation failures\n\n## Acceptance Criteria\n- [ ] Enhance `cli.unit.test.ts` with error path coverage\n- [ ] Test missing required arguments\n- [ ] Test invalid file paths\n- [ ] Test malformed schema input\n- [ ] Test output directory permission errors\n- [ ] Test graceful handling of concurrent generation\n\n## TDD Approach\n1. Define expected CLI error messages\n2. Write tests for each error scenario\n3. Verify exit codes are appropriate\n4. Ensure error messages are actionable","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:50.022304-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T02:26:56.291735-06:00","closed_at":"2026-01-22T02:26:56.291735-06:00","close_reason":"Closed","labels":["codegen","tdd","testing"]}
{"id":"evodb-pyo","title":"TDD: Consolidate 4 storage abstractions","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:38.116683-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:13:17.843019-06:00","closed_at":"2026-01-20T14:13:17.843019-06:00","close_reason":"Closed"}
{"id":"evodb-qaq","title":"Implement single-pass aggregation","description":"## Problem\nEach aggregation function iterates the entire row set independently. 3 aggregations on 100K rows = 300K iterations.\n\n## TDD Approach\n1. Write benchmark for multi-aggregate queries\n2. Implement aggregator state pattern\n3. Single pass updates all aggregators\n4. Verify correctness and performance\n\n## Expected Impact\n- 60-70% reduction in aggregation latency\n- Linear scaling with row count, not row × aggregate count\n\n## Current Code (query-ops.ts:511-673)\n```typescript\ncase 'countDistinct': {\n  for (const row of rows) { /* full iteration */ }\n}\ncase 'sum': {\n  for (const row of rows) { /* another full iteration */ }\n}\n```\n\n## Fix\n```typescript\nconst aggregators = aggregates.map(spec =\u003e createAggregator(spec));\n\nfor (const row of rows) {\n  for (const agg of aggregators) {\n    agg.update(getVal(row));\n  }\n}\n\nreturn aggregators.map(agg =\u003e agg.finalize());\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:45.712651-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:17:58.981385-06:00","closed_at":"2026-01-21T12:17:58.981385-06:00","close_reason":"Closed"}
{"id":"evodb-qmt","title":"TDD: Add codegen package tests (24% coverage)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:46.903693-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.328197-06:00","closed_at":"2026-01-20T16:49:22.328197-06:00","close_reason":"Closed","external_ref":"gh-89"}
{"id":"evodb-qp6","title":"TDD: Optimize null bitmap for sparse data","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:10:05.445643-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:48:04.603-06:00","closed_at":"2026-01-20T17:48:04.603-06:00","close_reason":"Closed"}
{"id":"evodb-qpi","title":"TDD: Add getRange bounds validation","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:10:04.308613-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:48:04.576192-06:00","closed_at":"2026-01-20T17:48:04.576192-06:00","close_reason":"Closed"}
{"id":"evodb-r00","title":"TDD: Fix ArrayBuffer offset calculations in storage","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:25.604272-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:36:42.824427-06:00","closed_at":"2026-01-20T17:36:42.824427-06:00","close_reason":"Closed"}
{"id":"evodb-r1h","title":"TDD: Add error path tests","description":"Only 21 error tests. Add tests for: corrupt blocks, R2 failures, memory constraints, concurrent write conflicts.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:31.941132-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:41:10.239006-06:00","closed_at":"2026-01-20T13:41:10.239006-06:00","close_reason":"Closed"}
{"id":"evodb-r3y","title":"TypeScript: Add runtime validation for JSON.parse results","description":"## Summary\n\nJSON.parse returns `unknown` in strict TypeScript, but the codebase often casts directly to specific types without runtime validation. This can lead to runtime errors if data doesn't match expected types.\n\n## Affected Locations\n\n### lakehouse/src/manifest.ts:304\n```typescript\nconst manifest = parsed as unknown as TableManifest;\n```\n- No validation that `parsed` actually conforms to TableManifest schema\n- Missing schemaVersion could cause subtle bugs\n\n### lakehouse/src/r2.ts:102-147\n```typescript\nfunction parseJsonWithContext\u003cT\u003e(text: string, path: string): T\nasync readJson\u003cT\u003e(path: string): Promise\u003cT | null\u003e\n```\n- Generic JSON parsing with no runtime validation\n- Type parameter T is not enforced at runtime\n\n## Recommendations\n\n### Option 1: Zod Schemas (Recommended)\n\n```typescript\nimport { z } from 'zod';\n\nconst TableManifestSchema = z.object({\n  schemaVersion: z.number(),\n  formatVersion: z.literal(1),\n  tableId: z.string(),\n  // ... rest of schema\n});\n\nexport type TableManifest = z.infer\u003ctypeof TableManifestSchema\u003e;\n\nfunction parseManifest(json: unknown): TableManifest {\n  return TableManifestSchema.parse(json);\n}\n```\n\n### Option 2: Type Guards\n\n```typescript\nfunction isTableManifest(value: unknown): value is TableManifest {\n  if (!isRecord(value)) return false;\n  if (typeof value.schemaVersion !== 'number') return false;\n  // ... validate all required fields\n  return true;\n}\n\nfunction parseManifest(json: unknown): TableManifest {\n  if (!isTableManifest(json)) {\n    throw new ValidationError('Invalid manifest format');\n  }\n  return json;\n}\n```\n\n## Impact\n\n- Prevents runtime type errors when reading corrupted/malformed data\n- Provides better error messages for debugging\n- Enables schema evolution with clear validation","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:24:22.732232-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.171617-06:00","closed_at":"2026-01-21T20:14:08.171617-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-r7t","title":"Unbounded Writer block index growth","description":"TDD: Writer accumulates block indices without bounds. Add configurable limit and eviction strategy to prevent OOM.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:17:59.93461-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:23:32.264479-06:00","closed_at":"2026-01-20T18:23:32.264482-06:00"}
{"id":"evodb-rctd","title":"TDD: Merge SimpleQueryEngine into QueryEngine","description":"## Overview\nMerge SimpleQueryEngine functionality into the main QueryEngine with a 'simple' mode option to reduce code duplication between packages.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests for unified query API\n  - Simple queries work with simple mode\n  - Complex queries work with full mode\n  - Mode switching behavior\n- [ ] Write tests for backwards compatibility\n  - Existing QueryEngine usage unchanged\n  - Existing SimpleQueryEngine usage works with new API\n- [ ] Write tests for query optimization\n  - Simple mode avoids unnecessary overhead\n  - Full mode enables all optimizations\n\n### GREEN Phase - Make Tests Pass\n- [ ] Add 'mode' option to QueryEngine constructor\n  ```typescript\n  interface QueryEngineOptions {\n    mode: 'simple' | 'full' | 'auto'\n  }\n  ```\n- [ ] Implement simple mode execution path\n  - Lightweight query parsing\n  - Direct index lookups\n  - Minimal memory overhead\n- [ ] Implement auto mode detection\n  - Analyze query complexity\n  - Choose appropriate mode\n- [ ] Ensure all existing tests pass\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Remove SimpleQueryEngine class\n- [ ] Remove reader package query duplication\n- [ ] Consolidate query utilities\n- [ ] Update all imports to use unified QueryEngine\n- [ ] Document mode selection guidelines\n\n## Acceptance Criteria\n- Single QueryEngine handles all query types\n- Simple mode maintains performance characteristics\n- No duplicate query code between packages\n- Clear documentation for mode selection\n- Backwards compatible API (or migration guide)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:47.388698-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:31:25.69305-06:00","closed_at":"2026-01-21T20:31:25.69305-06:00","close_reason":"Closed"}
{"id":"evodb-rd68","title":"WRITE: Create Cloudflare Workers deployment guide","description":"## Action: WRITE (Create from scratch)\n\nCreate a comprehensive deployment guide for running EvoDB on Cloudflare Workers.\n\n## Document Location\n`docs/guides/CLOUDFLARE-DEPLOYMENT.md`\n\n## Content Structure\n\n### 1. Prerequisites\n- Cloudflare account setup\n- Wrangler CLI installation and authentication\n- Node.js version requirements\n- EvoDB package installation\n\n### 2. Basic Deployment\n```bash\n# Installation\nnpm install @evodb/cloudflare wrangler\n\n# Initialize project\nwrangler init my-evodb-app\n```\n\n### 3. Wrangler Configuration\nProvide complete `wrangler.toml` examples:\n```toml\nname = \"my-evodb-app\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-01\"\n\n[[durable_objects.bindings]]\nname = \"EVODB\"\nclass_name = \"EvoDBObject\"\n\n[[migrations]]\ntag = \"v1\"\nnew_classes = [\"EvoDBObject\"]\n\n[[r2_buckets]]\nbinding = \"STORAGE\"\nbucket_name = \"evodb-storage\"\n```\n\n### 4. R2 Bucket Setup\n- Creating R2 buckets for persistence\n- Configuring bucket permissions\n- Cross-origin settings for client access\n- Lifecycle policies for data management\n\n### 5. Durable Objects Configuration\n- Defining Durable Object classes\n- Binding configuration\n- State management patterns\n- Hibernation settings\n\n### 6. Environment Variables \u0026 Secrets\n```bash\n# Setting secrets\nwrangler secret put DATABASE_ENCRYPTION_KEY\nwrangler secret put API_KEY\n\n# Environment variables in wrangler.toml\n[vars]\nENVIRONMENT = \"production\"\nLOG_LEVEL = \"info\"\n```\n\n### 7. Worker Code Example\nComplete working example:\n```typescript\nimport { EvoDBDurableObject, createHandler } from '@evodb/cloudflare';\n\nexport { EvoDBDurableObject };\n\nexport default {\n  async fetch(request: Request, env: Env): Promise\u003cResponse\u003e {\n    return createHandler(env)(request);\n  }\n};\n```\n\n### 8. Deployment Commands\n```bash\n# Development\nwrangler dev\n\n# Staging deployment\nwrangler deploy --env staging\n\n# Production deployment\nwrangler deploy --env production\n```\n\n### 9. Custom Domains \u0026 Routes\n- Adding custom domains\n- Route configuration\n- SSL/TLS settings\n\n### 10. Monitoring \u0026 Logging\n- Wrangler tail for real-time logs\n- Setting up analytics\n- Error tracking integration\n\n### 11. Troubleshooting\nCommon issues and solutions:\n- Memory limits\n- CPU time limits\n- Durable Object location hints\n- R2 connectivity issues\n\n## Acceptance Criteria\n- [ ] Complete wrangler.toml examples that work out of the box\n- [ ] All code examples tested and verified\n- [ ] R2 bucket setup fully documented\n- [ ] Durable Objects patterns explained\n- [ ] Secrets management covered\n- [ ] Production checklist included\n- [ ] Troubleshooting section covers common issues","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:51:19.685972-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:31:06.983569-06:00","closed_at":"2026-01-21T19:31:06.983569-06:00","close_reason":"Closed"}
{"id":"evodb-rjd","title":"TDD: Fix BigInt parsing error handling","description":"core/src/storage.ts:124 - BigInt(NaN) will throw. Add try-catch or validate radix conversion before BigInt.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:08:51.015926-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:11:46.747778-06:00","closed_at":"2026-01-20T13:11:46.747778-06:00","close_reason":"Closed"}
{"id":"evodb-rkg","title":"TDD: Add block size validation limits","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:34.935502-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.671577-06:00","closed_at":"2026-01-20T17:49:16.671577-06:00","close_reason":"Closed","external_ref":"gh-123"}
{"id":"evodb-s40","title":"TDD: Re-enable 4 skipped critical tests","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:44.646631-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:12.072381-06:00","closed_at":"2026-01-20T16:55:12.072381-06:00","close_reason":"Closed","external_ref":"gh-90"}
{"id":"evodb-s589","title":"TDD: Consolidate getNestedValue and compareValues across packages","description":"## Overview\n\nMultiple packages have duplicate implementations of `getNestedValue` and `compareValues` utility functions. This leads to inconsistent behavior, maintenance burden, and potential bugs when only some copies are updated.\n\n## TDD Approach\n\n### RED Phase - Write Failing Tests\n\n```typescript\n// tests/core/utils/value-utils.test.ts\n\ndescribe('Value Utilities Consolidation', () =\u003e {\n  describe('getNestedValue - Single Source of Truth', () =\u003e {\n    it('should be exported from core package only', () =\u003e {\n      // These imports should work\n      expect(typeof coreUtils.getNestedValue).toBe('function');\n      \n      // These old locations should re-export from core or not exist\n      // Test that all packages use the same implementation\n      expect(queryUtils.getNestedValue).toBe(coreUtils.getNestedValue);\n      expect(storageUtils.getNestedValue).toBe(coreUtils.getNestedValue);\n    });\n\n    it('should handle simple path access', () =\u003e {\n      const obj = { a: { b: { c: 42 } } };\n      expect(getNestedValue(obj, 'a.b.c')).toBe(42);\n      expect(getNestedValue(obj, ['a', 'b', 'c'])).toBe(42);\n    });\n\n    it('should handle array indices', () =\u003e {\n      const obj = { items: [{ name: 'first' }, { name: 'second' }] };\n      expect(getNestedValue(obj, 'items.0.name')).toBe('first');\n      expect(getNestedValue(obj, 'items[1].name')).toBe('second');\n    });\n\n    it('should return undefined for missing paths', () =\u003e {\n      const obj = { a: 1 };\n      expect(getNestedValue(obj, 'b.c.d')).toBeUndefined();\n    });\n\n    it('should handle null/undefined safely', () =\u003e {\n      expect(getNestedValue(null, 'a.b')).toBeUndefined();\n      expect(getNestedValue(undefined, 'a')).toBeUndefined();\n      expect(getNestedValue({ a: null }, 'a.b')).toBeUndefined();\n    });\n\n    it('should handle edge cases consistently', () =\u003e {\n      // Empty path\n      expect(getNestedValue({ a: 1 }, '')).toEqual({ a: 1 });\n      \n      // Numeric string keys\n      expect(getNestedValue({ '0': 'zero' }, '0')).toBe('zero');\n      \n      // Keys with dots\n      expect(getNestedValue({ 'a.b': 1 }, ['a.b'])).toBe(1);\n    });\n  });\n\n  describe('compareValues - Single Source of Truth', () =\u003e {\n    it('should be exported from core package only', () =\u003e {\n      expect(typeof coreUtils.compareValues).toBe('function');\n      expect(queryUtils.compareValues).toBe(coreUtils.compareValues);\n    });\n\n    it('should compare primitives correctly', () =\u003e {\n      expect(compareValues(1, 2)).toBeLessThan(0);\n      expect(compareValues(2, 1)).toBeGreaterThan(0);\n      expect(compareValues(1, 1)).toBe(0);\n      \n      expect(compareValues('a', 'b')).toBeLessThan(0);\n      expect(compareValues('b', 'a')).toBeGreaterThan(0);\n    });\n\n    it('should handle null/undefined in sorting', () =\u003e {\n      // Nulls should sort to end by default\n      expect(compareValues(1, null)).toBeLessThan(0);\n      expect(compareValues(null, 1)).toBeGreaterThan(0);\n      expect(compareValues(null, null)).toBe(0);\n      \n      // With nullsFirst option\n      expect(compareValues(1, null, { nullsFirst: true })).toBeGreaterThan(0);\n    });\n\n    it('should handle type coercion consistently', () =\u003e {\n      // Numbers vs strings\n      expect(compareValues(2, '10')).toBeLessThan(0); // Numeric comparison\n      expect(compareValues('2', '10')).toBeGreaterThan(0); // String comparison\n    });\n\n    it('should support custom comparators', () =\u003e {\n      const dateCompare = (a: string, b: string) =\u003e \n        new Date(a).getTime() - new Date(b).getTime();\n      \n      expect(compareValues('2024-01-01', '2024-02-01', { comparator: dateCompare }))\n        .toBeLessThan(0);\n    });\n\n    it('should handle deep equality for objects', () =\u003e {\n      expect(compareValues({ a: 1 }, { a: 1 })).toBe(0);\n      expect(compareValues({ a: 1 }, { a: 2 })).not.toBe(0);\n    });\n  });\n\n  describe('No Duplicate Implementations', () =\u003e {\n    it('should not have getNestedValue in query package source', async () =\u003e {\n      const querySource = await glob('packages/query/src/**/*.ts');\n      for (const file of querySource) {\n        const content = await fs.readFile(file, 'utf-8');\n        expect(content).not.toMatch(/^export function getNestedValue/m);\n      }\n    });\n\n    it('should not have compareValues in storage package source', async () =\u003e {\n      const storageSource = await glob('packages/storage/src/**/*.ts');\n      for (const file of storageSource) {\n        const content = await fs.readFile(file, 'utf-8');\n        expect(content).not.toMatch(/^export function compareValues/m);\n      }\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Consolidation\n\n```typescript\n// src/core/utils/value-utils.ts\n\nexport type PathSegment = string | number;\nexport type Path = string | PathSegment[];\n\n/**\n * Get a nested value from an object using a dot-notation path or array of segments.\n * This is the single source of truth for nested value access.\n */\nexport function getNestedValue\u003cT = unknown\u003e(\n  obj: unknown,\n  path: Path\n): T | undefined {\n  if (obj == null) {\n    return undefined;\n  }\n\n  const segments = typeof path === 'string' \n    ? parsePath(path) \n    : path;\n\n  if (segments.length === 0) {\n    return obj as T;\n  }\n\n  let current: unknown = obj;\n  \n  for (const segment of segments) {\n    if (current == null) {\n      return undefined;\n    }\n    \n    if (typeof current !== 'object') {\n      return undefined;\n    }\n    \n    current = (current as Record\u003cstring | number, unknown\u003e)[segment];\n  }\n\n  return current as T;\n}\n\nfunction parsePath(path: string): PathSegment[] {\n  if (path === '') return [];\n  \n  // Handle bracket notation: items[0].name -\u003e items.0.name\n  const normalized = path.replace(/\\[(\\d+)\\]/g, '.$1');\n  return normalized.split('.');\n}\n\nexport interface CompareOptions {\n  nullsFirst?: boolean;\n  comparator?: (a: unknown, b: unknown) =\u003e number;\n}\n\n/**\n * Compare two values for sorting. Returns negative if a \u003c b, positive if a \u003e b, 0 if equal.\n * This is the single source of truth for value comparison.\n */\nexport function compareValues(\n  a: unknown,\n  b: unknown,\n  options: CompareOptions = {}\n): number {\n  const { nullsFirst = false, comparator } = options;\n\n  // Handle nulls\n  if (a == null \u0026\u0026 b == null) return 0;\n  if (a == null) return nullsFirst ? -1 : 1;\n  if (b == null) return nullsFirst ? 1 : -1;\n\n  // Custom comparator\n  if (comparator) {\n    return comparator(a, b);\n  }\n\n  // Same types - direct comparison\n  if (typeof a === typeof b) {\n    if (typeof a === 'string') {\n      return a.localeCompare(b as string);\n    }\n    if (typeof a === 'number') {\n      return a - (b as number);\n    }\n    if (typeof a === 'boolean') {\n      return (a ? 1 : 0) - ((b as boolean) ? 1 : 0);\n    }\n    if (typeof a === 'object') {\n      return JSON.stringify(a) === JSON.stringify(b) ? 0 : \n        JSON.stringify(a) \u003c JSON.stringify(b) ? -1 : 1;\n    }\n  }\n\n  // Mixed types - convert to string\n  return String(a).localeCompare(String(b));\n}\n```\n\n```typescript\n// packages/query/src/utils/index.ts - Re-export from core\n\n// Re-export from core - DO NOT duplicate implementation\nexport { getNestedValue, compareValues } from '@evodb/core/utils';\n```\n\n### REFACTOR Phase - Optimize and Enhance\n\n1. **Memoize path parsing**: Cache parsed paths for repeated access\n2. **Add path validation**: Ensure paths are valid before traversal\n3. **Add TypeScript inference**: Better generic types for return values\n4. **Add benchmarks**: Ensure no performance regression\n\n```typescript\n// Optimized version with caching\nconst pathCache = new Map\u003cstring, PathSegment[]\u003e();\n\nexport function getNestedValue\u003cT = unknown\u003e(\n  obj: unknown,\n  path: Path\n): T | undefined {\n  if (obj == null) return undefined;\n\n  let segments: PathSegment[];\n  \n  if (typeof path === 'string') {\n    // Check cache first\n    let cached = pathCache.get(path);\n    if (!cached) {\n      cached = parsePath(path);\n      if (pathCache.size \u003c 1000) { // Limit cache size\n        pathCache.set(path, cached);\n      }\n    }\n    segments = cached;\n  } else {\n    segments = path;\n  }\n\n  if (segments.length === 0) return obj as T;\n\n  let current: unknown = obj;\n  \n  for (let i = 0; i \u003c segments.length; i++) {\n    if (current == null || typeof current !== 'object') {\n      return undefined;\n    }\n    current = (current as any)[segments[i]];\n  }\n\n  return current as T;\n}\n```\n\n## Files to Update\n\n1. `src/core/utils/value-utils.ts` - Create canonical implementation\n2. `src/core/utils/index.ts` - Export from core\n3. `packages/query/src/utils.ts` - Remove duplicate, re-export from core\n4. `packages/storage/src/utils.ts` - Remove duplicate, re-export from core\n5. `packages/*/src/**/*.ts` - Update imports to use core\n\n## Acceptance Criteria\n\n- [ ] Single implementation in `@evodb/core/utils`\n- [ ] All packages import from core\n- [ ] No duplicate function definitions in any package\n- [ ] All existing tests pass\n- [ ] Behavior is identical to previous implementations\n- [ ] Path caching improves repeated access performance\n- [ ] TypeScript types are preserved/improved","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:04.06838-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:45:28.506121-06:00","closed_at":"2026-01-21T19:45:28.506121-06:00","close_reason":"Closed"}
{"id":"evodb-s7bt","title":"TypeScript: Standardize type guard naming and organization","description":"## Summary\n\nType guards are well-implemented in core/src/guards.ts but there are inconsistencies in naming and organization across packages.\n\n## Current State\n\n### core/src/guards.ts (Good Pattern)\n\nWell-organized type guards with consistent naming:\n- `isArray`, `isRecord`, `isNumber`, `isString`, etc.\n- `isNumberTuple`, `isArrayOf` - compound guards\n- `hasProperty`, `hasProperties` - property checks\n- `assertArray`, `assertNumber`, etc. - assertion helpers\n\n### query/src/types.ts\n\n- `isPartitionValue()` - follows pattern\n- `isPartitionValues()` - follows pattern\n\n### core/src/types.ts\n\n- `isValidBlockId()` - validation not type guard (returns boolean, not narrows)\n- `isValidTableId()` - same issue\n- `isWalEntryMetadata()` - proper type guard\n\n### rpc/src/types.ts\n\n- `isCDCBatchMessage()` - proper discriminated union guard\n- `isAckMessage()`, `isNackMessage()`, etc.\n\n## Issues\n\n1. **Inconsistent return behavior**:\n   - `isValidBlockId(id)` returns `boolean` but doesn't narrow type\n   - Should be `isBlockId(id): id is BlockId` for narrowing\n\n2. **Missing guards for common patterns**:\n   - No `isColumn()` guard for Column interface\n   - No `isSchema()` guard for Schema interface\n   - No `isEncodedColumn()` guard\n\n3. **Scattered locations**:\n   - Type guards spread across types.ts and guards.ts\n   - No single source of truth\n\n## Recommendations\n\n### 1. Rename validation functions\n\n```typescript\n// Current (validation, not narrowing)\nexport function isValidBlockId(id: string): boolean {\n  return BLOCK_ID_REGEX.test(id);\n}\n\n// Add narrowing guard\nexport function isBlockId(id: unknown): id is BlockId {\n  return typeof id === 'string' \u0026\u0026 BLOCK_ID_REGEX.test(id);\n}\n```\n\n### 2. Add interface guards to guards.ts\n\n```typescript\nexport function isColumn(value: unknown): value is Column {\n  return isRecord(value) \u0026\u0026\n    typeof value.path === 'string' \u0026\u0026\n    typeof value.type === 'number' \u0026\u0026\n    typeof value.nullable === 'boolean' \u0026\u0026\n    isArray(value.values);\n}\n\nexport function isSchema(value: unknown): value is Schema {\n  return isRecord(value) \u0026\u0026\n    typeof value.id === 'number' \u0026\u0026\n    typeof value.version === 'number' \u0026\u0026\n    isArrayOf(value.columns, isSchemaColumn);\n}\n```\n\n### 3. Export all guards from core/src/guards/index.ts\n\nConsolidate all type guards in one location for easy discovery.\n\n## Benefits\n\n- Consistent narrowing behavior\n- Better type safety when parsing unknown data\n- Single source of truth for guards","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:12.865864-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:27:07.791231-06:00","closed_at":"2026-01-21T20:27:07.791231-06:00","close_reason":"Closed"}
{"id":"evodb-sdgz","title":"TDD: Consolidate duplicate R2 interfaces","description":"## Overview\nUnify the multiple R2 interface definitions scattered across packages into a single source of truth in core.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests verifying single interface source\n  - Import test that all packages use same interface\n  - Type compatibility tests between implementations\n- [ ] Write tests for R2 interface completeness\n  - All required methods present\n  - Method signatures match R2 API\n- [ ] Write tests for mock R2 implementations\n  - Ensure mocks satisfy the unified interface\n\n### GREEN Phase - Make Tests Pass\n- [ ] Create shared R2 types in `@evodb/core`\n  - `R2Bucket` interface\n  - `R2Object` interface\n  - `R2ObjectBody` interface\n  - `R2ListOptions` interface\n  - etc.\n- [ ] Export types from core package\n- [ ] Update all packages to import from core\n- [ ] Ensure type compatibility across all usages\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Update all imports to use core types\n- [ ] Remove duplicate interface definitions\n- [ ] Add JSDoc documentation for R2 types\n- [ ] Consider re-exporting from @cloudflare/workers-types if available\n\n## Acceptance Criteria\n- Single R2 interface definition in core\n- All packages import from core\n- No duplicate R2 type definitions\n- Full API coverage for R2 bucket operations","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:43.016469-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:27:57.301679-06:00","closed_at":"2026-01-21T20:27:57.301679-06:00","close_reason":"Closed"}
{"id":"evodb-son","title":"TDD: Add distributed tracing with OpenTelemetry","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:57.839461-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:48:04.445284-06:00","closed_at":"2026-01-20T17:48:04.445284-06:00","close_reason":"Closed"}
{"id":"evodb-sp9","title":"TDD: Add chaos tests for partial failures","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:05.451792-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.213788-06:00","closed_at":"2026-01-20T16:49:22.213788-06:00","close_reason":"Closed","external_ref":"gh-91"}
{"id":"evodb-t5q","title":"Consolidate error classes to 3-4 types","description":"## Problem\n10+ error classes extend EvoDBError but code paths never distinguish between types.\n\n## TDD Approach\n1. Audit error usage across codebase\n2. Consolidate to essential types: DatabaseError, QueryError, ValidationError, TimeoutError\n3. Remove captureStackTrace() complexity\n4. Verify error handling still works\n\n## Expected Impact\n- ~8KB bundle reduction\n- Simpler error handling\n\n## Current (10+ classes)\nQueryError, TimeoutError, ValidationError, StorageError, EncodingError, BlockFormatError, ManifestError, etc.\n\n## Target (3-4 classes)\n- EvoDBError (base)\n- QueryError (query issues)\n- ValidationError (schema/input issues)\n- TimeoutError (timing issues)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:36.928792-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:37:18.562466-06:00","closed_at":"2026-01-21T12:37:18.562466-06:00","close_reason":"Closed"}
{"id":"evodb-t6qd","title":"PRODUCT: Real-time sync/subscriptions API","description":"## Problem\nREADME mentions CDC streams but there's no high-level subscription/real-time API exposed.\n\n## Solution\nAdd real-time subscription API:\n```typescript\n// Subscribe to changes\nconst unsubscribe = await db.subscribe('users', {\n  filter: { role: 'admin' },\n  onChange: (change) =\u003e {\n    console.log('Change:', change.type, change.data);\n  }\n});\n\n// Query with live updates\nconst liveQuery = db.query('users')\n  .where('active', '=', true)\n  .subscribe();\n\nfor await (const snapshot of liveQuery) {\n  updateUI(snapshot.rows);\n}\n```\n\n## Implementation\n- Build on top of @evodb/rpc CDC infrastructure\n- WebSocket connections from client to edge DO\n- Server-sent events for simpler use cases\n\n## Impact\nEssential for modern real-time apps","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:29:58.607087-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:08:17.784632-06:00","closed_at":"2026-01-21T20:08:17.784632-06:00","close_reason":"Already implemented"}
{"id":"evodb-tc79","title":"TypeScript: Excellent strict mode configuration - Document for onboarding","description":"## Summary\n\nThe tsconfig.base.json has excellent strict mode configuration. This should be documented and maintained as a best practice.\n\n## Current Configuration (tsconfig.base.json)\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"lib\": [\"ES2022\"],\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \n    // Strict mode flags - ALL ENABLED\n    \"strict\": true,\n    \"strictNullChecks\": true,\n    \"strictFunctionTypes\": true,\n    \"strictBindCallApply\": true,\n    \"strictPropertyInitialization\": true,\n    \"noImplicitAny\": true,\n    \"noImplicitThis\": true,\n    \"useUnknownInCatchVariables\": true,\n    \n    // Additional safety\n    \"noUnusedLocals\": true,\n    \"noUnusedParameters\": true,\n    \"noImplicitReturns\": true,\n    \"noFallthroughCasesInSwitch\": true,\n    \n    // Module safety\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"verbatimModuleSyntax\": true\n  }\n}\n```\n\n## Analysis\n\n### Strict Flags (All Enabled)\n\n| Flag | Value | Impact |\n|------|-------|--------|\n| strict | true | Enables all strict type-checking options |\n| strictNullChecks | true | null/undefined distinct from other types |\n| strictFunctionTypes | true | Contravariant function parameters |\n| strictBindCallApply | true | Type-safe bind/call/apply |\n| strictPropertyInitialization | true | Class properties must be initialized |\n| noImplicitAny | true | No implicit any type |\n| noImplicitThis | true | Error on implicit any for this |\n| useUnknownInCatchVariables | true | catch(e) is unknown, not any |\n\n### Additional Safety Flags\n\n| Flag | Value | Impact |\n|------|-------|--------|\n| noUnusedLocals | true | Error on unused local variables |\n| noUnusedParameters | true | Error on unused function parameters |\n| noImplicitReturns | true | All code paths must return |\n| noFallthroughCasesInSwitch | true | No implicit fallthrough in switch |\n\n### Module Flags\n\n| Flag | Value | Impact |\n|------|-------|--------|\n| verbatimModuleSyntax | true | Strict ESM import/export |\n| isolatedModules | true | Each file treated as module |\n\n## Recommendations\n\n### 1. Document in CONTRIBUTING.md\n\nAdd section on TypeScript configuration:\n- Why strict mode is enabled\n- How to handle strict mode errors\n- When @ts-expect-error is acceptable\n\n### 2. Consider Additional Flags\n\n```json\n{\n  \"exactOptionalPropertyTypes\": true,  // undefined !== missing\n  \"noPropertyAccessFromIndexSignature\": true  // obj['key'] vs obj.key\n}\n```\n\nThese are more aggressive but could improve safety further.\n\n### 3. Create CI Check\n\nEnsure no package overrides strict settings:\n```bash\n# Check that no tsconfig disables strict\ngrep -r '\"strict\": false' packages/*/tsconfig.json\n```\n\n## Good Patterns Found\n\n1. **All packages extend tsconfig.base.json**\n2. **Composite mode for project references**\n3. **Declaration files generated for all packages**\n4. **Source maps for debugging**","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:31:03.607343-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:45.390296-06:00","closed_at":"2026-01-21T20:18:45.390296-06:00","close_reason":"Closed"}
{"id":"evodb-tft","title":"TDD: Fix SQL interpolation with quoted identifiers","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:29.245919-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:36:42.880402-06:00","closed_at":"2026-01-20T17:36:42.880402-06:00","close_reason":"Closed"}
{"id":"evodb-tl7u","title":"TDD: Remove skipped tests in RPC protocol tests","description":"## Problem\nThe RPC protocol tests have 3 skipped tests (it.skip) for ACK message encoding/decoding. Skipped tests indicate incomplete implementation or regression that needs attention.\n\n## Skipped Tests Found\n```\nrpc/src/__tests__/protocol.unit.test.ts:200:    it.skip('should encode ACK message', () =\u003e {\nrpc/src/__tests__/protocol.unit.test.ts:311:    it.skip('should encode ACK to binary', () =\u003e {});\nrpc/src/__tests__/protocol.unit.test.ts:655:  it.skip('should roundtrip ACK message', () =\u003e {});\n```\n\n## Acceptance Criteria\n- [ ] Either implement ACK encoding/decoding and enable tests\n- [ ] Or remove tests if ACK is not part of the protocol spec\n- [ ] Document why ACK messages exist or don't exist in protocol\n\n## TDD Approach\n1. Determine if ACK is part of the protocol design\n2. If yes: implement missing functionality to pass tests\n3. If no: remove skipped tests and document decision","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:18.442426-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:16:41.787547-06:00","closed_at":"2026-01-21T20:16:41.787547-06:00","close_reason":"Closed","labels":["rpc","tdd","tech-debt","testing"]}
{"id":"evodb-tu17","title":"EDIT: Update GETTING_STARTED.md with npm installation","description":"## Overview\nUpdate GETTING_STARTED.md to support public npm installation and improve developer onboarding experience.\n\n## Tasks\n\n### Replace workspace:* with npm install\n- Update all installation commands:\n  ```bash\n  # Before (internal)\n  pnpm add @evodb/core@workspace:*\n  \n  # After (public)\n  npm install @evodb/core\n  # or\n  pnpm add @evodb/core\n  # or\n  yarn add @evodb/core\n  ```\n- Include version pinning recommendations\n- Add peer dependency notes\n\n### Add Runnable Code Examples\n- Ensure every code example is copy-paste ready\n- Add complete working examples:\n  - Basic CRUD operations\n  - Schema definition\n  - Query patterns\n  - Sync setup\n- Include expected output comments\n\n### Add Troubleshooting Section\n- Common installation issues:\n  - Peer dependency conflicts\n  - TypeScript configuration\n  - Bundler setup\n- Runtime errors:\n  - IndexedDB not available\n  - Memory limit exceeded\n  - Sync connection issues\n- Debug mode instructions\n\n### Link to Example Projects\n- Create references to:\n  - Minimal starter template\n  - React integration example\n  - Node.js server example\n  - Full-stack example\n- Include CodeSandbox/StackBlitz links if available\n\n## Acceptance Criteria\n- [ ] All examples work with `npm install`\n- [ ] New users can follow guide without errors\n- [ ] Troubleshooting covers 90% of common issues\n- [ ] Time to first working app \u003c 5 minutes","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:32.586596-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:31:07.055548-06:00","closed_at":"2026-01-21T19:31:07.055548-06:00","close_reason":"Closed"}
{"id":"evodb-u602","title":"TDD: Replace unsafe as unknown casts with type guards","description":"## Overview\nReplace unsafe `as unknown` type casts throughout the codebase with proper type guards to improve type safety and catch errors at runtime.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests First\n- [ ] Write tests for type guard correctness\n  - Positive cases (valid types pass)\n  - Negative cases (invalid types fail)\n  - Edge cases (null, undefined, empty objects)\n- [ ] Write tests for each location using `as unknown`\n  - Identify all cast locations with grep\n  - Write test for expected behavior at each location\n- [ ] Write tests for error handling when type guard fails\n\n### GREEN Phase - Make Tests Pass\n- [ ] Create type guard functions for each cast location\n  - `isBlock(value): value is Block`\n  - `isManifest(value): value is Manifest`\n  - `isQueryResult(value): value is QueryResult`\n  - etc.\n- [ ] Replace `as unknown` casts with type guard calls\n- [ ] Add proper error handling for type guard failures\n- [ ] Ensure all tests pass\n\n### REFACTOR Phase - Improve Code Quality\n- [ ] Add discriminated unions where appropriate\n  - Use `type` or `kind` field for discrimination\n  - Enables exhaustive switch statements\n- [ ] Consolidate type guards into shared utilities\n- [ ] Add branded types for stronger type safety where needed\n- [ ] Document type guard usage patterns\n\n## Acceptance Criteria\n- Zero `as unknown` casts in codebase (or documented exceptions)\n- Type guards provide runtime validation\n- Discriminated unions enable exhaustive checking\n- Clear error messages when type checks fail","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:35:41.818345-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:28:49.476645-06:00","closed_at":"2026-01-21T20:28:49.476645-06:00","close_reason":"Closed"}
{"id":"evodb-usd","title":"Add input validation to column name validators for security","description":"## Problem\n\nThe column name validation in `core/src/query-ops.ts` (lines 182-229) is good but should be enhanced for defense in depth:\n\n**Current Implementation**:\n```typescript\nexport function validateColumnName(name: string): void {\n  // Checks for: empty, whitespace, control chars, SQL injection patterns\n  // Validates segments against VALID_COLUMN_SEGMENT regex\n}\n```\n\n**Potential Improvements**:\n\n1. **Maximum length check** - Currently no limit on column name length:\n   - `query/src/engine.ts:341` has MAX_COLUMN_NAME_LENGTH = 256 but this isn't used consistently\n   - Should be unified and applied in `validateColumnName`\n\n2. **Unicode normalization** - Mixed unicode could bypass validation:\n   - Should normalize to NFC before validation\n   - Consider blocking certain unicode categories\n\n3. **Double encoding detection** - Could miss double-encoded injection:\n   ```typescript\n   // e.g., %27 -\u003e ' after URL decoding\n   ```\n\n4. **Path traversal check** - The storage path validation is separate:\n   - `core/src/storage-provider.ts:39-94` has good path traversal checks\n   - Column names should also check for `..` patterns\n\n## Recommendation\n1. Unify MAX_COLUMN_NAME_LENGTH across all usages\n2. Add unicode normalization\n3. Consider a central security utility for all input validation\n4. Add comprehensive test cases for bypass attempts\n\n## Files\n- `core/src/query-ops.ts:182-229`\n- `query/src/engine.ts:524-552`\n- `core/src/storage-provider.ts:39-94`","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:11.452348-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:40:31.754643-06:00","closed_at":"2026-01-21T19:40:31.754643-06:00","close_reason":"Closed","labels":["query-engine","security","validation"]}
{"id":"evodb-ut2","title":"Add missing JSDoc documentation to public APIs","description":"## Problem\n\nWhile some files have excellent documentation (e.g., `core/src/errors.ts`, `core/src/query-ops.ts`), others are missing JSDoc comments on public APIs.\n\n**Well-documented examples**:\n- `core/src/errors.ts` - All classes have JSDoc with examples\n- `core/src/query-ops.ts` - Functions have clear descriptions\n- `query/src/engine.ts` - Detailed class and method documentation\n\n**Files needing documentation**:\n\n1. **`lakehouse/src/partition.ts`**:\n   - `PartitionIndex` class methods lack documentation\n   - `pruneFilesOptimized` function could use more context\n\n2. **`writer/src/buffer.ts`**:\n   - `BackpressureController` methods need explanation of algorithm\n   - Magic numbers (10000, 4*1024*1024) should be documented\n\n3. **`core/src/storage.ts`**:\n   - `InMemoryR2Bucket` class methods\n   - Helper functions\n\n## Recommendation\n1. Add JSDoc to all exported functions and classes\n2. Include @example for complex APIs\n3. Document magic numbers with constants or comments\n4. Consider generating API docs from JSDoc\n\n## Acceptance Criteria\n- All exported functions have JSDoc with description\n- Complex functions have @example\n- Magic numbers have named constants or comments","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:22.959816-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:10:39.865382-06:00","closed_at":"2026-01-22T03:10:39.865382-06:00","close_reason":"Added comprehensive JSDoc documentation to public APIs across three packages: lakehouse/src/partition.ts (PartitionIndex class, pruneFilesOptimized), writer/src/buffer.ts (BackpressureController, SizeBasedBuffer with named constants), and core/src/storage.ts (storage adapters, validation functions, ID utilities). All documentation includes descriptions, @param/@returns tags, and @example code snippets.","labels":["code-quality","documentation"]}
{"id":"evodb-v38","title":"P3 Polish: Developer Experience","description":"Epic for P3 nice-to-have improvements: console logging cleanup, documentation, and minor type improvements.","status":"closed","priority":3,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:27:48.995-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:05:24.273731-06:00","closed_at":"2026-01-20T11:05:24.273731-06:00","close_reason":"Closed"}
{"id":"evodb-v38.1","title":"Remove console.log from library code","description":"edge-cache has console logging in library code.\n\nRemove or replace with proper logging interface that can be configured/disabled.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:50.455873-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:41:07.903495-06:00","closed_at":"2026-01-20T10:41:07.903495-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-v38.1","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:30:50.456498-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v38.2","title":"Fix cross-package relative imports in codegen","description":"Codegen has cross-package relative imports that could break if package structure changes.\n\nUse proper workspace imports instead.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:55.022532-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:37:07.746832-06:00","closed_at":"2026-01-20T10:37:07.746832-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-v38.2","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:30:55.023329-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v38.3","title":"Fix production any type in ONet loader","description":"benchmark/src/datasets/onet/loader.ts line 474:\ntype: oiRating['Element Name'] as any\n\nDefine proper union type for Element Name values instead of using any.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:30:58.692133-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:36:58.139491-06:00","closed_at":"2026-01-20T10:36:58.139491-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-v38.3","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:30:58.692772-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v38.4","title":"Replace test file any types with proper mock types","description":"Multiple test files use any for mocking (writer.test.ts, compactor.test.ts, query-engine.test.ts).\n\nCreate proper mock types for better type safety in tests.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:31:02.115563-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T10:47:47.832622-06:00","closed_at":"2026-01-20T10:47:47.832622-06:00","close_reason":"Closed","dependencies":[{"issue_id":"evodb-v38.4","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:31:02.116139-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v38.5","title":"Add JSDoc to complex types in query/types.ts","description":"ZoneMapColumn and other complex types in query/src/types.ts lack documentation.\n\nAdd JSDoc comments explaining purpose and usage.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:31:05.498025-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:06:37.649661-06:00","closed_at":"2026-01-20T11:06:37.649661-06:00","close_reason":"Closed","external_ref":"gh-30","dependencies":[{"issue_id":"evodb-v38.5","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:31:05.500893-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v38.6","title":"Add explicit strict flags to all tsconfig.json","description":"Some packages use only strict: true without explicit flags (noImplicitAny, strictNullChecks, etc).\n\nAdd explicit flags for consistency and clarity across all packages.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T10:31:09.184885-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T11:06:37.648378-06:00","closed_at":"2026-01-20T11:06:37.648378-06:00","close_reason":"Closed","external_ref":"gh-31","dependencies":[{"issue_id":"evodb-v38.6","depends_on_id":"evodb-v38","type":"parent-child","created_at":"2026-01-20T10:31:09.185486-06:00","created_by":"Nathan Clevenger"}]}
{"id":"evodb-v3l","title":"Consolidate storage interface fragmentation","description":"TDD: 3+ overlapping storage interfaces exist. Consolidate into unified StorageProvider interface with clear contracts.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:07.517947-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:39:03.163565-06:00","closed_at":"2026-01-20T18:39:03.163568-06:00"}
{"id":"evodb-v5y8","title":"PRODUCT: Full-text search integration","description":"## Problem\nVector search exists (lance-reader) but no full-text search capability.\n\n## Solution\nAdd full-text search:\n```typescript\n// Create text index\nawait db.schema.createIndex('posts', {\n  columns: ['title', 'body'],\n  type: 'fulltext'\n});\n\n// Search with ranking\nconst results = await db.query('posts')\n  .search('typescript cloudflare workers')\n  .orderBy('_score', 'desc')\n  .limit(20);\n\n// Combined with filters\nconst results = await db.query('posts')\n  .where('published', '=', true)\n  .search('edge computing')\n  .limit(10);\n```\n\n## Implementation Options\n- Columnar inverted index in blocks\n- Integration with external search (Typesense, Meilisearch)\n- Lightweight BM25 implementation\n\n## Impact\nCommon requirement for content-heavy applications","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:52.953941-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.034688-06:00","closed_at":"2026-01-21T20:14:08.034688-06:00","close_reason":"Duplicate of already-implemented issues"}
{"id":"evodb-v7p","title":"TDD: Fix test any assertions in snippets-chain","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:59.679162-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:55:11.981094-06:00","closed_at":"2026-01-20T16:55:11.981094-06:00","close_reason":"Closed","external_ref":"gh-92"}
{"id":"evodb-vnv","title":"TDD: Implement RPC deduplication","description":"RPC deduplication is stubbed - data loss risk. Implement proper deduplication with sliding window.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:53.53576-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:24:21.504891-06:00","closed_at":"2026-01-20T13:24:21.504891-06:00","close_reason":"Closed"}
{"id":"evodb-vo8z","title":"DX: TypeScript IntelliSense for query builder","description":"## Problem\nQueryBuilder fluent API exists but TypeScript inference could be stronger. Users don't get column name completion.\n\n## Solution\nEnhance type inference:\n```typescript\n// When schema is locked, columns should be inferred\nawait db.schema.lock('users', {\n  name: { type: 'string', required: true },\n  email: { type: 'string', required: true }\n});\n\n// Now query builder should suggest 'name' and 'email'\ndb.query('users')\n  .where('na|')  // Autocomplete: name\n  .select(['n|']) // Autocomplete: name\n```\n\n## Implementation\n- Generate declaration files from locked schemas\n- Use template literal types for column names\n- Integrate with codegen pull command\n\n## Impact\nSignificantly improves developer experience","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:32.24815-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:20:59.417536-06:00","closed_at":"2026-01-21T20:20:59.417536-06:00","close_reason":"Closed"}
{"id":"evodb-vrt","title":"Run comprehensive performance benchmarks post-optimization","description":"## Context\n24 performance and bundle size optimizations were just implemented. Need to run comprehensive benchmarks to validate improvements.\n\n## Tasks\n1. Run existing benchmark suite in /benchmark package\n2. Measure bundle sizes before/after for all packages\n3. Benchmark query performance (filter, sort, aggregate)\n4. Benchmark memory usage during compaction/merge\n5. Compare against baseline metrics\n6. Document results in a benchmark report\n\n## Expected Improvements\n- 40-50% bundle size reduction\n- 40-60% query performance improvement  \n- 30-40% memory reduction during operations\n\n## Deliverables\n- Benchmark results with before/after comparison\n- Bundle size analysis for each package\n- Performance regression tests if needed","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:21:45.764543-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:44:39.05078-06:00","closed_at":"2026-01-21T19:44:39.05078-06:00","close_reason":"Benchmark results documented. Performance benchmarks pass: 19/19 core benchmark tests, 31/31 snippet benchmark tests. Bundle sizes: core=1.4MB, query=260KB, lakehouse=332KB, writer=524KB. Compression ratios: 32x for sorted integers (Delta+BitPack), 8x for booleans (bitmap). PERFORMANCE.md already contains comprehensive documentation including optimization strategies, memory tuning for Snippets (32MB limit), and benchmark data. Zero-copy decode achieves target throughput for 50MB numeric data."}
{"id":"evodb-vss","title":"TDD: Add column name validation in filter predicates","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:10:02.922049-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:48:04.550536-06:00","closed_at":"2026-01-20T17:48:04.550536-06:00","close_reason":"Closed"}
{"id":"evodb-vtgk","title":"Add defensive null checks in writer buffer operations","description":"## Problem\n\nThe writer buffer operations in `writer/src/buffer.ts` and `writer/src/writer.ts` could benefit from more defensive programming:\n\n**Potential issues**:\n\n1. **LSN comparison edge cases** (`buffer.ts:187`):\n```typescript\nconst shouldUpdate = currentCursor === undefined || newLsn \u003e currentCursor;\n```\n- Doesn't handle BigInt edge cases like negative LSNs\n- Could validate LSN is positive before comparison\n\n2. **Entry size calculation** (`buffer.ts:349-352`):\n```typescript\nprivate estimateEntrySize(entry: WalEntry): number {\n  return 24 + entry.data.length + 4;\n}\n```\n- Assumes entry.data is always present and has length\n- Could throw if malformed entry is passed\n\n3. **State persistence** (`writer.ts:219-236`):\n```typescript\nconst state: PersistentState = {\n  lastBlockSeq: this.lastBlockSeq,\n  pendingBlocks: this.pendingBlocks,\n  blockIndex: this.blockIndex,\n  // ...\n}\n```\n- If serialization fails, state could be lost\n- Should wrap in try/catch with error logging\n\n4. **Source stats Map operations** (`writer.ts:245-260`):\n- Uses Map get/set without null checks on some paths\n\n## Recommendation\n1. Add validation at buffer entry points\n2. Wrap state persistence in try/catch\n3. Add defensive null checks for Map operations\n4. Validate BigInt LSN values are positive\n\n## Files\n- `writer/src/buffer.ts:187,349-352`\n- `writer/src/writer.ts:219-236,245-260`","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:27:40.4521-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:18:47.890149-06:00","closed_at":"2026-01-21T20:18:47.890149-06:00","close_reason":"Closed","labels":["error-handling","robustness","writer"]}
{"id":"evodb-w0uo","title":"WRITE: Create SECURITY.md with security model documentation","description":"## Overview\nCreate comprehensive security documentation covering the security model, best practices, and implementation guidance.\n\n## Tasks\n\n### Input Validation Approach\n- Document validation strategies:\n  - Schema validation with Zod\n  - Query parameter sanitization\n  - Type coercion rules\n- Explain validation boundaries\n- Show custom validator patterns\n\n### Access Control Patterns\n- Document permission models:\n  - Row-level security\n  - Field-level permissions\n  - Collection-level access\n- Explain policy definition syntax\n- Show common access control patterns:\n  - User owns data\n  - Role-based access\n  - Attribute-based access\n\n### Data Encryption\n- At-rest encryption:\n  - IndexedDB encryption options\n  - Key management strategies\n  - Field-level encryption\n- In-transit encryption:\n  - Sync protocol security\n  - TLS requirements\n  - Certificate handling\n\n### Security Best Practices\n- Secure configuration defaults\n- Environment-specific recommendations\n- Audit logging setup\n- Incident response guidance\n- Common vulnerability prevention:\n  - Injection attacks\n  - Data leakage\n  - Replay attacks\n\n## Acceptance Criteria\n- [ ] All security features documented with examples\n- [ ] Threat model overview included\n- [ ] Integration examples for common auth providers\n- [ ] Security checklist for production deployments","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:30.572602-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T19:32:31.113791-06:00","closed_at":"2026-01-21T19:32:31.113791-06:00","close_reason":"Closed"}
{"id":"evodb-w1m","title":"Add Plugin Architecture for Custom Encodings and Indexes","description":"## Current State\nThe codebase has hard-coded support for:\n- Encoding types: Plain, RLE, Dict, Delta\n- Index types: IVF-PQ, HNSW\n- Column types: Null, Bool, Int32, Int64, Float64, String, Binary, Array, Object, Timestamp, Date\n\nAdding new types requires modifying core code.\n\n## Proposed Improvement\n1. Define extension point interfaces:\n```typescript\n// Encoding plugin\ninterface EncodingPlugin {\n  name: string;\n  typeId: number;\n  encode(values: unknown[], stats: ColumnStats): Uint8Array;\n  decode(data: Uint8Array, stats: ColumnStats): unknown[];\n  estimateSize(values: unknown[]): number;\n}\n\n// Index plugin\ninterface IndexPlugin {\n  name: string;\n  build(data: ArrayBuffer, config: IndexConfig): Promise\u003cIndex\u003e;\n  search(index: Index, query: Vector, k: number): Promise\u003cSearchResult[]\u003e;\n  serialize(index: Index): Uint8Array;\n  deserialize(data: Uint8Array): Index;\n}\n```\n\n2. Create registry pattern:\n```typescript\nconst encodingRegistry = new EncodingRegistry();\nencodingRegistry.register(new DictionaryEncoding());\nencodingRegistry.register(new RunLengthEncoding());\n// Custom encoding\nencodingRegistry.register(new BitPackedEncoding());\n```\n\n3. Use registry in core operations:\n```typescript\nfunction encodeColumn(column: Column, registry: EncodingRegistry): EncodedColumn {\n  const encoding = registry.selectBest(column);\n  return encoding.encode(column.values, column.stats);\n}\n```\n\n## Use Cases\n- Custom compression for domain-specific data\n- Specialized indexes (e.g., geo-spatial, full-text)\n- Custom column types (e.g., Decimal, Money, UUID)\n\n## Migration Path\n- Phase 1: Add plugin interfaces\n- Phase 2: Refactor existing encodings/indexes as plugins\n- Phase 3: Document plugin development\n\n## Edge Computing Impact\n- Only load plugins that are used\n- Custom optimizations for specific workloads","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:54.032892-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-22T03:18:04.616131-06:00","closed_at":"2026-01-22T03:18:04.616131-06:00","close_reason":"Closed","labels":["architecture","extensibility","plugins"]}
{"id":"evodb-wjr","title":"TDD: Split monolithic test files","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:58:51.462066-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:49:22.299212-06:00","closed_at":"2026-01-20T16:49:22.299212-06:00","close_reason":"Closed"}
{"id":"evodb-woi","title":"TDD: Add max recursion depth to shred walk()","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T17:09:59.029319-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T17:49:16.678501-06:00","closed_at":"2026-01-20T17:49:16.678501-06:00","close_reason":"Closed","external_ref":"gh-127"}
{"id":"evodb-wqa","title":"Reuse TextEncoder instead of creating in loop","description":"## Problem\nTextEncoder is created inside encoding loop, losing cache benefits and adding constructor overhead.\n\n## TDD Approach\n1. Write benchmark for string encoding\n2. Move TextEncoder to outer scope\n3. Pass encoder to encoding functions\n4. Verify performance improvement\n\n## Expected Impact\n- Reduced constructor overhead\n- Better UTF-8 conversion cache utilization\n\n## Current Code (encode.ts:752-773)\n```typescript\nfunction encodeValue(...) {\n  const encoder = new TextEncoder();  // Created in loop\n  const e = encoder.encode(v as string);\n}\n```\n\n## Fix\n```typescript\nconst encoder = new TextEncoder();  // Module level\n\nexport function encodePlain(col: Column): Uint8Array {\n  for (const v of col.values) {\n    chunks.push(encodeValue(v, col.type, encoder));\n  }\n}\n```","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:04:49.145334-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T12:28:35.063875-06:00","closed_at":"2026-01-21T12:28:35.063875-06:00","close_reason":"Closed"}
{"id":"evodb-wuev","title":"EDIT: Expand DEBUGGING.md with more troubleshooting scenarios","description":"## Overview\n\nExpand the existing DEBUGGING.md documentation with additional troubleshooting scenarios covering CDC, R2 storage, query performance, and memory issues.\n\n## Content Requirements\n\n### 1. CDC Debugging Section\n- Change Data Capture event inspection\n- Event ordering issues\n- Missing events troubleshooting\n- Replay and recovery procedures\n- CDC lag monitoring\n- Debugging CDC pipelines\n\n```typescript\n// Example: Inspecting CDC events\ndb.cdc.inspect({ table: 'users', from: timestamp })\n```\n\n### 2. R2 Storage Debugging\n- R2 connection issues\n- Object not found errors\n- Permission and CORS problems\n- Multipart upload failures\n- Storage quota issues\n- R2 vs local storage debugging\n\n### 3. Query Performance Debugging\n- Query plan analysis\n- Index usage verification\n- Slow query identification\n- Explain output interpretation\n- Query optimization suggestions\n- Profiling query execution\n\n```typescript\n// Example: Query profiling\nconst result = await db.query(sql).explain()\nconsole.log(result.plan)\n```\n\n### 4. Memory Leak Detection\n- Heap snapshot analysis\n- Common leak patterns in EvoDb\n- Connection pool leaks\n- Event listener cleanup\n- Buffer accumulation\n- Tools: Node.js inspector, Chrome DevTools\n\n### 5. Additional Scenarios\n- Worker timeout debugging\n- Durable Object state issues\n- Replication lag diagnosis\n- Index corruption detection\n- Transaction deadlock analysis\n\n## File Location\n\n`docs/DEBUGGING.md` (existing file to expand)\n\n## Acceptance Criteria\n\n- [ ] Each section has step-by-step troubleshooting guide\n- [ ] Real error messages with explanations\n- [ ] Code snippets for diagnostic commands\n- [ ] Links to relevant tools and resources","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:50:46.874276-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:04:13.070095-06:00","closed_at":"2026-01-21T20:04:13.070095-06:00","close_reason":"Closed"}
{"id":"evodb-wug5","title":"PRODUCT: Dashboard/admin UI for monitoring","description":"## Problem\nNo visual interface for monitoring EvoDB deployments.\n\n## Solution\nCreate an admin dashboard:\n1. **Metrics Dashboard**\n   - Query latency (p50, p99)\n   - Cache hit rates\n   - Block compaction status\n   - CDC lag\n\n2. **Data Explorer**\n   - Browse tables and schemas\n   - Run ad-hoc queries\n   - View time-travel snapshots\n\n3. **Schema Manager**\n   - Visual schema diff\n   - Migration preview\n   - Lock/unlock tables\n\n## Implementation\n- Standalone web app (React)\n- Connects to EvoDB via API\n- Can be self-hosted or SaaS\n\n## Impact\nEssential for production operations","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:34.329662-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:19:31.530474-06:00","closed_at":"2026-01-21T20:19:31.530474-06:00","close_reason":"Closed"}
{"id":"evodb-wvrw","title":"TDD: Implement CSV/JSON/Parquet import/export","description":"## Overview\nImplement data import/export functionality supporting CSV, JSON, and Parquet formats with streaming, schema inference, and progress reporting.\n\n## TDD Phases\n\n### RED Phase - Write Failing Tests\n\n#### 1. Format Detection Tests\n```typescript\ndescribe('Format Detection', () =\u003e {\n  it('should detect CSV format from file extension', () =\u003e {\n    expect(detectFormat('data.csv')).toBe('csv');\n    expect(detectFormat('data.CSV')).toBe('csv');\n    expect(detectFormat('path/to/data.csv')).toBe('csv');\n  });\n\n  it('should detect JSON format from file extension', () =\u003e {\n    expect(detectFormat('data.json')).toBe('json');\n    expect(detectFormat('data.jsonl')).toBe('jsonl');\n    expect(detectFormat('data.ndjson')).toBe('jsonl');\n  });\n\n  it('should detect Parquet format from file extension', () =\u003e {\n    expect(detectFormat('data.parquet')).toBe('parquet');\n    expect(detectFormat('data.pq')).toBe('parquet');\n  });\n\n  it('should detect format from content when extension ambiguous', async () =\u003e {\n    const csvContent = 'id,name\\n1,Alice\\n2,Bob';\n    expect(await detectFormatFromContent(csvContent)).toBe('csv');\n    \n    const jsonContent = '[{\"id\": 1, \"name\": \"Alice\"}]';\n    expect(await detectFormatFromContent(jsonContent)).toBe('json');\n    \n    const jsonlContent = '{\"id\": 1}\\n{\"id\": 2}';\n    expect(await detectFormatFromContent(jsonlContent)).toBe('jsonl');\n  });\n\n  it('should detect format from magic bytes for Parquet', async () =\u003e {\n    const parquetBuffer = Buffer.from([0x50, 0x41, 0x52, 0x31]); // PAR1\n    expect(await detectFormatFromContent(parquetBuffer)).toBe('parquet');\n  });\n\n  it('should throw on unrecognized format', () =\u003e {\n    expect(() =\u003e detectFormat('data.xyz')).toThrow('Unrecognized format');\n  });\n});\n```\n\n#### 2. Streaming Import Tests\n```typescript\ndescribe('Streaming Import', () =\u003e {\n  describe('CSV Import', () =\u003e {\n    it('should import CSV file into table', async () =\u003e {\n      const result = await db.import({\n        file: 'users.csv',\n        table: 'users',\n      });\n      \n      expect(result.rowsImported).toBe(1000);\n      const count = await db.queryOne('SELECT COUNT(*) as count FROM users');\n      expect(count.count).toBe(1000);\n    });\n\n    it('should handle CSV with custom delimiter', async () =\u003e {\n      const result = await db.import({\n        file: 'data.tsv',\n        table: 'data',\n        csv: { delimiter: '\\t' },\n      });\n      \n      expect(result.rowsImported).toBe(100);\n    });\n\n    it('should handle CSV with quoted fields', async () =\u003e {\n      // CSV content: id,name\\n1,\"Alice, Jr.\"\\n2,\"Bob \"\"The Builder\"\"\"\n      const result = await db.import({\n        file: 'quoted.csv',\n        table: 'users',\n      });\n      \n      const users = await db.query('SELECT * FROM users');\n      expect(users[0].name).toBe('Alice, Jr.');\n      expect(users[1].name).toBe('Bob \"The Builder\"');\n    });\n\n    it('should handle CSV with headers', async () =\u003e {\n      const result = await db.import({\n        file: 'data.csv',\n        table: 'data',\n        csv: { header: true },\n      });\n      \n      // First row should be used as column names, not data\n      expect(result.rowsImported).toBe(99); // 100 lines - 1 header\n    });\n\n    it('should handle CSV without headers', async () =\u003e {\n      const result = await db.import({\n        file: 'no-header.csv',\n        table: 'data',\n        csv: { header: false },\n        columns: ['col1', 'col2', 'col3'],\n      });\n      \n      expect(result.rowsImported).toBe(100);\n    });\n\n    it('should stream large CSV files without loading into memory', async () =\u003e {\n      const memBefore = process.memoryUsage().heapUsed;\n      \n      await db.import({\n        file: 'large.csv', // 1GB file\n        table: 'large_data',\n      });\n      \n      const memAfter = process.memoryUsage().heapUsed;\n      // Memory increase should be minimal (\u003c 100MB for batch buffers)\n      expect(memAfter - memBefore).toBeLessThan(100 * 1024 * 1024);\n    });\n  });\n\n  describe('JSON Import', () =\u003e {\n    it('should import JSON array', async () =\u003e {\n      // File: [{\"id\": 1, \"name\": \"Alice\"}, {\"id\": 2, \"name\": \"Bob\"}]\n      const result = await db.import({\n        file: 'users.json',\n        table: 'users',\n      });\n      \n      expect(result.rowsImported).toBe(2);\n    });\n\n    it('should import JSONL (newline-delimited JSON)', async () =\u003e {\n      // File: {\"id\": 1}\\n{\"id\": 2}\\n{\"id\": 3}\n      const result = await db.import({\n        file: 'events.jsonl',\n        table: 'events',\n      });\n      \n      expect(result.rowsImported).toBe(3);\n    });\n\n    it('should handle nested JSON with flattening', async () =\u003e {\n      // File: [{\"user\": {\"id\": 1, \"profile\": {\"name\": \"Alice\"}}}]\n      const result = await db.import({\n        file: 'nested.json',\n        table: 'users',\n        json: { flatten: true, separator: '_' },\n      });\n      \n      const users = await db.query('SELECT * FROM users');\n      expect(users[0].user_id).toBe(1);\n      expect(users[0].user_profile_name).toBe('Alice');\n    });\n\n    it('should handle JSON with arrays as JSON columns', async () =\u003e {\n      // File: [{\"id\": 1, \"tags\": [\"a\", \"b\", \"c\"]}]\n      const result = await db.import({\n        file: 'tagged.json',\n        table: 'items',\n      });\n      \n      const items = await db.query('SELECT * FROM items');\n      expect(JSON.parse(items[0].tags)).toEqual(['a', 'b', 'c']);\n    });\n  });\n\n  describe('Parquet Import', () =\u003e {\n    it('should import Parquet file', async () =\u003e {\n      const result = await db.import({\n        file: 'data.parquet',\n        table: 'data',\n      });\n      \n      expect(result.rowsImported).toBe(10000);\n    });\n\n    it('should preserve Parquet column types', async () =\u003e {\n      await db.import({\n        file: 'typed.parquet',\n        table: 'typed_data',\n      });\n      \n      const schema = await db.query(\"PRAGMA table_info('typed_data')\");\n      expect(schema).toContainEqual(expect.objectContaining({ name: 'id', type: 'INTEGER' }));\n      expect(schema).toContainEqual(expect.objectContaining({ name: 'amount', type: 'REAL' }));\n      expect(schema).toContainEqual(expect.objectContaining({ name: 'created_at', type: 'TEXT' })); // timestamp as ISO string\n    });\n\n    it('should read Parquet row groups for streaming', async () =\u003e {\n      const progressUpdates: number[] = [];\n      \n      await db.import({\n        file: 'large.parquet',\n        table: 'large_data',\n        onProgress: (progress) =\u003e progressUpdates.push(progress.percentage),\n      });\n      \n      // Should have multiple progress updates (one per row group)\n      expect(progressUpdates.length).toBeGreaterThan(5);\n    });\n  });\n});\n```\n\n#### 3. Schema Inference Tests\n```typescript\ndescribe('Schema Inference', () =\u003e {\n  it('should infer INTEGER type', async () =\u003e {\n    const schema = await inferSchema('integers.csv');\n    expect(schema.columns[0].type).toBe('INTEGER');\n  });\n\n  it('should infer REAL type', async () =\u003e {\n    const schema = await inferSchema('floats.csv');\n    expect(schema.columns[0].type).toBe('REAL');\n  });\n\n  it('should infer TEXT type', async () =\u003e {\n    const schema = await inferSchema('strings.csv');\n    expect(schema.columns[0].type).toBe('TEXT');\n  });\n\n  it('should infer BLOB type for base64 encoded data', async () =\u003e {\n    const schema = await inferSchema('binary.csv', { detectBlob: true });\n    expect(schema.columns[0].type).toBe('BLOB');\n  });\n\n  it('should infer nullable columns', async () =\u003e {\n    // CSV with some null values\n    const schema = await inferSchema('nullable.csv');\n    expect(schema.columns[0].nullable).toBe(true);\n  });\n\n  it('should sample rows for type inference', async () =\u003e {\n    const schema = await inferSchema('large.csv', { sampleSize: 1000 });\n    // Should not read entire file for type inference\n    expect(schema.inferenceMetadata.rowsSampled).toBe(1000);\n  });\n\n  it('should handle mixed types by choosing most general', async () =\u003e {\n    // Column with \"1\", \"2.5\", \"three\" -\u003e TEXT\n    const schema = await inferSchema('mixed.csv');\n    expect(schema.columns[0].type).toBe('TEXT');\n  });\n\n  it('should create table from inferred schema', async () =\u003e {\n    await db.import({\n      file: 'data.csv',\n      table: 'auto_table',\n      createTable: true,\n    });\n    \n    const exists = await db.queryOne(\n      \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\",\n      ['auto_table']\n    );\n    expect(exists).toBeTruthy();\n  });\n\n  it('should allow schema override', async () =\u003e {\n    await db.import({\n      file: 'data.csv',\n      table: 'typed_table',\n      createTable: true,\n      schema: {\n        columns: [\n          { name: 'id', type: 'INTEGER', primaryKey: true },\n          { name: 'value', type: 'TEXT', nullable: false },\n        ],\n      },\n    });\n    \n    const tableInfo = await db.query(\"PRAGMA table_info('typed_table')\");\n    expect(tableInfo[0]).toMatchObject({ name: 'id', type: 'INTEGER', pk: 1 });\n  });\n});\n```\n\n#### 4. Export Tests\n```typescript\ndescribe('Export', () =\u003e {\n  beforeEach(async () =\u003e {\n    await db.execute('CREATE TABLE test_data (id INTEGER, name TEXT, value REAL)');\n    await db.execute('INSERT INTO test_data VALUES (1, \"Alice\", 10.5), (2, \"Bob\", 20.3)');\n  });\n\n  describe('CSV Export', () =\u003e {\n    it('should export table to CSV', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'output.csv',\n        format: 'csv',\n      });\n      \n      const content = await readFile('output.csv', 'utf-8');\n      expect(content).toBe('id,name,value\\n1,Alice,10.5\\n2,Bob,20.3\\n');\n    });\n\n    it('should export with custom delimiter', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'output.tsv',\n        format: 'csv',\n        csv: { delimiter: '\\t' },\n      });\n      \n      const content = await readFile('output.tsv', 'utf-8');\n      expect(content).toContain('id\\tname\\tvalue');\n    });\n\n    it('should escape special characters in CSV', async () =\u003e {\n      await db.execute('INSERT INTO test_data VALUES (3, \"Charlie, Jr.\", 30.0)');\n      \n      await db.export({\n        query: 'SELECT * FROM test_data WHERE id = 3',\n        file: 'escaped.csv',\n        format: 'csv',\n      });\n      \n      const content = await readFile('escaped.csv', 'utf-8');\n      expect(content).toContain('\"Charlie, Jr.\"');\n    });\n  });\n\n  describe('JSON Export', () =\u003e {\n    it('should export to JSON array', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'output.json',\n        format: 'json',\n      });\n      \n      const content = JSON.parse(await readFile('output.json', 'utf-8'));\n      expect(content).toEqual([\n        { id: 1, name: 'Alice', value: 10.5 },\n        { id: 2, name: 'Bob', value: 20.3 },\n      ]);\n    });\n\n    it('should export to JSONL', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'output.jsonl',\n        format: 'jsonl',\n      });\n      \n      const lines = (await readFile('output.jsonl', 'utf-8')).trim().split('\\n');\n      expect(JSON.parse(lines[0])).toEqual({ id: 1, name: 'Alice', value: 10.5 });\n      expect(JSON.parse(lines[1])).toEqual({ id: 2, name: 'Bob', value: 20.3 });\n    });\n  });\n\n  describe('Parquet Export', () =\u003e {\n    it('should export to Parquet', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'output.parquet',\n        format: 'parquet',\n      });\n      \n      // Verify by reading back\n      const result = await db.import({\n        file: 'output.parquet',\n        table: 'reimported',\n      });\n      \n      const data = await db.query('SELECT * FROM reimported');\n      expect(data).toEqual([\n        { id: 1, name: 'Alice', value: 10.5 },\n        { id: 2, name: 'Bob', value: 20.3 },\n      ]);\n    });\n\n    it('should set Parquet compression', async () =\u003e {\n      await db.export({\n        query: 'SELECT * FROM test_data',\n        file: 'compressed.parquet',\n        format: 'parquet',\n        parquet: { compression: 'snappy' },\n      });\n      \n      const stats = await stat('compressed.parquet');\n      // Compressed file should be smaller\n      expect(stats.size).toBeLessThan(1000);\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement to Pass Tests\n\n```typescript\n// packages/core/src/import-export/importer.ts\nexport class DataImporter {\n  async import(options: ImportOptions): Promise\u003cImportResult\u003e {\n    const format = options.format ?? detectFormat(options.file);\n    const importer = this.getImporter(format);\n    \n    // Infer schema if creating table\n    if (options.createTable) {\n      const schema = options.schema ?? await this.inferSchema(options.file, format);\n      await this.createTable(options.table, schema);\n    }\n    \n    // Stream import\n    let rowsImported = 0;\n    const batchSize = options.batchSize ?? 1000;\n    let batch: Record\u003cstring, unknown\u003e[] = [];\n    \n    for await (const row of importer.stream(options.file, options)) {\n      batch.push(row);\n      \n      if (batch.length \u003e= batchSize) {\n        await this.insertBatch(options.table, batch);\n        rowsImported += batch.length;\n        options.onProgress?.({\n          rowsImported,\n          percentage: importer.getProgress?.() ?? 0,\n        });\n        batch = [];\n      }\n    }\n    \n    // Insert remaining rows\n    if (batch.length \u003e 0) {\n      await this.insertBatch(options.table, batch);\n      rowsImported += batch.length;\n    }\n    \n    return { rowsImported };\n  }\n\n  private async insertBatch(table: string, rows: Record\u003cstring, unknown\u003e[]): Promise\u003cvoid\u003e {\n    if (rows.length === 0) return;\n    \n    const columns = Object.keys(rows[0]);\n    const placeholders = columns.map(() =\u003e '?').join(', ');\n    const sql = `INSERT INTO ${table} (${columns.join(', ')}) VALUES (${placeholders})`;\n    \n    await this.db.transaction(async () =\u003e {\n      for (const row of rows) {\n        await this.db.execute(sql, columns.map(c =\u003e row[c]));\n      }\n    });\n  }\n}\n\n// packages/core/src/import-export/csv-importer.ts\nexport class CSVImporter implements FormatImporter {\n  async *stream(file: string, options: CSVOptions): AsyncGenerator\u003cRecord\u003cstring, unknown\u003e\u003e {\n    const stream = createReadStream(file);\n    const parser = parse({\n      delimiter: options.delimiter ?? ',',\n      columns: options.header ?? true,\n      skip_empty_lines: true,\n      relax_quotes: true,\n    });\n    \n    stream.pipe(parser);\n    \n    for await (const record of parser) {\n      yield record;\n    }\n  }\n}\n\n// packages/core/src/import-export/parquet-importer.ts\nexport class ParquetImporter implements FormatImporter {\n  private progress = 0;\n\n  async *stream(file: string, options: ParquetOptions): AsyncGenerator\u003cRecord\u003cstring, unknown\u003e\u003e {\n    const reader = await parquet.ParquetReader.openFile(file);\n    const cursor = reader.getCursor();\n    const totalRows = reader.getRowCount();\n    let rowsRead = 0;\n    \n    let record: Record\u003cstring, unknown\u003e | null;\n    while ((record = await cursor.next())) {\n      yield record;\n      rowsRead++;\n      this.progress = (rowsRead / totalRows) * 100;\n    }\n    \n    await reader.close();\n  }\n\n  getProgress(): number {\n    return this.progress;\n  }\n}\n```\n\n### REFACTOR Phase - Improve Code Quality\n\n#### 1. Progress Reporting\n```typescript\n// packages/core/src/import-export/progress-reporter.ts\nexport class ProgressReporter {\n  private startTime: number;\n  private lastReport: number = 0;\n  private rowsProcessed: number = 0;\n  \n  constructor(\n    private totalRows: number | undefined,\n    private callback: ProgressCallback,\n    private reportInterval: number = 100 // ms\n  ) {\n    this.startTime = Date.now();\n  }\n\n  update(rows: number): void {\n    this.rowsProcessed += rows;\n    \n    const now = Date.now();\n    if (now - this.lastReport \u003c this.reportInterval) return;\n    \n    this.lastReport = now;\n    const elapsed = now - this.startTime;\n    const rowsPerSecond = this.rowsProcessed / (elapsed / 1000);\n    \n    this.callback({\n      rowsProcessed: this.rowsProcessed,\n      totalRows: this.totalRows,\n      percentage: this.totalRows ? (this.rowsProcessed / this.totalRows) * 100 : undefined,\n      rowsPerSecond,\n      elapsed,\n      eta: this.totalRows \n        ? ((this.totalRows - this.rowsProcessed) / rowsPerSecond) * 1000 \n        : undefined,\n    });\n  }\n\n  finish(): ImportStats {\n    const elapsed = Date.now() - this.startTime;\n    return {\n      rowsProcessed: this.rowsProcessed,\n      elapsed,\n      rowsPerSecond: this.rowsProcessed / (elapsed / 1000),\n    };\n  }\n}\n```\n\n#### 2. Resume Capability\n```typescript\n// packages/core/src/import-export/resumable-import.ts\nexport class ResumableImporter {\n  private checkpointFile: string;\n\n  async import(options: ImportOptions): Promise\u003cImportResult\u003e {\n    this.checkpointFile = `${options.file}.checkpoint`;\n    \n    // Load checkpoint if exists\n    const checkpoint = await this.loadCheckpoint();\n    const startPosition = checkpoint?.position ?? 0;\n    const startRow = checkpoint?.rowsImported ?? 0;\n    \n    const importer = this.getImporter(options.format);\n    let rowsImported = startRow;\n    \n    try {\n      for await (const { row, position } of importer.streamWithPosition(options.file, startPosition)) {\n        await this.insertRow(options.table, row);\n        rowsImported++;\n        \n        // Save checkpoint periodically\n        if (rowsImported % 10000 === 0) {\n          await this.saveCheckpoint({ position, rowsImported });\n        }\n      }\n      \n      // Clean up checkpoint on success\n      await this.deleteCheckpoint();\n      \n      return { rowsImported, resumed: startRow \u003e 0 };\n    } catch (error) {\n      // Checkpoint already saved, can resume\n      throw new ResumableError(error, rowsImported);\n    }\n  }\n\n  private async saveCheckpoint(data: Checkpoint): Promise\u003cvoid\u003e {\n    await writeFile(this.checkpointFile, JSON.stringify(data));\n  }\n\n  private async loadCheckpoint(): Promise\u003cCheckpoint | null\u003e {\n    try {\n      const content = await readFile(this.checkpointFile, 'utf-8');\n      return JSON.parse(content);\n    } catch {\n      return null;\n    }\n  }\n}\n```\n\n#### 3. Parallel Processing\n```typescript\n// packages/core/src/import-export/parallel-importer.ts\nexport class ParallelImporter {\n  async import(options: ImportOptions \u0026 { workers?: number }): Promise\u003cImportResult\u003e {\n    const workers = options.workers ?? os.cpus().length;\n    const fileSize = (await stat(options.file)).size;\n    const chunkSize = Math.ceil(fileSize / workers);\n    \n    // Split file into chunks (for formats that support it)\n    const chunks = await this.splitFile(options.file, chunkSize);\n    \n    // Process chunks in parallel\n    const results = await Promise.all(\n      chunks.map((chunk, i) =\u003e \n        this.processChunk(chunk, options, i)\n      )\n    );\n    \n    return {\n      rowsImported: results.reduce((sum, r) =\u003e sum + r.rowsImported, 0),\n      parallelChunks: workers,\n    };\n  }\n\n  private async processChunk(\n    chunk: ChunkInfo, \n    options: ImportOptions,\n    workerId: number\n  ): Promise\u003c{ rowsImported: number }\u003e {\n    const tempTable = `${options.table}_chunk_${workerId}`;\n    \n    // Import to temp table\n    const result = await this.importer.import({\n      ...options,\n      file: chunk.file,\n      table: tempTable,\n    });\n    \n    // Merge into main table\n    await this.db.execute(`INSERT INTO ${options.table} SELECT * FROM ${tempTable}`);\n    await this.db.execute(`DROP TABLE ${tempTable}`);\n    \n    return result;\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] All RED phase tests written and failing\n- [ ] All GREEN phase implementations passing tests\n- [ ] CSV import/export with custom delimiters\n- [ ] JSON and JSONL import/export\n- [ ] Parquet import/export with compression\n- [ ] Automatic format detection\n- [ ] Schema inference from data\n- [ ] Streaming import for large files\n- [ ] Progress reporting with ETA\n- [ ] Resume capability for interrupted imports\n- [ ] Memory-efficient processing\n\n## Labels\nimport, export, csv, json, parquet, streaming","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:37:50.044879-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:12:07.072113-06:00","closed_at":"2026-01-21T20:12:07.072113-06:00","close_reason":"Closed"}
{"id":"evodb-wvz","title":"Optimize string intern pool LRU cache hit handling","description":"## Problem\nString intern pool uses delete() + set() for LRU which is O(2) when move-to-end could be O(1).\n\n## TDD Approach\n1. Benchmark current LRU implementation\n2. Implement doubly-linked list for O(1) move-to-end\n3. Verify cache behavior preserved\n4. Verify performance improvement\n\n## Expected Impact\n- Faster cache operations at high hit rates (90%+)\n\n## Current Code (string-intern-pool.ts:157-206)\n```typescript\nif (existing !== undefined) {\n  this.cache.delete(s);      // O(1)\n  this.cache.set(s, { ... }); // O(1) but unnecessary\n}\n```\n\n## Fix\nUse doubly-linked list for O(1) move-to-tail on cache hit.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T12:05:34.240171-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T13:03:17.269149-06:00","closed_at":"2026-01-21T13:03:17.269149-06:00","close_reason":"Closed"}
{"id":"evodb-xhy","title":"TDD: Replace bloom filter Map with bit arrays","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:57:56.292664-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:36:48.954869-06:00","closed_at":"2026-01-20T14:36:48.954869-06:00","close_reason":"Closed"}
{"id":"evodb-xl5","title":"Reduce type assertions (as any, as unknown) in production code","description":"## Problem\n\nThere are type assertions (`as any`, `as unknown`) scattered throughout the codebase. While some are necessary (especially in tests), several in production code could be replaced with proper typing:\n\n**Production code issues**:\n\n1. **`observability/src/metrics.ts:290,315`**:\n```typescript\nconst internalMetric = metric as unknown as InternalMetric;\nconst internalHistogram = histogram as unknown as InternalHistogram;\n```\nThese suggest the internal type structure isn't properly exposed.\n\n2. **`lakehouse/src/manifest.ts:304`**:\n```typescript\nconst manifest = parsed as unknown as TableManifest;\n```\nShould use a type guard or validation function instead.\n\n3. **`core/src/query-ops.ts:835,851,867,883`**:\n```typescript\nmin: null as unknown,\nmax: null as unknown,\n```\nThese could use proper union types.\n\n4. **`benchmark/src/generators/data-generator.ts:84,85`**:\n```typescript\nmin: undefined as unknown,\nmax: undefined as unknown,\n```\n\n**Test code** (acceptable but could be improved):\n- Numerous `as any` in test mocks - these are acceptable but could use proper mock types\n\n## Recommendation\n1. Create proper internal types that don't require casting\n2. Use type guards instead of `as unknown as T`\n3. Create mock type utilities for tests\n4. Add `// @ts-expect-error` with explanation instead of silent `as any`\n\n## Files\n- `observability/src/metrics.ts`\n- `lakehouse/src/manifest.ts`\n- `core/src/query-ops.ts`","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:26:36.015194-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:49:08.036031-06:00","closed_at":"2026-01-21T20:49:08.036031-06:00","close_reason":"Reduced type assertions in production code. Fixed core/src/query-ops.ts, query/src/unified-engine.ts, and core/src/sync.ts. Key changes: 1) Added normalizeOperator function for type-safe operator conversion, 2) Refactored LWWMap to use setInternal for nullable values, 3) Simplified aggregator state initialization. Verified with 247 passing tests.","labels":["code-quality","type-safety"]}
{"id":"evodb-xvii","title":"PRODUCT: Data import/export utilities","description":"## Problem\nNo utilities for bulk data import from common formats (CSV, JSON, Parquet).\n\n## Solution\nAdd import/export utilities:\n```typescript\nimport { importCSV, importJSON, exportParquet } from '@evodb/import';\n\n// Import from CSV\nawait importCSV(db, 'users', './users.csv', {\n  delimiter: ',',\n  headers: true,\n  batchSize: 10000\n});\n\n// Import from JSON Lines\nawait importJSON(db, 'events', './events.jsonl', { format: 'jsonl' });\n\n// Export to Parquet for external analytics\nawait exportParquet(db, 'events', './events.parquet');\n```\n\n## CLI Support\n```bash\nnpx evodb import users ./users.csv\nnpx evodb export events ./events.parquet\n```\n\n## Impact\nEssential for onboarding existing data","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:30:33.335262-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:12:07.101601-06:00","closed_at":"2026-01-21T20:12:07.101601-06:00","close_reason":"Closed"}
{"id":"evodb-yg1","title":"TDD: Add writer integration tests","description":"Writer has only 5 tests. Add 20+ integration tests for full CDC pipeline validation.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:54.621544-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:32:03.177884-06:00","closed_at":"2026-01-20T13:32:03.177884-06:00","close_reason":"Closed"}
{"id":"evodb-yx1","title":"TDD: Add property-based testing with fast-check","description":"Add fast-check for generated test cases on shredding, encoding, partition calculation. Target 100+ cases per fn.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:10:10.343621-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:58.028729-06:00","closed_at":"2026-01-20T16:54:58.028729-06:00","close_reason":"Closed","external_ref":"gh-58"}
{"id":"evodb-yzb","title":"TDD: Add storage adapter pattern for R2","description":"R2Bucket tightly coupled throughout. Add StorageAdapter interface with R2Adapter and MockAdapter implementations.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:09:19.825991-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T13:17:41.85131-06:00","closed_at":"2026-01-20T13:17:41.85131-06:00","close_reason":"Closed"}
{"id":"evodb-z9g","title":"TDD: Add migration guide doc","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T13:59:45.939697-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T16:54:57.905099-06:00","closed_at":"2026-01-20T16:54:57.905099-06:00","close_reason":"Closed"}
{"id":"evodb-zdo","title":"Cache error handling silent fallthrough","description":"TDD: Cache errors silently fall through without logging. Add proper error logging and metrics for cache failures.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T18:18:00.99131-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T18:54:46.026933-06:00","closed_at":"2026-01-20T18:54:46.026933-06:00","close_reason":"Closed","external_ref":"gh-137"}
{"id":"evodb-zeg","title":"P0: Limit vitest concurrency to prevent 50GB RAM usage","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-20T14:37:39.368287-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-20T14:43:50.900276-06:00","closed_at":"2026-01-20T14:43:50.900276-06:00","close_reason":"Closed"}
{"id":"evodb-zgbb","title":"TDD: Expand property-based testing coverage","description":"## Overview\nExpand property-based testing to discover edge cases and verify invariants across query operations, merge logic, and data shredding.\n\n## TDD Phases\n\n### RED Phase - Write Property Tests\n\nWrite property-based tests for core operations:\n\n```typescript\nimport { fc } from '@fast-check/vitest';\n\ndescribe('Property-Based Tests', () =\u003e {\n  describe('query operations', () =\u003e {\n    it('filter then count equals count with filter', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryTable,\n          arbitraryFilter,\n          (table, filter) =\u003e {\n            const filtered = table.filter(filter);\n            const countFiltered = table.count(filter);\n            return filtered.length === countFiltered;\n          }\n        )\n      );\n    });\n    \n    it('sort is idempotent', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryTable,\n          arbitrarySortSpec,\n          (table, sort) =\u003e {\n            const once = table.sort(sort);\n            const twice = once.sort(sort);\n            return deepEqual(once, twice);\n          }\n        )\n      );\n    });\n    \n    it('limit + offset covers all rows exactly once', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryTable,\n          fc.integer({ min: 1, max: 100 }),\n          (table, pageSize) =\u003e {\n            const pages = collectAllPages(table, pageSize);\n            return pages.flat().length === table.length;\n          }\n        )\n      );\n    });\n  });\n  \n  describe('merge operations', () =\u003e {\n    it('merge is associative', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryDelta,\n          arbitraryDelta,\n          arbitraryDelta,\n          (a, b, c) =\u003e {\n            const left = merge(merge(a, b), c);\n            const right = merge(a, merge(b, c));\n            return deepEqual(left, right);\n          }\n        )\n      );\n    });\n    \n    it('merge is commutative for non-conflicting changes', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryNonConflictingDeltas,\n          ([a, b]) =\u003e {\n            return deepEqual(merge(a, b), merge(b, a));\n          }\n        )\n      );\n    });\n  });\n  \n  describe('shredding', () =\u003e {\n    it('shred then unshred is identity', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryRow,\n          (row) =\u003e {\n            const shredded = shred(row);\n            const unshredded = unshred(shredded);\n            return deepEqual(row, unshredded);\n          }\n        )\n      );\n    });\n    \n    it('shredded representation is smaller or equal', () =\u003e {\n      fc.assert(\n        fc.property(\n          arbitraryRows,\n          (rows) =\u003e {\n            const original = JSON.stringify(rows).length;\n            const shredded = shred(rows);\n            return shredded.byteLength \u003c= original;\n          }\n        )\n      );\n    });\n  });\n});\n```\n\n### GREEN Phase - Implement Generators for All Types\n\n1. Implement base type generators:\n   ```typescript\n   const arbitraryColumnValue = fc.oneof(\n     fc.string(),\n     fc.integer(),\n     fc.double(),\n     fc.boolean(),\n     fc.constant(null),\n     fc.date(),\n     fc.array(fc.string())\n   );\n   \n   const arbitraryRow = fc.dictionary(\n     fc.string().filter(isValidColumnName),\n     arbitraryColumnValue\n   );\n   \n   const arbitraryTable = fc.array(arbitraryRow, { minLength: 0, maxLength: 1000 });\n   ```\n\n2. Implement operation generators:\n   ```typescript\n   const arbitraryFilter = fc.oneof(\n     fc.record({ op: fc.constant('eq'), field: fc.string(), value: arbitraryColumnValue }),\n     fc.record({ op: fc.constant('gt'), field: fc.string(), value: fc.integer() }),\n     fc.record({ op: fc.constant('contains'), field: fc.string(), value: fc.string() }),\n     // Compound filters\n     fc.record({ op: fc.constant('and'), filters: fc.array(arbitrarySimpleFilter) }),\n     fc.record({ op: fc.constant('or'), filters: fc.array(arbitrarySimpleFilter) })\n   );\n   \n   const arbitraryDelta = fc.record({\n     inserts: fc.array(arbitraryRow),\n     updates: fc.array(fc.tuple(arbitraryRowId, arbitraryPartialRow)),\n     deletes: fc.array(arbitraryRowId)\n   });\n   ```\n\n3. Make all property tests pass\n\n### REFACTOR Phase - Shrinking and Edge Case Generators\n\n1. Add custom shrinking:\n   ```typescript\n   const arbitraryTableWithShrinking = fc.array(arbitraryRow).chain(rows =\u003e {\n     return fc.constant(rows).map(r =\u003e ({\n       value: r,\n       shrink: () =\u003e shrinkTable(r)\n     }));\n   });\n   ```\n\n2. Add edge case generators:\n   ```typescript\n   const edgeCaseValues = fc.oneof(\n     fc.constant(''),           // empty string\n     fc.constant(0),            // zero\n     fc.constant(-0),           // negative zero\n     fc.constant(Infinity),     // infinity\n     fc.constant(NaN),          // NaN\n     fc.constant(null),         // null\n     fc.constant(undefined),    // undefined\n     fc.constant([]),           // empty array\n     fc.constant({}),           // empty object\n     fc.string().filter(s =\u003e s.includes('\\0')),  // null bytes\n     fc.string().filter(s =\u003e s.includes('\\n')),  // newlines\n     fc.unicodeString(),        // unicode edge cases\n   );\n   ```\n\n3. Add stress generators:\n   - Very large tables (100K+ rows)\n   - Deeply nested objects\n   - Wide tables (1000+ columns)\n   - Binary data in strings\n\n## Acceptance Criteria\n- [ ] All RED phase property tests written\n- [ ] Generators implemented for all types\n- [ ] All property tests passing\n- [ ] Custom shrinking for better failure reports\n- [ ] Edge case generators finding real bugs\n- [ ] Property tests run in \u003c 30 seconds\n- [ ] At least 100 iterations per property","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:36:23.717104-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:11:39.059971-06:00","closed_at":"2026-01-21T20:11:39.059971-06:00","close_reason":"Closed"}
{"id":"evodb-zrq","title":"TypeScript: Add generic constraints to improve type inference","description":"## Summary\n\nSeveral generic functions could benefit from tighter constraints to improve type inference and catch errors at compile time.\n\n## Locations\n\n### core/src/evodb.ts - QueryBuilder\u003cT\u003e\n\n```typescript\nexport class QueryBuilder\u003cT = Record\u003cstring, unknown\u003e\u003e {\n```\n\nThe `T` parameter has no constraint, allowing `QueryBuilder\u003cnumber\u003e` which makes no sense. Consider:\n\n```typescript\nexport class QueryBuilder\u003cT extends Record\u003cstring, unknown\u003e = Record\u003cstring, unknown\u003e\u003e {\n```\n\n### query/src/types.ts - EngineQueryResult\u003cT\u003e\n\n```typescript\nexport interface EngineQueryResult\u003cT = Record\u003cstring, unknown\u003e\u003e {\n```\n\nSame issue - should constrain T to objects.\n\n### lakehouse/src/r2.ts - readJson\u003cT\u003e\n\n```typescript\nasync readJson\u003cT\u003e(path: string): Promise\u003cT | null\u003e\n```\n\nNo constraint means T could be anything. Consider:\n\n```typescript\nasync readJson\u003cT extends Record\u003cstring, unknown\u003e\u003e(path: string): Promise\u003cT | null\u003e\n```\n\n### rpc/src/types.ts - WalEntry\u003cT\u003e\n\n```typescript\nexport type WalEntry\u003cT = unknown\u003e = CoreRpcWalEntry\u003cT\u003e;\n```\n\nThe `T` represents row data, should be constrained to objects:\n\n```typescript\nexport type WalEntry\u003cT extends Record\u003cstring, unknown\u003e = Record\u003cstring, unknown\u003e\u003e = CoreRpcWalEntry\u003cT\u003e;\n```\n\n### writer/src/types.ts - RPCMessage\u003cT\u003e\n\n```typescript\nexport interface RPCMessage\u003cT = unknown\u003e {\n  payload: T;\n}\n```\n\nCould benefit from constraint if payload is always an object.\n\n## Benefits\n\n1. Better type inference when using these APIs\n2. Compile-time errors for invalid type parameters\n3. Clearer API contracts\n4. Better IDE autocomplete\n\n## Migration\n\nThis is a breaking change for consumers using unconstrained generics. Consider:\n1. Add constraints as optional (default still works)\n2. Document the expected types clearly\n3. Provide migration guide","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-21T17:25:07.271817-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-21T20:14:08.117357-06:00","closed_at":"2026-01-21T20:14:08.117357-06:00","close_reason":"Duplicate of already-implemented issues"}
