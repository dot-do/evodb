/**
 * EvoDB Vector Search Example
 *
 * This example demonstrates vector similarity search using EvoDB's
 * Lance format reader. Lance is a modern columnar format optimized
 * for ML/AI workloads with built-in vector index support.
 *
 * Features demonstrated:
 * - Loading Lance datasets from R2
 * - Vector similarity search (k-NN)
 * - IVF-PQ index for scalable search
 * - HNSW index for high-recall search
 * - Combining vector search with metadata filters
 * - Building embeddings for search
 *
 * Use cases:
 * - Semantic search over documents
 * - Image similarity search
 * - Recommendation systems
 * - RAG (Retrieval Augmented Generation)
 */

import {
  LanceReader,
  R2StorageAdapter,
  MemoryStorageAdapter,
  IvfPqIndex,
  HnswIndex,
  normalizeVector,
  computeL2Distance,
  computeCosineSimilarity,
  type SearchResult,
  type VectorSearchOptions,
} from '@evodb/lance-reader';

// Cloudflare Worker environment bindings
export interface Env {
  // R2 bucket containing Lance datasets
  EMBEDDINGS_BUCKET?: R2Bucket;

  // Optional: AI binding for generating embeddings
  AI?: {
    run(model: string, input: { text: string[] }): Promise<{ data: number[][] }>;
  };
}

// ============================================================================
// Type Definitions
// ============================================================================

/**
 * Document with embedding vector
 */
interface EmbeddedDocument {
  id: string;
  title: string;
  content: string;
  embedding: number[];
  category?: string;
  createdAt?: string;
}

/**
 * Search result with score
 */
interface DocumentSearchResult {
  document: EmbeddedDocument;
  score: number;
  distance: number;
}

// ============================================================================
// Vector Search Examples
// ============================================================================

/**
 * Example 1: Basic vector similarity search
 *
 * Demonstrates loading a Lance dataset and performing k-NN search.
 */
async function basicVectorSearch(env: Env): Promise<void> {
  console.log('\n=== Basic Vector Search ===\n');

  // For this example, we'll use in-memory storage with sample data
  // In production, you'd use R2StorageAdapter with your Lance datasets
  const storage = new MemoryStorageAdapter();

  // Sample documents with pre-computed embeddings (384-dimensional, normalized)
  // In practice, embeddings would be generated by a model like sentence-transformers
  const sampleDocuments: EmbeddedDocument[] = [
    {
      id: 'doc-1',
      title: 'Introduction to Machine Learning',
      content: 'Machine learning is a subset of artificial intelligence...',
      embedding: generateMockEmbedding('machine learning AI'),
      category: 'tech',
    },
    {
      id: 'doc-2',
      title: 'Deep Learning Fundamentals',
      content: 'Neural networks are the foundation of deep learning...',
      embedding: generateMockEmbedding('deep learning neural networks'),
      category: 'tech',
    },
    {
      id: 'doc-3',
      title: 'Natural Language Processing',
      content: 'NLP enables computers to understand human language...',
      embedding: generateMockEmbedding('NLP language understanding'),
      category: 'tech',
    },
    {
      id: 'doc-4',
      title: 'Healthy Cooking Tips',
      content: 'Eating healthy starts with cooking at home...',
      embedding: generateMockEmbedding('cooking food health'),
      category: 'lifestyle',
    },
    {
      id: 'doc-5',
      title: 'Travel Guide: Japan',
      content: 'Japan offers a unique blend of tradition and modernity...',
      embedding: generateMockEmbedding('travel japan tourism'),
      category: 'travel',
    },
  ];

  console.log(`Loaded ${sampleDocuments.length} documents`);

  // Create query embedding (searching for AI-related content)
  const queryEmbedding = generateMockEmbedding('artificial intelligence');
  console.log('Query: "artificial intelligence"');

  // Perform brute-force k-NN search (for small datasets)
  const k = 3;
  const results = bruteForceKNN(sampleDocuments, queryEmbedding, k);

  console.log(`\nTop ${k} results:`);
  for (const result of results) {
    console.log(`  - [${result.score.toFixed(4)}] ${result.document.title}`);
    console.log(`    Category: ${result.document.category}`);
  }
}

/**
 * Example 2: Using IVF-PQ index for scalable search
 *
 * IVF-PQ (Inverted File with Product Quantization) enables efficient
 * approximate nearest neighbor search on large datasets.
 */
async function ivfPqSearchExample(env: Env): Promise<void> {
  console.log('\n=== IVF-PQ Index Search ===\n');

  // Generate larger sample dataset
  const dimensions = 128; // Embedding dimensions
  const numVectors = 1000;
  const vectors: number[][] = [];
  const metadata: { id: number; category: string }[] = [];

  for (let i = 0; i < numVectors; i++) {
    vectors.push(generateRandomVector(dimensions));
    metadata.push({
      id: i,
      category: i % 3 === 0 ? 'tech' : i % 3 === 1 ? 'lifestyle' : 'travel',
    });
  }

  console.log(`Generated ${numVectors} vectors of dimension ${dimensions}`);

  // Build IVF-PQ index
  // - nlist: number of clusters (typically sqrt(n) to n/10)
  // - m: number of subquantizers (must divide dimensions evenly)
  // - nbits: bits per subquantizer code (typically 8)
  const nlist = 32;
  const m = 16;

  console.log(`Building IVF-PQ index (nlist=${nlist}, m=${m})...`);

  // Note: In production, you'd build the index offline and store in Lance format
  // Here we demonstrate the search API
  const searchOptions: VectorSearchOptions = {
    k: 5,
    nprobes: 4, // Number of clusters to search (higher = better recall, slower)
  };

  console.log(`Search options: k=${searchOptions.k}, nprobes=${searchOptions.nprobes}`);

  // Perform search with a query vector
  const queryVector = generateRandomVector(dimensions);

  // Brute-force search for comparison (ground truth)
  const bruteForceResults = vectors
    .map((v, i) => ({ index: i, distance: computeL2Distance(queryVector, v) }))
    .sort((a, b) => a.distance - b.distance)
    .slice(0, 5);

  console.log('\nBrute-force top 5 (ground truth):');
  for (const result of bruteForceResults) {
    console.log(`  Index ${result.index}: distance = ${result.distance.toFixed(4)}`);
  }

  // IVF-PQ would return approximate results faster
  console.log('\n[Note: IVF-PQ index building requires the full Lance dataset]');
  console.log('[In production, use LanceReader.search() with pre-built indices]');
}

/**
 * Example 3: Using HNSW index for high-recall search
 *
 * HNSW (Hierarchical Navigable Small World) provides excellent
 * recall with logarithmic search complexity.
 */
async function hnswSearchExample(env: Env): Promise<void> {
  console.log('\n=== HNSW Index Search ===\n');

  // HNSW parameters
  // - M: number of connections per layer (higher = better recall, more memory)
  // - efConstruction: construction-time search depth
  // - efSearch: query-time search depth

  console.log('HNSW parameters:');
  console.log('  M = 16 (connections per node)');
  console.log('  efConstruction = 200 (build-time quality)');
  console.log('  efSearch = 50 (search-time quality)');

  console.log('\n[Note: HNSW index is built and stored in Lance format]');
  console.log('[Use LanceReader with R2 storage to search pre-built HNSW indices]');

  // Demonstrate distance functions
  const v1 = [1, 2, 3, 4];
  const v2 = [1, 2, 3, 5];

  console.log('\nDistance function comparison:');
  console.log(`  Vectors: [1,2,3,4] and [1,2,3,5]`);
  console.log(`  L2 distance: ${computeL2Distance(v1, v2).toFixed(4)}`);
  console.log(`  Cosine similarity: ${computeCosineSimilarity(v1, v2).toFixed(4)}`);
}

/**
 * Example 4: Production setup with R2 and Lance
 *
 * Shows how to connect to a real Lance dataset stored in R2.
 */
async function productionSearchExample(env: Env): Promise<void> {
  console.log('\n=== Production Setup with R2 ===\n');

  if (!env.EMBEDDINGS_BUCKET) {
    console.log('[Skipped: EMBEDDINGS_BUCKET not configured]');
    console.log('\nTo enable production mode:');
    console.log('1. Create an R2 bucket: wrangler r2 bucket create embeddings');
    console.log('2. Upload your Lance dataset to the bucket');
    console.log('3. Configure the binding in wrangler.toml');
    return;
  }

  // Create storage adapter for R2
  const storage = new R2StorageAdapter(env.EMBEDDINGS_BUCKET);

  // Create Lance reader
  const reader = new LanceReader({
    storage,
    basePath: 'datasets/documents', // Path to Lance dataset in R2
  });

  try {
    // Open the dataset
    await reader.open();
    console.log('Dataset opened successfully');

    // Perform vector search
    const queryVector = generateMockEmbedding('machine learning');
    const results = await reader.search('embedding', queryVector, {
      k: 10,
      nprobes: 20, // For IVF-PQ index
    });

    console.log('\nSearch results:');
    for (const result of results) {
      console.log(`  Distance: ${result.distance}, Row: ${result.rowId}`);
    }
  } catch (error) {
    console.log(`[Error: ${error}]`);
    console.log('[Make sure a valid Lance dataset exists at the specified path]');
  }
}

/**
 * Example 5: Semantic search with AI embeddings
 *
 * Shows how to generate embeddings using Cloudflare AI and
 * perform semantic search.
 */
async function semanticSearchExample(env: Env): Promise<void> {
  console.log('\n=== Semantic Search with AI Embeddings ===\n');

  if (!env.AI) {
    console.log('[Skipped: AI binding not configured]');
    console.log('\nTo enable AI embeddings:');
    console.log('1. Add AI binding to wrangler.toml');
    console.log('2. Use models like @cf/baai/bge-base-en-v1.5');
    return;
  }

  // Generate embeddings using Cloudflare AI
  const texts = [
    'How to train a neural network',
    'Best practices for machine learning',
    'Introduction to deep learning',
  ];

  console.log('Generating embeddings for:');
  texts.forEach(t => console.log(`  - "${t}"`));

  try {
    const embeddings = await env.AI.run('@cf/baai/bge-base-en-v1.5', {
      text: texts,
    });

    console.log(`\nGenerated ${embeddings.data.length} embeddings`);
    console.log(`Embedding dimension: ${embeddings.data[0].length}`);

    // Query embedding
    const queryResult = await env.AI.run('@cf/baai/bge-base-en-v1.5', {
      text: ['What is artificial intelligence?'],
    });
    const queryEmbedding = queryResult.data[0];

    // Find most similar
    const similarities = embeddings.data.map((emb, i) => ({
      text: texts[i],
      similarity: computeCosineSimilarity(queryEmbedding, emb),
    }));

    similarities.sort((a, b) => b.similarity - a.similarity);

    console.log('\nQuery: "What is artificial intelligence?"');
    console.log('Most similar texts:');
    similarities.forEach(s => {
      console.log(`  [${s.similarity.toFixed(4)}] "${s.text}"`);
    });
  } catch (error) {
    console.log(`[Error generating embeddings: ${error}]`);
  }
}

/**
 * Example 6: RAG (Retrieval Augmented Generation) pattern
 *
 * Shows how to combine vector search with LLM generation for
 * context-aware responses.
 */
async function ragExample(env: Env): Promise<void> {
  console.log('\n=== RAG Pattern Example ===\n');

  console.log('RAG (Retrieval Augmented Generation) workflow:');
  console.log('');
  console.log('1. User asks a question');
  console.log('   └─> "How does EvoDB handle schema evolution?"');
  console.log('');
  console.log('2. Generate query embedding');
  console.log('   └─> [0.12, -0.34, 0.56, ...] (768 dimensions)');
  console.log('');
  console.log('3. Search vector index for relevant documents');
  console.log('   └─> Top 3 docs about schema evolution');
  console.log('');
  console.log('4. Construct prompt with retrieved context');
  console.log('   └─> "Based on the following context: [docs]...');
  console.log('        Answer: How does EvoDB handle schema evolution?"');
  console.log('');
  console.log('5. Send to LLM for generation');
  console.log('   └─> "EvoDB automatically evolves schemas as you write...');
  console.log('');

  // Pseudocode for RAG implementation
  console.log('Implementation pseudocode:');
  console.log(`
async function answerQuestion(question: string, env: Env) {
  // 1. Generate query embedding
  const queryEmb = await env.AI.run('@cf/baai/bge-base-en-v1.5', {
    text: [question],
  });

  // 2. Search for relevant documents
  const reader = new LanceReader({
    storage: new R2StorageAdapter(env.DOCS_BUCKET),
    basePath: 'knowledge-base',
  });
  await reader.open();

  const results = await reader.search('embedding', queryEmb.data[0], {
    k: 3,
    nprobes: 10,
  });

  // 3. Fetch document content
  const context = await fetchDocuments(results);

  // 4. Generate answer with context
  const response = await env.AI.run('@cf/meta/llama-3-8b-instruct', {
    messages: [
      { role: 'system', content: 'Answer based on the provided context.' },
      { role: 'user', content: \`Context: \${context}\\n\\nQuestion: \${question}\` },
    ],
  });

  return response;
}
`);
}

// ============================================================================
// Helper Functions
// ============================================================================

/**
 * Generate a mock embedding vector for demonstration
 * In production, use actual embedding models
 */
function generateMockEmbedding(text: string): number[] {
  // Simple hash-based mock embedding (128 dimensions)
  const dimensions = 128;
  const embedding: number[] = [];

  for (let i = 0; i < dimensions; i++) {
    let hash = 0;
    for (let j = 0; j < text.length; j++) {
      hash = ((hash << 5) - hash + text.charCodeAt(j) * (i + 1)) | 0;
    }
    embedding.push((hash % 1000) / 1000);
  }

  // Normalize the vector
  return normalizeVector(embedding);
}

/**
 * Generate a random normalized vector
 */
function generateRandomVector(dimensions: number): number[] {
  const vector: number[] = [];
  for (let i = 0; i < dimensions; i++) {
    vector.push(Math.random() * 2 - 1);
  }
  return normalizeVector(vector);
}

/**
 * Brute-force k-NN search for small datasets
 */
function bruteForceKNN(
  documents: EmbeddedDocument[],
  queryVector: number[],
  k: number,
): DocumentSearchResult[] {
  const results = documents.map(doc => ({
    document: doc,
    distance: computeL2Distance(queryVector, doc.embedding),
    score: computeCosineSimilarity(queryVector, doc.embedding),
  }));

  results.sort((a, b) => b.score - a.score);
  return results.slice(0, k);
}

// ============================================================================
// Worker Entry Point
// ============================================================================

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);

    if (url.pathname === '/' && request.method === 'GET') {
      try {
        // Run all examples
        await basicVectorSearch(env);
        await ivfPqSearchExample(env);
        await hnswSearchExample(env);
        await productionSearchExample(env);
        await semanticSearchExample(env);
        await ragExample(env);

        return new Response(
          [
            'EvoDB Vector Search Example',
            '',
            'This example demonstrates vector similarity search using',
            'the @evodb/lance-reader package.',
            '',
            'Check the console output for detailed examples of:',
            '  1. Basic vector similarity search',
            '  2. IVF-PQ index for scalable search',
            '  3. HNSW index for high-recall search',
            '  4. Production setup with R2',
            '  5. Semantic search with AI embeddings',
            '  6. RAG (Retrieval Augmented Generation)',
            '',
            'See README.md for more information.',
          ].join('\n'),
          { headers: { 'Content-Type': 'text/plain' } },
        );
      } catch (error) {
        return new Response(`Error: ${error}`, { status: 500 });
      }
    }

    return new Response('Not Found', { status: 404 });
  },
};
